# Translations template for PROJECT.
# Copyright (C) 2025 ORGANIZATION
# This file is distributed under the same license as the PROJECT project.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2025.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: PROJECT VERSION\n"
"Report-Msgid-Bugs-To: EMAIL@ADDRESS\n"
"POT-Creation-Date: 2025-03-25 00:48+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.17.0\n"

#: ../../source/featured_modules/10_depth_estimation_pipeline.rst:2
msgid "DepthEstimationPipeline"
msgstr ""

#: ../../source/featured_modules/10_depth_estimation_pipeline.rst:4
msgid ""
"A method that implements depth map reconstruction from a monocular image "
"by a neural network and scaling of the reconstructed depth map using a "
"sparse lidar point cloud."
msgstr ""

#: ../../source/featured_modules/10_depth_estimation_pipeline.rst:7
msgid "Usage example"
msgstr ""

#: ../../source/featured_modules/10_depth_estimation_pipeline.rst:9
msgid ""
"This is an example of indoor depth reconstruction with state-of-the-art "
"(2024) DepthAnything-v2 neural network model. At start, you should first "
"initialize depth estimation neural network model, which can be imported "
"from `the DepthAnything-v2 package which is added to OPR as a submodule "
"<https://github.com/DepthAnything/Depth-"
"Anything-V2/tree/28ad5a0797dfb8ac76d1e3dcddbe2160cbcc6c8d>`_:"
msgstr ""

#: ../../source/featured_modules/10_depth_estimation_pipeline.rst:18
msgid ""
"For the best performance, we recommend to use version \"small\" of the "
"model:"
msgstr ""

#: ../../source/featured_modules/10_depth_estimation_pipeline.rst:37
msgid ""
"Next, you should create an instance of DepthEstimationPipeline. It "
"requires a camera intrinsic matrix and a lidar-to-camera transformation "
"matrix for correct operation. This is an example of the matrices for "
"ITLP-Campus dataset:"
msgstr ""

#: ../../source/featured_modules/10_depth_estimation_pipeline.rst:53
msgid "Create the DepthEstimationPipeline with these matrices:"
msgstr ""

#: ../../source/featured_modules/10_depth_estimation_pipeline.rst:61
msgid ""
"Now you can run the pipeline on a pair of an RGB image and a lidar point "
"cloud:"
msgstr ""

#: ../../source/featured_modules/10_depth_estimation_pipeline.rst:67
msgid ""
"``\"test_img\"`` is an RGB image stored as numpy.ndarray of shape (h, w, "
"3) and uint8 data type"
msgstr ""

#: ../../source/featured_modules/10_depth_estimation_pipeline.rst:68
msgid ""
"``\"test_cloud\"`` is a lidar point cloud stored as numpy.ndarray of "
"shape (N, 3) and float data type (the values are (x, y, z) coordinates in"
" meters)"
msgstr ""

#: ../../source/featured_modules/10_depth_estimation_pipeline.rst:69
msgid ""
"``\"predicted_depth\"`` is the reconstructed depth stored as "
"numpy.ndarray of shape (h, w) and float data type. The values of the "
"array are depths in meters."
msgstr ""

#: ../../source/featured_modules/10_depth_estimation_pipeline.rst:71
msgid ""
"An illustrated demo of the work of DepthEstimationPipeline can be found "
"in `the demo notebook <https://github.com/OPR-"
"Project/OpenPlaceRecognition/blob/depth_reconstruction_nclt/notebooks/test_depth_reconstruction.ipynb>`_"
msgstr ""

