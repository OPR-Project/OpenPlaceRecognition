{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines subpackage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `opr.pipelines` subpackage contains ready-to-use pipelines for model inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "from hydra.utils import instantiate\n",
    "import numpy as np\n",
    "from omegaconf import OmegaConf\n",
    "from scipy.spatial.transform import Rotation\n",
    "\n",
    "try:\n",
    "    from geotransformer.utils.registration import compute_registration_error\n",
    "except ImportError:\n",
    "    print(\"WARNIGN: geotransformer not installed, registration error will not be computed\")\n",
    "\n",
    "from opr.datasets.itlp import ITLPCampus\n",
    "from opr.pipelines.place_recognition import PlaceRecognitionPipeline\n",
    "from opr.pipelines.registration import PointcloudRegistrationPipeline\n",
    "\n",
    "\n",
    "def pose_to_matrix(pose):\n",
    "    \"\"\"From the 6D poses in the [tx ty tz qx qy qz qw] format to 4x4 pose matrices.\"\"\"\n",
    "    position = pose[:3]\n",
    "    orientation_quat = pose[3:]\n",
    "    rotation = Rotation.from_quat(orientation_quat)\n",
    "    pose_matrix = np.eye(4)\n",
    "    pose_matrix[:3,:3] = rotation.as_matrix()\n",
    "    pose_matrix[:3,3] = position\n",
    "    return pose_matrix\n",
    "\n",
    "\n",
    "def compute_error(estimated_pose, gt_pose):\n",
    "    \"\"\"For the 6D poses in the [tx ty tz qx qy qz qw] format.\"\"\"\n",
    "    estimated_pose = pose_to_matrix(estimated_pose)\n",
    "    gt_pose = pose_to_matrix(gt_pose)\n",
    "    error_pose = np.linalg.inv(estimated_pose) @ gt_pose\n",
    "    dist_error = np.sum(error_pose[:3, 3]**2) ** 0.5\n",
    "    r = Rotation.from_matrix(error_pose[:3, :3])\n",
    "    rotvec = r.as_rotvec()\n",
    "    angle_error = (np.sum(rotvec**2)**0.5) * 180 / np.pi\n",
    "    angle_error = abs(90 - abs(angle_error-90))\n",
    "    return dist_error, angle_error\n",
    "\n",
    "def compute_translation_error(gt_pose, pred_pose):\n",
    "    \"\"\"For the 4x4 pose matrices.\"\"\"\n",
    "    gt_trans = gt_pose[:3, 3]\n",
    "    pred_trans = pred_pose[:3, 3]\n",
    "    error = np.linalg.norm(gt_trans - pred_trans)\n",
    "    return error\n",
    "\n",
    "def compute_rotation_error(gt_pose, pred_pose):\n",
    "    \"\"\"For the 4x4 pose matrices.\"\"\"\n",
    "    gt_rot = Rotation.from_matrix(gt_pose[:3, :3])\n",
    "    pred_rot = Rotation.from_matrix(pred_pose[:3, :3])\n",
    "    error = Rotation.inv(gt_rot) * pred_rot\n",
    "    error = error.as_euler('xyz', degrees=True)\n",
    "    error = np.linalg.norm(error)\n",
    "    return error\n",
    "\n",
    "def compute_absolute_pose_error(gt_pose, pred_pose):\n",
    "    \"\"\"For the 4x4 pose matrices.\"\"\"\n",
    "    rotation_error = compute_rotation_error(gt_pose, pred_pose)\n",
    "    translation_error = compute_translation_error(gt_pose, pred_pose)\n",
    "    return rotation_error, translation_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage example - Place Recognition Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATABASE_TRACK_DIR = \"/home/docker_opr/Datasets/ITLP-Campus-data/subsampled_data/outdoor/old/00_2023-02-10\"\n",
    "QUERY_TRACK_DIR = \"/home/docker_opr/Datasets/ITLP-Campus-data/subsampled_data/outdoor/old/01_2023-02-21\"\n",
    "\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "MODEL_CONFIG_PATH = \"../configs/model/place_recognition/minkloc3d.yaml\"\n",
    "WEIGHTS_PATH = \"../weights/place_recognition/minkloc3d_nclt.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Init query dataset\n",
    "\n",
    "The pipeline infer method accepts an input in the format of dictionary with keys in the following format:\n",
    "- `\"image_{camera_name}\"` for images from cameras,\n",
    "- `\"mask_{camera_name}\"` for semantic segmentation masks,\n",
    "- `\"pointcloud_lidar_coords\"` for pointcloud coordinates from lidar,\n",
    "- `\"pointcloud_lidar_feats\"` for pointcloud features from lidar.\n",
    "\n",
    "The data type of all values are `torch.Tensor`.\n",
    "\n",
    "You can load and preprocess the data manually, but in this example we will use the `opr.datasets.itlp.ITLPCampus` ` dataset class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_dataset = ITLPCampus(\n",
    "    dataset_root=QUERY_TRACK_DIR,\n",
    "    sensors=[\"lidar\"],\n",
    "    mink_quantization_size=0.5,\n",
    "    load_semantics=False,\n",
    "    load_text_descriptions=False,\n",
    "    load_text_labels=False,\n",
    "    load_aruco_labels=False,\n",
    "    indoor=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize model\n",
    "\n",
    "We will use hydra's `instantiate` function to initialize the model. The model is a `MinkLoc3D` - a simple LiDAR-only architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = OmegaConf.load(MODEL_CONFIG_PATH)\n",
    "model = instantiate(model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The minimum requirement to initialize the `PlaceRecognitionPipeline` is that the database directory should contain the `index.faiss` file and the `track.csv` file.\n",
    "\n",
    "The `index.faiss` file is a Faiss index, which contains the descriptors of the database. The `track.csv` file contains the metadata of the database, including the id and the pose of the descriptors.\n",
    "\n",
    "The details on how to create the database are described in the [build_database.ipynb](./build_database.ipynb) notebook.\n",
    "\n",
    "Note that the actual data are not required, as the pipeline will only load the `index.faiss` and the `track.csv` file. This can be useful in the real-world scenario, where the database size is too large to be stored on the local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = PlaceRecognitionPipeline(\n",
    "    database_dir=DATABASE_TRACK_DIR,\n",
    "    model=model,\n",
    "    model_weights_path=WEIGHTS_PATH,\n",
    "    device=DEVICE,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_data.keys() = dict_keys(['pointcloud_lidar_coords', 'pointcloud_lidar_feats'])\n",
      "sample_output.keys() = dict_keys(['idx', 'pose', 'descriptor'])\n",
      "sample_output['idx'] = 6\n",
      "pose = [ 6.40806188e-01 -6.96985360e+00 -2.62666965e+00  2.23428939e-03\n",
      "  1.21724469e-02  3.57296189e-01  9.33909135e-01]\n",
      "pose_gt = [-0.3476145  -7.5303183  -1.8204867  -0.01149435  0.01080806  0.3418056\n",
      "  0.9396382 ]\n",
      "dist_error = 1.3932074823071565, angle_error = 2.466076215796548\n"
     ]
    }
   ],
   "source": [
    "sample_data = query_dataset[5]\n",
    "sample_pose_gt = sample_data.pop(\"pose\")  # removing those keys are not necessary, we just\n",
    "sample_data.pop(\"idx\")                    # want to simulate that we pass the data without GT information :)\n",
    "print(f\"sample_data.keys() = {sample_data.keys()}\")\n",
    "sample_output = pipe.infer(sample_data)\n",
    "print(f\"sample_output.keys() = {sample_output.keys()}\")\n",
    "print(f\"sample_output['idx'] = {sample_output['idx']}\")\n",
    "print(f\"pose = {sample_output['pose']}\")\n",
    "print(f\"pose_gt = {sample_pose_gt.numpy()}\")\n",
    "dist_error, angle_error = compute_error(sample_output[\"pose\"], sample_pose_gt.numpy())\n",
    "print(f\"dist_error = {dist_error}, angle_error = {angle_error}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage example - Pointcloud Registration Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRACK_DIR = \"/home/docker_opr/Datasets/ITLP-Campus-data/subsampled_data/indoor/00_2023-10-25-night/floor_1\"\n",
    "\n",
    "REGISTRATION_MODEL_CONFIG_PATH = \"../configs/model/registration/geotransformer_kitti.yaml\"\n",
    "REGISTRATION_WEIGHTS_PATH = \"../weights/registration/geotransformer_kitti.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init dataset\n",
    "\n",
    "For the demonstration purpose, we will use the `opr.datasets.itlp.ITLPCampus` dataset class to load the data.\n",
    "\n",
    "We will instantiate only one track and test the registration performance by evaluating the transformation between the two consecutive frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ITLPCampus(\n",
    "    dataset_root=TRACK_DIR,\n",
    "    sensors=[\"lidar\"],\n",
    "    mink_quantization_size=0.5,\n",
    "    load_semantics=False,\n",
    "    load_text_descriptions=False,\n",
    "    load_text_labels=False,\n",
    "    load_aruco_labels=False,\n",
    "    indoor=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "geotransformer = instantiate(OmegaConf.load(REGISTRATION_MODEL_CONFIG_PATH))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "registration_pipe = PointcloudRegistrationPipeline(\n",
    "    model=geotransformer,\n",
    "    model_weights_path=REGISTRATION_WEIGHTS_PATH,\n",
    "    device=\"cuda\",  # the GeoTransformer currently only supports CUDA\n",
    "    voxel_downsample_size=0.3,  # recommended for geotransformer_kitti configuration\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gt_transformation = \n",
      "[[ 0.62815438  0.77747642 -0.03086264  0.95043122]\n",
      " [-0.77807516  0.62788344 -0.01901165 -0.29708926]\n",
      " [ 0.00459703  0.0359557   0.99934281  0.02541637]\n",
      " [ 0.          0.          0.          1.        ]]\n",
      "\n",
      "estimated_transformation = \n",
      "[[ 0.63717115  0.77047485 -0.01952516  1.1074791 ]\n",
      " [-0.77067107  0.6366326  -0.0276524  -0.06237316]\n",
      " [-0.00887516  0.03266682  0.9994269   0.0161792 ]\n",
      " [ 0.          0.          0.          1.        ]]\n",
      "\n",
      "Relative Rotation Error (RRE) = 1.039\n",
      "Relative Translation Error (RTE) = 0.283\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/docker_opr/OpenPlaceRecognition/third_party/GeoTransformer/geotransformer/modules/geotransformer/superpoint_matching.py:44: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  ref_sel_indices = corr_indices // matching_scores.shape[1]\n"
     ]
    }
   ],
   "source": [
    "i = 3\n",
    "db_pc = dataset[i-1][\"pointcloud_lidar_coords\"]\n",
    "query_pc = dataset[i][\"pointcloud_lidar_coords\"]\n",
    "db_pose = pose_to_matrix(dataset[i-1][\"pose\"])\n",
    "query_pose = pose_to_matrix(dataset[i][\"pose\"])\n",
    "# we want to find the transformation from the \"database\" pose to the \"query\" pose\n",
    "gt_transformation = np.linalg.inv(db_pose) @ query_pose\n",
    "\n",
    "estimated_transformation = registration_pipe.infer(query_pc, db_pc)\n",
    "\n",
    "print(f\"gt_transformation = \\n{gt_transformation}\\n\")\n",
    "print(f\"estimated_transformation = \\n{estimated_transformation}\\n\")\n",
    "\n",
    "rre, rte = compute_registration_error(gt_transformation, estimated_transformation)\n",
    "print(f\"Relative Rotation Error (RRE) = {rre:0.3f}\\nRelative Translation Error (RTE) = {rte:0.3f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gt_pose = \n",
      "[[ 0.61203326  0.78962221 -0.04372709  2.97850537]\n",
      " [-0.78974521  0.61315495  0.01853377 -0.18161125]\n",
      " [ 0.04144616  0.02318998  0.99887158  0.14517449]\n",
      " [ 0.          0.          0.          1.        ]]\n",
      "optimized_pose = \n",
      "[[ 0.62136691  0.78284234 -0.03257031  3.14041519]\n",
      " [-0.78302508  0.62191456  0.00967767  0.04942517]\n",
      " [ 0.027832    0.01949002  0.9994226   0.12941422]\n",
      " [ 0.          0.          0.          1.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"gt_pose = \\n{query_pose}\")\n",
    "print(f\"optimized_pose = \\n{db_pose @ estimated_transformation}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
