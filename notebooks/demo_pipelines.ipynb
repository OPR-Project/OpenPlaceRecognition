{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines subpackage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `opr.pipelines` subpackage contains ready-to-use pipelines for model inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage example - Place Recognition Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hydra.utils import instantiate\n",
    "import numpy as np\n",
    "from omegaconf import OmegaConf\n",
    "from scipy.spatial.transform import Rotation\n",
    "\n",
    "from opr.datasets.itlp import ITLPCampus\n",
    "from opr.pipelines.place_recognition import PlaceRecognitionPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATABASE_TRACK_DIR = \"/home/docker_opr/Datasets/ITLP-Campus-data/subsampled_data/outdoor/old/00_2023-02-10\"\n",
    "QUERY_TRACK_DIR = \"/home/docker_opr/Datasets/ITLP-Campus-data/subsampled_data/outdoor/old/01_2023-02-21\"\n",
    "\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "MODEL_CONFIG_PATH = \"../configs/model/place_recognition/minkloc3d.yaml\"\n",
    "WEIGHTS_PATH = \"../weights/place_recognition/minkloc3d_nclt.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pose_to_matrix(pose):\n",
    "    position = pose[:3]\n",
    "    orientation_quat = pose[3:]\n",
    "    rotation = Rotation.from_quat(orientation_quat)\n",
    "    pose_matrix = np.eye(4)\n",
    "    pose_matrix[:3,:3] = rotation.as_matrix()\n",
    "    pose_matrix[:3,3] = position\n",
    "    return pose_matrix\n",
    "\n",
    "\n",
    "def compute_error(estimated_pose, gt_pose):\n",
    "    estimated_pose = pose_to_matrix(estimated_pose)\n",
    "    gt_pose = pose_to_matrix(gt_pose)\n",
    "    error_pose = np.linalg.inv(estimated_pose) @ gt_pose\n",
    "    dist_error = np.sum(error_pose[:3, 3]**2) ** 0.5\n",
    "    r = Rotation.from_matrix(error_pose[:3, :3])\n",
    "    rotvec = r.as_rotvec()\n",
    "    angle_error = (np.sum(rotvec**2)**0.5) * 180 / np.pi\n",
    "    angle_error = abs(90 - abs(angle_error-90))\n",
    "    return dist_error, angle_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Init query dataset\n",
    "\n",
    "The pipeline infer method accepts an input in the format of dictionary with keys in the following format:\n",
    "- `\"image_{camera_name}\"` for images from cameras,\n",
    "- `\"mask_{camera_name}\"` for semantic segmentation masks,\n",
    "- `\"pointcloud_lidar_coords\"` for pointcloud coordinates from lidar,\n",
    "- `\"pointcloud_lidar_feats\"` for pointcloud features from lidar.\n",
    "\n",
    "The data type of all values are `torch.Tensor`.\n",
    "\n",
    "You can load and preprocess the data manually, but in this example we will use the `opr.datasets.itlp.ITLPCampus` ` dataset class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_dataset = ITLPCampus(\n",
    "    dataset_root=QUERY_TRACK_DIR,\n",
    "    sensors=[\"lidar\"],\n",
    "    mink_quantization_size=0.5,\n",
    "    load_semantics=False,\n",
    "    load_text_descriptions=False,\n",
    "    load_text_labels=False,\n",
    "    load_aruco_labels=False,\n",
    "    indoor=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize model\n",
    "\n",
    "We will use hydra's `instantiate` function to initialize the model. The model is a `MinkLoc3D` - a simple LiDAR-only architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = OmegaConf.load(MODEL_CONFIG_PATH)\n",
    "model = instantiate(model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The minimum requirement to initialize the `PlaceRecognitionPipeline` is that the database directory should contain the `index.faiss` file and the `track.csv` file.\n",
    "\n",
    "The `index.faiss` file is a Faiss index, which contains the descriptors of the database. The `track.csv` file contains the metadata of the database, including the id and the pose of the descriptors.\n",
    "\n",
    "The details on how to create the database are described in the [build_database.ipynb](./build_database.ipynb) notebook.\n",
    "\n",
    "Note that the actual data are not required, as the pipeline will only load the `index.faiss` and the `track.csv` file. This can be useful in the real-world scenario, where the database size is too large to be stored on the local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = PlaceRecognitionPipeline(\n",
    "    database_dir=DATABASE_TRACK_DIR,\n",
    "    model=model,\n",
    "    model_weights_path=WEIGHTS_PATH,\n",
    "    device=DEVICE,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_data.keys() = dict_keys(['pointcloud_lidar_coords', 'pointcloud_lidar_feats'])\n",
      "sample_output.keys() = dict_keys(['idx', 'pose', 'descriptor'])\n",
      "sample_output['idx'] = 6\n",
      "pose = [ 6.40806188e-01 -6.96985360e+00 -2.62666965e+00  2.23428939e-03\n",
      "  1.21724469e-02  3.57296189e-01  9.33909135e-01]\n",
      "pose_gt = [-0.3476145  -7.5303183  -1.8204867  -0.01149435  0.01080806  0.3418056\n",
      "  0.9396382 ]\n",
      "dist_error = 1.3932074823071565, angle_error = 2.466076215796548\n"
     ]
    }
   ],
   "source": [
    "sample_data = query_dataset[5]\n",
    "sample_pose_gt = sample_data.pop(\"pose\")  # removing those keys are not necessary, we just\n",
    "sample_data.pop(\"idx\")                    # want to simulate that we pass the data without GT information :)\n",
    "print(f\"sample_data.keys() = {sample_data.keys()}\")\n",
    "sample_output = pipe.infer(sample_data)\n",
    "print(f\"sample_output.keys() = {sample_output.keys()}\")\n",
    "print(f\"sample_output['idx'] = {sample_output['idx']}\")\n",
    "print(f\"pose = {sample_output['pose']}\")\n",
    "print(f\"pose_gt = {sample_pose_gt.numpy()}\")\n",
    "dist_error, angle_error = compute_error(sample_output[\"pose\"], sample_pose_gt.numpy())\n",
    "print(f\"dist_error = {dist_error}, angle_error = {angle_error}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
