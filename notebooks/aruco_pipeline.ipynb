{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines subpackage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `opr.pipelines` subpackage contains ready-to-use pipelines for model inference. In this tutorial we will examine how to build database and utilize`opr.pipelines.localization.ArucoLocalizationPipeline`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "import cv2\n",
    "from hydra.utils import instantiate\n",
    "import numpy as np\n",
    "from omegaconf import OmegaConf\n",
    "from scipy.spatial.transform import Rotation\n",
    "import matplotlib.pyplot as plt\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from geotransformer.utils.registration import compute_registration_error\n",
    "from geotransformer.utils.pointcloud import get_transform_from_rotation_translation\n",
    "\n",
    "from opr.datasets.itlp import ITLPCampus\n",
    "from opr.pipelines.localization import ArucoLocalizationPipeline\n",
    "from opr.pipelines.place_recognition import PlaceRecognitionPipeline\n",
    "from opr.pipelines.registration import PointcloudRegistrationPipeline\n",
    "\n",
    "\n",
    "def pose_to_matrix(pose):\n",
    "    \"\"\"From the 6D poses in the [tx ty tz qx qy qz qw] format to 4x4 pose matrices.\"\"\"\n",
    "    position = pose[:3]\n",
    "    orientation_quat = pose[3:]\n",
    "    rotation = Rotation.from_quat(orientation_quat)\n",
    "    pose_matrix = np.eye(4)\n",
    "    pose_matrix[:3,:3] = rotation.as_matrix()\n",
    "    pose_matrix[:3,3] = position\n",
    "    return pose_matrix\n",
    "\n",
    "def compute_error(estimated_pose, gt_pose):\n",
    "    \"\"\"For the 6D poses in the [tx ty tz qx qy qz qw] format.\"\"\"\n",
    "    estimated_pose = pose_to_matrix(estimated_pose)\n",
    "    gt_pose = pose_to_matrix(gt_pose)\n",
    "    error_pose = np.linalg.inv(estimated_pose) @ gt_pose\n",
    "    dist_error = np.sum(error_pose[:3, 3]**2) ** 0.5\n",
    "    r = Rotation.from_matrix(error_pose[:3, :3])\n",
    "    rotvec = r.as_rotvec()\n",
    "    angle_error = (np.sum(rotvec**2)**0.5) * 180 / np.pi\n",
    "    angle_error = abs(90 - abs(angle_error-90))\n",
    "    return dist_error, angle_error\n",
    "\n",
    "def compute_translation_error(gt_pose, pred_pose):\n",
    "    \"\"\"For the 4x4 pose matrices.\"\"\"\n",
    "    gt_trans = gt_pose[:3, 3]\n",
    "    pred_trans = pred_pose[:3, 3]\n",
    "    error = np.linalg.norm(gt_trans - pred_trans)\n",
    "    return error\n",
    "\n",
    "def compute_rotation_error(gt_pose, pred_pose):\n",
    "    \"\"\"For the 4x4 pose matrices.\"\"\"\n",
    "    gt_rot = Rotation.from_matrix(gt_pose[:3, :3])\n",
    "    pred_rot = Rotation.from_matrix(pred_pose[:3, :3])\n",
    "    error = Rotation.inv(gt_rot) * pred_rot\n",
    "    error = error.as_euler('xyz', degrees=True)\n",
    "    error = np.linalg.norm(error)\n",
    "    return error\n",
    "\n",
    "def compute_absolute_pose_error(gt_pose, pred_pose):\n",
    "    \"\"\"For the 4x4 pose matrices.\"\"\"\n",
    "    rotation_error = compute_rotation_error(gt_pose, pred_pose)\n",
    "    translation_error = compute_translation_error(gt_pose, pred_pose)\n",
    "    return rotation_error, translation_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage example - ArucoLocalizationPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weights download\n",
    "\n",
    "You can download the `minkloc3d_nclt.pth` from the HuggingFace model hub:\n",
    "https://huggingface.co/OPR-Project/PlaceRecognition-NCLT.\n",
    "\n",
    "```bash\n",
    "wget https://huggingface.co/OPR-Project/PlaceRecognition-NCLT/resolve/main/minkloc3d_nclt.pth\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset download\n",
    "\n",
    "You can download the dataset:\n",
    "\n",
    "- Kaggle:\n",
    "  - [ITLP Campus Indoor](https://www.kaggle.com/datasets/alexandermelekhin/itlp-campus-indoor)\n",
    "- Hugging Face:\n",
    "  - [ITLP Campus Indoor](https://huggingface.co/datasets/OPR-Project/ITLP-Campus-Indoor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to build the database\n",
    "\n",
    "The minimum requirement to initialize the `PlaceRecognitionPipeline` is that the database directory should contain the `index.faiss` file and the `track.csv` file.\n",
    "\n",
    "The `index.faiss` file is a Faiss index, which contains the descriptors of the database. The `track.csv` file contains the metadata of the database, including the id and the pose of the descriptors.\n",
    "\n",
    "The details on how to create the database are described in the [build_database.ipynb](./build_database.ipynb) notebook.\n",
    "\n",
    "Note that the actual data are not required, as the pipeline will only load the `index.faiss` and the `track.csv` file. This can be useful in the real-world scenario, where the database size is too large to be stored on the local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATABASE_DIR = \"/home/docker_opr/Datasets/OpenPlaceRecognition/itlp_campus_indoor/00_2023-10-25-night\"\n",
    "DATABASE_TRACK_DIR = \"/home/docker_opr/Datasets/OpenPlaceRecognition/itlp_campus_indoor/00_2023-10-25-night\"\n",
    "QUERY_TRACK_DIR = \"/home/docker_opr/Datasets/OpenPlaceRecognition/itlp_campus_indoor/01_2023-11-09-twilight\"\n",
    "\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "MODEL_CONFIG_PATH = \"../configs/model/place_recognition/minkloc3d.yaml\"\n",
    "WEIGHTS_PATH = \"../weights/place_recognition/minkloc3d_nclt.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DefaultImageTransform:\n",
    "    def __call__(self, img: np.ndarray):\n",
    "        \"\"\"Applies transformations to the given image.\n",
    "\n",
    "        Args:\n",
    "            img (np.ndarray): The image in the cv2 format.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Augmented PyTorch tensor in the channel-first format.\n",
    "        \"\"\"\n",
    "        return A.Compose([ToTensorV2()])(image=img)[\"image\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_dataset = ITLPCampus(\n",
    "    dataset_root=QUERY_TRACK_DIR,\n",
    "    sensors=[\"lidar\", \"front_cam\", \"back_cam\"],\n",
    "    mink_quantization_size=0.5,\n",
    "    load_semantics=False,\n",
    "    load_text_descriptions=False,\n",
    "    load_text_labels=False,\n",
    "    load_aruco_labels=False,\n",
    "    indoor=False,\n",
    "    image_transform=DefaultImageTransform()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_dataset[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(query_dataset[0][\"image_front_cam\"].permute(1, 2, 0).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(query_dataset[0][\"image_back_cam\"].permute(1, 2, 0).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = OmegaConf.load(MODEL_CONFIG_PATH)\n",
    "model = instantiate(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = PlaceRecognitionPipeline(\n",
    "    database_dir=DATABASE_DIR,\n",
    "    model=model,\n",
    "    model_weights_path=WEIGHTS_PATH,\n",
    "    device=DEVICE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGISTRATION_MODEL_CONFIG_PATH = \"../configs/model/registration/geotransformer_kitti.yaml\"\n",
    "REGISTRATION_WEIGHTS_PATH = \"../weights/registration/geotransformer_kitti.pth\"\n",
    "\n",
    "geotransformer = instantiate(OmegaConf.load(REGISTRATION_MODEL_CONFIG_PATH))\n",
    "\n",
    "registration_pipe = PointcloudRegistrationPipeline(\n",
    "    model=geotransformer,\n",
    "    model_weights_path=REGISTRATION_WEIGHTS_PATH,\n",
    "    device=\"cuda\",  # the GeoTransformer currently only supports CUDA\n",
    "    voxel_downsample_size=0.3,  # recommended for geotransformer_kitti configuration\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TF tree: world (localization frame) -> baselink (often is a robot \"center\") -> sensor (camera frame) -> aruco frame\n",
    "### Place recognition model return baselink2world transformation.\n",
    "### In our ArucoLocalizationPipeline we use sensor2baselink precalculated calibaration to find transformation between encoded in marker aruco pose\n",
    "### (in world frame) to robot current baselink.\n",
    "\n",
    "camera_metadata = {\n",
    "    \"front_cam_intrinsics\": [[683.6199340820312, 0.0, 615.1160278320312],\n",
    "                             [0.0, 683.6199340820312, 345.32354736328125],\n",
    "                             [0.0, 0.0, 1.0]],\n",
    "    \"front_cam_distortion\": [0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "    \"front_cam2baselink\": [-0.2388, 0.06, 0.75, -0.5, 0.49999999999755174, -0.5, 0.5000000000024483],\n",
    "    \"back_cam_intrinsics\": [[910.4178466796875, 0.0, 648.44140625],\n",
    "                            [0.0, 910.4166870117188, 354.0118408203125],\n",
    "                            [0.0, 0.0, 1.0]],\n",
    "    \"back_cam_distortion\": [0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "    \"back_cam2baselink\": [-0.3700594606670597, -0.006647301538708517, 0.7427924789987381, -0.4981412857230513, -0.4907829006275322, 0.5090864815669471, 0.5018149813673275]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aruco_metadata = {\n",
    "    \"aruco_type\": cv2.aruco.DICT_4X4_250,\n",
    "    \"aruco_size\": 0.2,\n",
    "    \"aruco_gt_pose_by_id\": {\n",
    "        11: [-1.82410266, -0.08805033, 1.0027102, 0.52255674, 0.54868202, 0.46970398, 0.45305703],\n",
    "        12: [-1.89103056, -0.16381068, 100.68857262, 0.5104185, 0.48182796, 0.47396991, 0.53166464],\n",
    "        13: [-1.65826485, -0.23034471, 200.85429952, 0.53442465, 0.45258975, 0.45886568, 0.54680444],\n",
    "        14: [-1.97089803, -0.128250737, 300.867689, 0.42405458, 0.59517182, 0.56236239, 0.38690667],\n",
    "        15: [-1.50259591, -0.0313240790, 400.699101, 0.45946361, 0.4809916,  0.50211729, 0.55264681]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = ITLPCampus(\n",
    "    dataset_root=DATABASE_TRACK_DIR,\n",
    "    sensors=[\"lidar\", \"front_cam\", \"back_cam\"],\n",
    "    mink_quantization_size=0.5,\n",
    "    load_semantics=False,\n",
    "    load_text_descriptions=False,\n",
    "    load_text_labels=False,\n",
    "    load_aruco_labels=False,\n",
    "    indoor=False,\n",
    "    image_transform=DefaultImageTransform()\n",
    ")\n",
    "\n",
    "pipe = ArucoLocalizationPipeline(\n",
    "    place_recognition_pipeline=pipe,\n",
    "    registration_pipeline=registration_pipe,\n",
    "    db_dataset=database,\n",
    "    aruco_metadata=aruco_metadata,\n",
    "    camera_metadata=camera_metadata\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = query_dataset[0]\n",
    "sample_pose_gt = sample_data.pop(\"pose\")  # removing those keys are not necessary, we just\n",
    "sample_data.pop(\"idx\")                    # want to simulate that we pass the data without GT information :)\n",
    "print(f\"sample_data.keys() = {sample_data.keys()}\")\n",
    "sample_output = pipe.infer(sample_data)\n",
    "print(f\"sample_output.keys() = {sample_output.keys()}\")\n",
    "\n",
    "print(\"---\")\n",
    "print(f\"sample_output['pose_by_aruco'] = {sample_output['pose_by_aruco']}\")\n",
    "print(f\"sample_output['pose_by_place_recognition'] = {sample_output['pose_by_place_recognition']}\")\n",
    "print(f\"pose_gt = {sample_pose_gt.numpy()}\")\n",
    "dist_error, angle_error = compute_error(sample_output['pose_by_aruco'], sample_pose_gt.numpy())\n",
    "print(f\"dist_error = {dist_error}, angle_error = {angle_error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "PR_MATCH_THRESHOLD = 25.0\n",
    "pr_matches = []\n",
    "rre_list = []\n",
    "rte_list = []\n",
    "times = []\n",
    "\n",
    "for sample in query_dataset:\n",
    "    gt_pose = sample.pop(\"pose\")\n",
    "    gt_pose = get_transform_from_rotation_translation(Rotation.from_quat(gt_pose[3:]).as_matrix(), gt_pose[:3])\n",
    "\n",
    "    start_time = time()\n",
    "    pipe_out = pipe.infer(sample)\n",
    "    times.append(time() - start_time)\n",
    "\n",
    "    estimated_pose = pipe_out[\"pose_by_place_recognition\"]\n",
    "    estimated_pose = get_transform_from_rotation_translation(Rotation.from_quat(estimated_pose[3:]).as_matrix(), estimated_pose[:3])\n",
    "\n",
    "    _, db_match_distance = compute_registration_error(gt_pose, estimated_pose)\n",
    "    pr_matched = db_match_distance <= PR_MATCH_THRESHOLD\n",
    "    pr_matches.append(pr_matched)\n",
    "\n",
    "    rre, rte = compute_registration_error(gt_pose, estimated_pose)\n",
    "    rre_list.append(rre)\n",
    "    rte_list.append(rte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"PlaceRecognition R@1 = {np.mean(pr_matches):0.3f}\")\n",
    "print(f\"Localization Mean RRE = {np.mean(rre_list):0.3f}\")\n",
    "print(f\"Localization Mean RTE = {np.mean(rte_list):0.3f}\")\n",
    "\n",
    "print(f\"Localization Median RRE = {np.median(rre_list):0.3f}\")\n",
    "print(f\"Localization Median RTE = {np.median(rte_list):0.3f}\")\n",
    "\n",
    "print(f\"Mean Time = {(np.mean(times) * 1000):0.2f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PlaceRecognition R@1 = 0.624\n",
    "\n",
    "Localization Mean RRE = 47.070\n",
    "\n",
    "Localization Mean RTE = 52.451\n",
    "\n",
    "Localization Median RRE = 4.961\n",
    "\n",
    "Localization Median RTE = 3.989\n",
    "\n",
    "Mean Time = 327.63 ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "PR_MATCH_THRESHOLD = 25.0\n",
    "pr_matches = []\n",
    "rre_list = []\n",
    "rte_list = []\n",
    "times = []\n",
    "\n",
    "for i, sample in enumerate(query_dataset):\n",
    "    print(f\"--- frame_{i} ---\")\n",
    "    gt_pose = sample.pop(\"pose\")\n",
    "    gt_pose = get_transform_from_rotation_translation(Rotation.from_quat(gt_pose[3:]).as_matrix(), gt_pose[:3])\n",
    "\n",
    "    start_time = time()\n",
    "    pipe_out = pipe.infer(sample)\n",
    "    times.append(time() - start_time)\n",
    "\n",
    "    estimated_pose = pipe_out[\"pose_by_aruco\"] if pipe_out[\"pose_by_aruco\"] is not None else pipe_out[\"pose_by_place_recognition\"]\n",
    "    estimated_pose = get_transform_from_rotation_translation(Rotation.from_quat(estimated_pose[3:]).as_matrix(), estimated_pose[:3])\n",
    "\n",
    "    _, db_match_distance = compute_registration_error(gt_pose, estimated_pose)\n",
    "    pr_matched = db_match_distance <= PR_MATCH_THRESHOLD\n",
    "    pr_matches.append(pr_matched)\n",
    "\n",
    "    rre, rte = compute_registration_error(gt_pose, estimated_pose)\n",
    "    rre_list.append(rre)\n",
    "    rte_list.append(rte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"ArucoPlaceRecognition R@1 = {np.mean(pr_matches):0.3f}\")\n",
    "print(f\"ArucoLocalization Mean RRE = {np.mean(rre_list):0.3f}\")\n",
    "print(f\"ArucoLocalization Mean RTE = {np.mean(rte_list):0.3f}\")\n",
    "\n",
    "print(f\"ArucoLocalization Median RRE = {np.median(rre_list):0.3f}\")\n",
    "print(f\"ArucoLocalization Median RTE = {np.median(rte_list):0.3f}\")\n",
    "\n",
    "print(f\"Mean Time = {(np.mean(times) * 1000):0.2f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ArucoPlaceRecognition R@1 = 0.630\n",
    "\n",
    "ArucoLocalization Mean RRE = 46.201\n",
    "\n",
    "ArucoLocalization Mean RTE = 51.554\n",
    "\n",
    "ArucoLocalization Median RRE = 5.019\n",
    "\n",
    "ArucoLocalization Median RTE = 3.692\n",
    "\n",
    "Mean Time = 325.66 ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aruco_detected_idx = [0, 1, 235, 236, 237, 238, 239, 240, 241, 485, 486, 487, 488, 489, 676, 726, 727, 988, 989, 990, 991, 992, 993, 1308, 1309]\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "PR_MATCH_THRESHOLD = 25.0\n",
    "pr_matches = []\n",
    "rre_list = []\n",
    "rte_list = []\n",
    "times = []\n",
    "\n",
    "for idx in aruco_detected_idx:\n",
    "    print(f\"--- frame_{idx} ---\")\n",
    "    sample = query_dataset[idx]\n",
    "    gt_pose = sample.pop(\"pose\")\n",
    "    gt_pose = get_transform_from_rotation_translation(Rotation.from_quat(gt_pose[3:]).as_matrix(), gt_pose[:3])\n",
    "\n",
    "    start_time = time()\n",
    "    pipe_out = pipe.infer(sample)\n",
    "    times.append(time() - start_time)\n",
    "\n",
    "    estimated_pose = pipe_out[\"pose_by_place_recognition\"]\n",
    "    estimated_pose = get_transform_from_rotation_translation(Rotation.from_quat(estimated_pose[3:]).as_matrix(), estimated_pose[:3])\n",
    "    \n",
    "    _, db_match_distance = compute_registration_error(gt_pose, estimated_pose)\n",
    "    pr_matched = db_match_distance <= PR_MATCH_THRESHOLD\n",
    "    pr_matches.append(pr_matched)\n",
    "\n",
    "    rre, rte = compute_registration_error(gt_pose, estimated_pose)\n",
    "    print(rre, rte)\n",
    "    rre_list.append(rre)\n",
    "    rte_list.append(rte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"PlaceRecognition R@1 = {np.mean(pr_matches):0.3f}\")\n",
    "print(f\"Localization Mean RRE = {np.mean(rre_list):0.3f}\")\n",
    "print(f\"Localization Mean RTE = {np.mean(rte_list):0.3f}\")\n",
    "\n",
    "print(f\"Localization Median RRE = {np.median(rre_list):0.3f}\")\n",
    "print(f\"Localization Median RTE = {np.median(rte_list):0.3f}\")\n",
    "\n",
    "print(f\"Mean Time = {(np.mean(times) * 1000):0.2f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PlaceRecognition R@1 = 0.640\n",
    "\n",
    "Localization Mean RRE = 61.577\n",
    "\n",
    "Localization Mean RTE = 51.851\n",
    "\n",
    "Localization Median RRE = 5.819\n",
    "\n",
    "Localization Median RTE = 0.991\n",
    "\n",
    "Mean Time = 349.67 ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aruco_detected_idx = [0, 1, 235, 236, 237, 238, 239, 240, 241, 485, 486, 487, 488, 489, 676, 726, 727, 988, 989, 990, 991, 992, 993, 1308, 1309]\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "PR_MATCH_THRESHOLD = 25.0\n",
    "pr_matches = []\n",
    "rre_list = []\n",
    "rte_list = []\n",
    "times = []\n",
    "\n",
    "for idx in aruco_detected_idx:\n",
    "    print(f\"--- frame_{idx} ---\")\n",
    "    sample = query_dataset[idx]\n",
    "    gt_pose = sample.pop(\"pose\")\n",
    "    gt_pose = get_transform_from_rotation_translation(Rotation.from_quat(gt_pose[3:]).as_matrix(), gt_pose[:3])\n",
    "\n",
    "    start_time = time()\n",
    "    pipe_out = pipe.infer(sample)\n",
    "    times.append(time() - start_time)\n",
    "\n",
    "    estimated_pose = pipe_out[\"pose_by_aruco\"] if pipe_out[\"pose_by_aruco\"] is not None else pipe_out[\"pose_by_place_recognition\"]\n",
    "    estimated_pose = get_transform_from_rotation_translation(Rotation.from_quat(estimated_pose[3:]).as_matrix(), estimated_pose[:3])\n",
    "\n",
    "    _, db_match_distance = compute_registration_error(gt_pose, estimated_pose)\n",
    "    pr_matched = db_match_distance <= PR_MATCH_THRESHOLD\n",
    "    pr_matches.append(pr_matched)\n",
    "\n",
    "    rre, rte = compute_registration_error(gt_pose, estimated_pose)\n",
    "    print(rre, rte)\n",
    "    rre_list.append(rre)\n",
    "    rte_list.append(rte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"ArcuoPlaceRecognition R@1 = {np.mean(pr_matches):0.3f}\")\n",
    "print(f\"ArucoLocalization Mean RRE = {np.mean(rre_list):0.3f}\")\n",
    "print(f\"ArucoLocalization Mean RTE = {np.mean(rte_list):0.3f}\")\n",
    "\n",
    "print(f\"ArucoLocalization Median RRE = {np.median(rre_list):0.3f}\")\n",
    "print(f\"ArucoLocalization Median RTE = {np.median(rte_list):0.3f}\")\n",
    "\n",
    "print(f\"Mean Time = {(np.mean(times) * 1000):0.2f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ArcuoPlaceRecognition R@1 = 0.960\n",
    "\n",
    "ArucoLocalization Mean RRE = 14.824\n",
    "\n",
    "ArucoLocalization Mean RTE = 4.789\n",
    "\n",
    "ArucoLocalization Median RRE = 10.509\n",
    "\n",
    "ArucoLocalization Median RTE = 0.676\n",
    "\n",
    "Mean Time = 363.80 ms"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
