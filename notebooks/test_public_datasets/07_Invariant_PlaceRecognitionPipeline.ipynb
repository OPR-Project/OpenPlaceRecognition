{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PlaceRecognitionPipeline invariant to weather changes\n",
    "\n",
    "A module that implements an algorithm for generating global vector representations of multimodal data outdoors, invariant to changes in weather conditions and seasons.\n",
    "\n",
    "This invariance is achieved by using semantic segmentation masks and highlighting special elements of the scene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "from time import time\n",
    "from pathlib import Path\n",
    "\n",
    "import faiss\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from hydra.utils import instantiate\n",
    "import numpy as np\n",
    "from omegaconf import OmegaConf\n",
    "from scipy.spatial.transform import Rotation\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from opr.datasets import NCLTDataset\n",
    "from opr.pipelines.place_recognition import PlaceRecognitionPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pose_to_matrix(pose):\n",
    "    \"\"\"From the 6D poses in the [tx ty tz qx qy qz qw] format to 4x4 pose matrices.\"\"\"\n",
    "    position = pose[:3]\n",
    "    orientation_quat = pose[3:]\n",
    "    rotation = Rotation.from_quat(orientation_quat)\n",
    "    pose_matrix = np.eye(4)\n",
    "    pose_matrix[:3,:3] = rotation.as_matrix()\n",
    "    pose_matrix[:3,3] = position\n",
    "    return pose_matrix\n",
    "\n",
    "\n",
    "def compute_error(estimated_pose, gt_pose):\n",
    "    \"\"\"For the 6D poses in the [tx ty tz qx qy qz qw] format.\"\"\"\n",
    "    estimated_pose = pose_to_matrix(estimated_pose)\n",
    "    gt_pose = pose_to_matrix(gt_pose)\n",
    "    error_pose = np.linalg.inv(estimated_pose) @ gt_pose\n",
    "    dist_error = np.sum(error_pose[:3, 3]**2) ** 0.5\n",
    "    r = Rotation.from_matrix(error_pose[:3, :3])\n",
    "    rotvec = r.as_rotvec()\n",
    "    angle_error = (np.sum(rotvec**2)**0.5) * 180 / np.pi\n",
    "    angle_error = abs(90 - abs(angle_error-90))\n",
    "    return dist_error, angle_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can **download the dataset**:\n",
    "\n",
    "- Kaggle:\n",
    "  - [NCLT_OpenPlaceRecognition](https://www.kaggle.com/datasets/creatorofuniverses/nclt-iprofi-hack-23)\n",
    "- Hugging Face:\n",
    "  - [NCLT_OpenPlaceRecognition](https://huggingface.co/datasets/OPR-Project/NCLT_OpenPlaceRecognition)\n",
    "\n",
    "To **download the model weights**, run the following command:\n",
    "\n",
    "```bash\n",
    "wget -O ../../weights/place_recognition/multimodal_semantic_with_soc_outdoor_nclt.pth https://huggingface.co/OPR-Project/PlaceRecognition-NCLT/resolve/main/multimodal_semantic_with_soc_outdoor_nclt.pth\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-04-18 15:01:30--  https://huggingface.co/OPR-Project/PlaceRecognition-NCLT/resolve/main/multimodal_semantic_with_soc_outdoor_nclt.pth\n",
      "Resolving huggingface.co (huggingface.co)... 18.239.50.49, 18.239.50.16, 18.239.50.103, ...\n",
      "Connecting to huggingface.co (huggingface.co)|18.239.50.49|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://cdn-lfs-us-1.hf.co/repos/e8/30/e8306844a097b119f688c0cfcf564a9f584f52c28b0d3c5b11e560cb0c3e7eeb/36a58cec8b5434ade952afb95b565f8dee8e126cb24e36403648acbf2dda7bf6?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27multimodal_semantic_with_soc_outdoor_nclt.pth%3B+filename%3D%22multimodal_semantic_with_soc_outdoor_nclt.pth%22%3B&Expires=1744992090&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0NDk5MjA5MH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zL2U4LzMwL2U4MzA2ODQ0YTA5N2IxMTlmNjg4YzBjZmNmNTY0YTlmNTg0ZjUyYzI4YjBkM2M1YjExZTU2MGNiMGMzZTdlZWIvMzZhNThjZWM4YjU0MzRhZGU5NTJhZmI5NWI1NjVmOGRlZThlMTI2Y2IyNGUzNjQwMzY0OGFjYmYyZGRhN2JmNj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=nIgolNZVBsB3ltEVGIdOIQxhkLsxg8DKVdoIrIPB0bduyQ3gC0vIbKMyqzeOHgcbNIg%7E8a%7EqpmN6XB-AbspearrbFQ-rrWZ5-6C2eRvq1nAspj37boSUernBPvzzPli3dLtxO3St5odno8i6P2NGRXt5hn0fbcgLuOWidtFiG0g-yEaKe92cjhDGbcj1MdHHNyrkDlE5Mx80QTjudxMkXM8uRUf21hMEVGRr5M5vs0gOYLm5hrPuAwV9Os0erlif8AJq0t%7Ejsuu1dmLeUNIcsrSAPqLkjShom3b%7Er6KzS%7EVR9WxwG3KJzHMFyZYt-WlUpwFzEYTkyMldReg2lAd3-Q__&Key-Pair-Id=K24J24Z295AEI9 [following]\n",
      "--2025-04-18 15:01:30--  https://cdn-lfs-us-1.hf.co/repos/e8/30/e8306844a097b119f688c0cfcf564a9f584f52c28b0d3c5b11e560cb0c3e7eeb/36a58cec8b5434ade952afb95b565f8dee8e126cb24e36403648acbf2dda7bf6?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27multimodal_semantic_with_soc_outdoor_nclt.pth%3B+filename%3D%22multimodal_semantic_with_soc_outdoor_nclt.pth%22%3B&Expires=1744992090&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0NDk5MjA5MH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zL2U4LzMwL2U4MzA2ODQ0YTA5N2IxMTlmNjg4YzBjZmNmNTY0YTlmNTg0ZjUyYzI4YjBkM2M1YjExZTU2MGNiMGMzZTdlZWIvMzZhNThjZWM4YjU0MzRhZGU5NTJhZmI5NWI1NjVmOGRlZThlMTI2Y2IyNGUzNjQwMzY0OGFjYmYyZGRhN2JmNj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=nIgolNZVBsB3ltEVGIdOIQxhkLsxg8DKVdoIrIPB0bduyQ3gC0vIbKMyqzeOHgcbNIg%7E8a%7EqpmN6XB-AbspearrbFQ-rrWZ5-6C2eRvq1nAspj37boSUernBPvzzPli3dLtxO3St5odno8i6P2NGRXt5hn0fbcgLuOWidtFiG0g-yEaKe92cjhDGbcj1MdHHNyrkDlE5Mx80QTjudxMkXM8uRUf21hMEVGRr5M5vs0gOYLm5hrPuAwV9Os0erlif8AJq0t%7Ejsuu1dmLeUNIcsrSAPqLkjShom3b%7Er6KzS%7EVR9WxwG3KJzHMFyZYt-WlUpwFzEYTkyMldReg2lAd3-Q__&Key-Pair-Id=K24J24Z295AEI9\n",
      "Resolving cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)... 65.9.86.113, 65.9.86.109, 65.9.86.32, ...\n",
      "Connecting to cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)|65.9.86.113|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 34069178 (32M) [binary/octet-stream]\n",
      "Saving to: ‘../../weights/place_recognition/multimodal_semantic_with_soc_outdoor_nclt.pth’\n",
      "\n",
      "../../weights/place 100%[===================>]  32.49M  4.79MB/s    in 6.9s    \n",
      "\n",
      "2025-04-18 15:01:38 (4.68 MB/s) - ‘../../weights/place_recognition/multimodal_semantic_with_soc_outdoor_nclt.pth’ saved [34069178/34069178]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -O ../../weights/place_recognition/multimodal_semantic_with_soc_outdoor_nclt.pth https://huggingface.co/OPR-Project/PlaceRecognition-NCLT/resolve/main/multimodal_semantic_with_soc_outdoor_nclt.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_ROOT = \"/home/docker_opr/Datasets/OpenPlaceRecognition/NCLT_preprocessed\"  # change to your dataset path\n",
    "\n",
    "SEMANTIC_ANNO = \"/home/docker_opr/OpenPlaceRecognition/configs/dataset/anno/oneformer.yaml\"\n",
    "\n",
    "SENSOR_SUITE = [\"image_Cam5\", \"image_Cam2\", \"mask_Cam5\", \"mask_Cam2\", \"pointcloud_lidar\"]\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 4\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "MODEL_CONFIG_PATH = \"../../configs/model/place_recognition/multimodal_semantic_with_soc_outdoor.yaml\"\n",
    "WEIGHTS_PATH = \"../../weights/place_recognition/multimodal_semantic_with_soc_outdoor_nclt.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 tracks\n",
      "['2012-01-08', '2012-01-22', '2012-02-12', '2012-02-18', '2012-03-31', '2012-05-26', '2012-08-04', '2012-10-28', '2012-11-04', '2012-12-01']\n",
      "WARNING: track list limited\n",
      "['2012-01-08', '2012-01-22']\n"
     ]
    }
   ],
   "source": [
    "TRACK_LIST = sorted([str(subdir.name) for subdir in Path(DATASET_ROOT).iterdir() if subdir.is_dir()])\n",
    "print(f\"Found {len(TRACK_LIST)} tracks\")\n",
    "print(TRACK_LIST)\n",
    "print(\"WARNING: track list limited\")\n",
    "TRACK_LIST = TRACK_LIST[:2]\n",
    "print(TRACK_LIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-18 15:02:11.229\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mopr.models.place_recognition.pointmamba\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m16\u001b[0m - \u001b[33m\u001b[1mThe 'pointmamba' package is not installed. Please install it manually if neccessary.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_config = OmegaConf.load(MODEL_CONFIG_PATH)\n",
    "model = instantiate(model_config)\n",
    "model.load_state_dict(torch.load(WEIGHTS_PATH))\n",
    "model = model.to(DEVICE)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate descriptors for databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_anno_cfg = OmegaConf.load(SEMANTIC_ANNO)\n",
    "dataset = NCLTDataset(\n",
    "    dataset_root=DATASET_ROOT,\n",
    "    subset=\"test\",\n",
    "    data_to_load=SENSOR_SUITE,\n",
    "    pointcloud_quantization_size=0.5,\n",
    "    max_point_distance=None,\n",
    "    load_soc=True,\n",
    "    anno=semantic_anno_cfg,\n",
    "    top_k_soc=5,\n",
    ")\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    collate_fn=dataset.collate_fn,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/86 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 86/86 [01:58<00:00,  1.38s/it]\n"
     ]
    }
   ],
   "source": [
    "descriptors = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(dataloader):\n",
    "        batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "        final_descriptor = model(batch)[\"final_descriptor\"]\n",
    "        descriptors.append(final_descriptor.detach().cpu().numpy())\n",
    "\n",
    "descriptors = np.concatenate(descriptors, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved index /home/docker_opr/Datasets/OpenPlaceRecognition/NCLT_preprocessed/2012-01-08/index.faiss\n",
      "Saved index /home/docker_opr/Datasets/OpenPlaceRecognition/NCLT_preprocessed/2012-01-22/index.faiss\n",
      "Saved index /home/docker_opr/Datasets/OpenPlaceRecognition/NCLT_preprocessed/2012-02-12/index.faiss\n",
      "Saved index /home/docker_opr/Datasets/OpenPlaceRecognition/NCLT_preprocessed/2012-02-18/index.faiss\n",
      "Saved index /home/docker_opr/Datasets/OpenPlaceRecognition/NCLT_preprocessed/2012-03-31/index.faiss\n",
      "Saved index /home/docker_opr/Datasets/OpenPlaceRecognition/NCLT_preprocessed/2012-05-26/index.faiss\n",
      "Saved index /home/docker_opr/Datasets/OpenPlaceRecognition/NCLT_preprocessed/2012-08-04/index.faiss\n",
      "Saved index /home/docker_opr/Datasets/OpenPlaceRecognition/NCLT_preprocessed/2012-10-28/index.faiss\n",
      "Saved index /home/docker_opr/Datasets/OpenPlaceRecognition/NCLT_preprocessed/2012-11-04/index.faiss\n",
      "Saved index /home/docker_opr/Datasets/OpenPlaceRecognition/NCLT_preprocessed/2012-12-01/index.faiss\n"
     ]
    }
   ],
   "source": [
    "dataset_df = dataset.dataset_df\n",
    "\n",
    "for track, indices in dataset_df.groupby(\"track\").groups.items():\n",
    "    track_descriptors = descriptors[indices]\n",
    "    track_index = faiss.IndexFlatL2(track_descriptors.shape[1])\n",
    "    track_index.add(track_descriptors)\n",
    "    faiss.write_index(track_index, f\"{DATASET_ROOT}/{track}/index.faiss\")\n",
    "    print(f\"Saved index {DATASET_ROOT}/{track}/index.faiss\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "275it [00:39,  7.03it/s]\n",
      "331it [00:43,  7.56it/s]\n"
     ]
    }
   ],
   "source": [
    "PR_THRESHOLD = 25.0\n",
    "\n",
    "test_csv = pd.read_csv(Path(DATASET_ROOT) / \"test.csv\", index_col=0)\n",
    "\n",
    "all_recalls = []\n",
    "all_mean_dist_errors = []\n",
    "all_mean_angle_errors = []\n",
    "all_median_dist_errors = []\n",
    "all_median_angle_errors = []\n",
    "all_times = []\n",
    "\n",
    "for db_track in TRACK_LIST:\n",
    "    pipe = PlaceRecognitionPipeline(\n",
    "        database_dir=Path(DATASET_ROOT) / db_track,\n",
    "        model=model,\n",
    "        device=DEVICE,\n",
    "    )\n",
    "    for query_track in TRACK_LIST:\n",
    "        if db_track == query_track:\n",
    "            continue\n",
    "        query_dataset = copy(dataset)\n",
    "        query_dataset.dataset_df = query_dataset.dataset_df[query_dataset.dataset_df[\"track\"] == query_track]\n",
    "        query_df = pd.read_csv(Path(DATASET_ROOT) / query_track / \"track.csv\", index_col=0)\n",
    "\n",
    "        # filter out only test subset\n",
    "        query_df = query_df[query_df['image'].isin(query_dataset.dataset_df['image'])].reset_index(drop=True)\n",
    "        # and do not forget to change the database_df in the pipeline\n",
    "        pipe.database_df = pipe.database_df[pipe.database_df['image'].isin(test_csv['image'])].reset_index(drop=True)\n",
    "\n",
    "        pr_matches = []\n",
    "        dist_errors = []\n",
    "        angle_errors = []\n",
    "        times = []\n",
    "\n",
    "        true_pairs = []\n",
    "        false_pairs = []\n",
    "\n",
    "        for q_i, query in tqdm(enumerate(query_dataset)):\n",
    "            query[\"pose\"] = query_df.iloc[q_i][[\"tx\", \"ty\", \"tz\", \"qx\", \"qy\", \"qz\", \"qw\"]].to_numpy()\n",
    "            t = time()\n",
    "            output = pipe.infer(query)\n",
    "            times.append(time() - t)\n",
    "            dist_error, angle_error = compute_error(output[\"pose\"], query[\"pose\"])\n",
    "            pr_matches.append(dist_error < PR_THRESHOLD)\n",
    "            dist_errors.append(dist_error)\n",
    "            angle_errors.append(angle_error)\n",
    "            if dist_error < 10:\n",
    "                true_pairs.append((q_i, output[\"idx\"]))\n",
    "            elif dist_error > 100:\n",
    "                false_pairs.append((q_i, output[\"idx\"]))\n",
    "\n",
    "        all_recalls.append(np.mean(pr_matches))\n",
    "        all_mean_dist_errors.append(np.mean(dist_errors))\n",
    "        all_mean_angle_errors.append(np.mean(angle_errors))\n",
    "        all_median_dist_errors.append(np.median(dist_errors))\n",
    "        all_median_angle_errors.append(np.median(angle_errors))\n",
    "        all_times.extend(times[1:]) # drop the first iteration cause it is always slower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9313540236198847"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(all_recalls).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.34987849526576"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(all_mean_dist_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.616232466285878"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(all_median_dist_errors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_str = f\"\"\"Average Recall@1: {np.mean(all_recalls)*100:.2f}\n",
    "Average mean dist error: {np.mean(all_mean_dist_errors):.2f}\n",
    "Average mean angle error: {np.mean(all_mean_angle_errors):.2f}\n",
    "Average median dist error: {np.mean(all_median_dist_errors):.2f}\n",
    "Average median angle error: {np.mean(all_median_angle_errors):.2f}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Recall@1: 93.14\n",
      "Average mean dist error: 11.35\n",
      "Average mean angle error: 11.52\n",
      "Average median dist error: 4.62\n",
      "Average median angle error: 6.39\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(results_str)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
