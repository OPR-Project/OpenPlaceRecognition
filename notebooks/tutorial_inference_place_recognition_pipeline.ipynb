{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b171e01a",
   "metadata": {},
   "source": [
    "# Tutorial on `opr.inference.pipelines.place_recognition_pipeline` subpackage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9aef3d",
   "metadata": {},
   "source": [
    "This tutorial demonstrates the new top‑k Place Recognition pipeline that uses the `Index` module.\n",
    "\n",
    "You will:\n",
    "- Build a tiny on‑disk database index: `descriptors.npy`, `meta.parquet` (with `idx`, `pose[7]`), `schema.json`.\n",
    "- Create a stub model that returns a descriptor (as if it was computed by a real network).\n",
    "- Load the FAISS Flat index, run a top‑k search via `PlaceRecognitionPipeline`, and interpret the outputs.\n",
    "\n",
    "Requirements:\n",
    "- `faiss` (faiss‑cpu or faiss‑gpu)\n",
    "- `pandas` + a Parquet engine: `pyarrow` (recommended) or `fastparquet`\n",
    "- `torch`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9029b9a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment looks good.\n"
     ]
    }
   ],
   "source": [
    "# Install/check requirements (optional)\n",
    "import importlib\n",
    "missing = []\n",
    "for pkg in (\"faiss\", \"pandas\", \"numpy\", \"pyarrow\", \"torch\"):\n",
    "    if importlib.util.find_spec(pkg) is None:\n",
    "        missing.append(pkg)\n",
    "if missing:\n",
    "    print(\"Missing packages:\", \", \".join(missing))\n",
    "    print(\"Install with: pip install faiss-cpu pyarrow pandas numpy torch\")\n",
    "else:\n",
    "    print(\"Environment looks good.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c35d15c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: ['meta.parquet', 'descriptors.npy', 'schema.json']\n"
     ]
    }
   ],
   "source": [
    "# Prepare a tiny database index\n",
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "base = Path(\"./_demo_index_pr\").resolve()\n",
    "base.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "N, D = 10, 8\n",
    "rng = np.random.default_rng(1)\n",
    "descriptors = rng.normal(size=(N, D)).astype(np.float32)\n",
    "poses = [[float(i), float(i+1), float(i+2), 0.0, 0.0, 0.0, 1.0] for i in range(N)]\n",
    "meta = pd.DataFrame({\"idx\": np.arange(1000, 1000+N, dtype=np.int64), \"pose\": poses})\n",
    "\n",
    "np.save(base / \"descriptors.npy\", descriptors)\n",
    "meta.to_parquet(base / \"meta.parquet\")\n",
    "schema = {\"version\": \"1\", \"dim\": D, \"metric\": \"l2\", \"created_at\": \"\", \"opr_version\": \"\"}\n",
    "(base / \"schema.json\").write_text(json.dumps(schema))\n",
    "\n",
    "print(f\"Wrote: {[p.name for p in base.iterdir()]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa4ce555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stub model ready, descriptor norm: 2.0769312381744385\n"
     ]
    }
   ],
   "source": [
    "# Create a stub model that returns a query descriptor\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class StubModel(nn.Module):\n",
    "    def __init__(self, descriptor: np.ndarray):\n",
    "        super().__init__()\n",
    "        self.register_buffer(\"_desc\", torch.from_numpy(descriptor.astype(np.float32, copy=False)))\n",
    "    def forward(self, _: dict[str, torch.Tensor]):\n",
    "        return {\"final_descriptor\": self._desc.unsqueeze(0)}\n",
    "\n",
    "query_desc = descriptors[0] + 0.02\n",
    "model = StubModel(query_desc)\n",
    "print(f\"Stub model ready, descriptor norm: {float(torch.linalg.norm(model._desc))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8137d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load index and run PlaceRecognitionPipeline\n",
    "from opr.inference.index import FaissFlatIndex\n",
    "from opr.inference.pipelines import PlaceRecognitionPipeline\n",
    "\n",
    "index = FaissFlatIndex.load(base)\n",
    "pipeline = PlaceRecognitionPipeline(index=index, model=model, device=\"cpu\")\n",
    "\n",
    "result = pipeline.infer(input_data={}, k=5)\n",
    "print(\"descriptor shape:\", result.descriptor.shape)\n",
    "print(\"indices:\", result.indices.tolist())\n",
    "print(\"distances:\", [float(x) for x in result.distances])\n",
    "print(\"db_idx:\", result.db_idx.tolist())\n",
    "print(\"db_pose (first):\", result.db_pose[0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
