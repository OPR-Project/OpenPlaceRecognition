{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines subpackage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `opr.pipelines` subpackage contains ready-to-use pipelines for model inference. In this tutorial we will examine how to build database and utilize `opr.pipelines.localization.LocalizationPipeline` with dynamic objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dynamic objects aware database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import faiss\n",
    "from hydra.utils import instantiate\n",
    "import numpy as np\n",
    "from omegaconf import OmegaConf\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from opr.datasets.itlp import ITLPCampus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATABASE_TRACK_DIR = \"/home/docker_opr/Datasets/ITLP-Campus-data/subsampled_data/indoor/00_2023-10-25-night/floor_1/\"\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "NUM_WORKERS = 4\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "MODEL_CONFIG_PATH = \"../configs/model/place_recognition/minkloc3d.yaml\"\n",
    "WEIGHTS_PATH = \"../weights/place_recognition/minkloc3d_nclt.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from albumentations.pytorch import ToTensorV2\n",
    "import albumentations as A\n",
    "\n",
    "class ToTensorTransform:\n",
    "    def __init__(self):\n",
    "        transform_list = [ToTensorV2()]\n",
    "        self.transform = A.Compose(transform_list)\n",
    "\n",
    "    def __call__(self, img: np.ndarray):\n",
    "        \"\"\"Applies transformations to the given image.\"\"\"\n",
    "        return self.transform(image=img)[\"image\"]\n",
    "\n",
    "db_dataset = ITLPCampus(\n",
    "    dataset_root=DATABASE_TRACK_DIR,\n",
    "    sensors=[\"lidar\", \"back_cam\", \"front_cam\"],\n",
    "    mink_quantization_size=0.5,\n",
    "    load_semantics=True,\n",
    "    exclude_dynamic_classes=True,\n",
    "    indoor=True,\n",
    "    semantic_transform=ToTensorTransform(),\n",
    "    image_transform=ToTensorTransform()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_dataset[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_dataloader = DataLoader(\n",
    "    db_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    collate_fn=db_dataset.collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = OmegaConf.load(MODEL_CONFIG_PATH)\n",
    "model = instantiate(model_config)\n",
    "model.load_state_dict(torch.load(WEIGHTS_PATH))\n",
    "model = model.to(DEVICE)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptors = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(db_dataloader):\n",
    "        batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "        final_descriptor = model(batch)[\"final_descriptor\"]\n",
    "        descriptors.append(final_descriptor.detach().cpu().numpy())\n",
    "\n",
    "descriptors = np.concatenate(descriptors, axis=0)\n",
    "descriptors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.IndexFlatL2(descriptors.shape[1])\n",
    "index.add(descriptors)\n",
    "print(index.is_trained)\n",
    "print(index.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss.write_index(index, \"index.faiss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from hydra.utils import instantiate\n",
    "import numpy as np\n",
    "from omegaconf import OmegaConf\n",
    "from scipy.spatial.transform import Rotation\n",
    "\n",
    "try:\n",
    "    from geotransformer.utils.registration import compute_registration_error\n",
    "    from geotransformer.utils.pointcloud import get_transform_from_rotation_translation\n",
    "except ImportError:\n",
    "    print(\"WARNIGN: geotransformer not installed, registration error will not be computed\")\n",
    "\n",
    "from opr.datasets.itlp import ITLPCampus\n",
    "from opr.pipelines.place_recognition import PlaceRecognitionPipeline\n",
    "from opr.pipelines.registration import PointcloudRegistrationPipeline\n",
    "from opr.pipelines.localization import LocalizationPipeline\n",
    "\n",
    "\n",
    "def pose_to_matrix(pose):\n",
    "    \"\"\"From the 6D poses in the [tx ty tz qx qy qz qw] format to 4x4 pose matrices.\"\"\"\n",
    "    position = pose[:3]\n",
    "    orientation_quat = pose[3:]\n",
    "    rotation = Rotation.from_quat(orientation_quat)\n",
    "    pose_matrix = np.eye(4)\n",
    "    pose_matrix[:3,:3] = rotation.as_matrix()\n",
    "    pose_matrix[:3,3] = position\n",
    "    return pose_matrix\n",
    "\n",
    "\n",
    "def compute_error(estimated_pose, gt_pose):\n",
    "    \"\"\"For the 6D poses in the [tx ty tz qx qy qz qw] format.\"\"\"\n",
    "    estimated_pose = pose_to_matrix(estimated_pose)\n",
    "    gt_pose = pose_to_matrix(gt_pose)\n",
    "    error_pose = np.linalg.inv(estimated_pose) @ gt_pose\n",
    "    dist_error = np.sum(error_pose[:3, 3]**2) ** 0.5\n",
    "    r = Rotation.from_matrix(error_pose[:3, :3])\n",
    "    rotvec = r.as_rotvec()\n",
    "    angle_error = (np.sum(rotvec**2)**0.5) * 180 / np.pi\n",
    "    angle_error = abs(90 - abs(angle_error-90))\n",
    "    return dist_error, angle_error\n",
    "\n",
    "def compute_translation_error(gt_pose, pred_pose):\n",
    "    \"\"\"For the 4x4 pose matrices.\"\"\"\n",
    "    gt_trans = gt_pose[:3, 3]\n",
    "    pred_trans = pred_pose[:3, 3]\n",
    "    error = np.linalg.norm(gt_trans - pred_trans)\n",
    "    return error\n",
    "\n",
    "def compute_rotation_error(gt_pose, pred_pose):\n",
    "    \"\"\"For the 4x4 pose matrices.\"\"\"\n",
    "    gt_rot = Rotation.from_matrix(gt_pose[:3, :3])\n",
    "    pred_rot = Rotation.from_matrix(pred_pose[:3, :3])\n",
    "    error = Rotation.inv(gt_rot) * pred_rot\n",
    "    error = error.as_euler('xyz', degrees=True)\n",
    "    error = np.linalg.norm(error)\n",
    "    return error\n",
    "\n",
    "def compute_absolute_pose_error(gt_pose, pred_pose):\n",
    "    \"\"\"For the 4x4 pose matrices.\"\"\"\n",
    "    rotation_error = compute_rotation_error(gt_pose, pred_pose)\n",
    "    translation_error = compute_translation_error(gt_pose, pred_pose)\n",
    "    return rotation_error, translation_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage example - Place Recognition Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATABASE_DIR = \"/home/docker_opr/Datasets/ITLP-Campus-sample-databases-dynamic/indoor/00_2023-10-25-night/floor_1\"\n",
    "DATABASE_TRACK_DIR = \"/home/docker_opr/Datasets/ITLP-Campus-data/subsampled_data/indoor/00_2023-10-25-night/floor_1\"\n",
    "QUERY_TRACK_DIR = \"/home/docker_opr/Datasets/ITLP-Campus-data/subsampled_data/indoor/01_2023-11-09-twilight/floor_1\"\n",
    "\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "MODEL_CONFIG_PATH = \"../configs/model/place_recognition/minkloc3d.yaml\"\n",
    "WEIGHTS_PATH = \"../weights/place_recognition/minkloc3d_nclt.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Init query dataset\n",
    "\n",
    "The pipeline infer method accepts an input in the format of dictionary with keys in the following format:\n",
    "- `\"image_{camera_name}\"` for images from cameras,\n",
    "- `\"mask_{camera_name}\"` for semantic segmentation masks,\n",
    "- `\"pointcloud_lidar_coords\"` for pointcloud coordinates from lidar,\n",
    "- `\"pointcloud_lidar_feats\"` for pointcloud features from lidar.\n",
    "\n",
    "The data type of all values are `torch.Tensor`.\n",
    "\n",
    "You can load and preprocess the data manually, but in this example we will use the `opr.datasets.itlp.ITLPCampus` ` dataset class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_dataset = ITLPCampus(\n",
    "    dataset_root=QUERY_TRACK_DIR,\n",
    "    sensors=[\"lidar\", \"front_cam\", \"back_cam\"],\n",
    "    mink_quantization_size=0.5,\n",
    "    load_semantics=True,\n",
    "    exclude_dynamic_classes=True,\n",
    "    indoor=True,\n",
    "    semantic_transform=ToTensorTransform(),\n",
    "    image_transform=ToTensorTransform()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(query_dataset[0][\"image_front_cam\"].permute(1, 2, 0).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(query_dataset[0][\"image_back_cam\"].permute(1, 2, 0).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize model\n",
    "\n",
    "We will use hydra's `instantiate` function to initialize the model. The model is a `MinkLoc3D` - a simple LiDAR-only architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = OmegaConf.load(MODEL_CONFIG_PATH)\n",
    "model = instantiate(model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The minimum requirement to initialize the `PlaceRecognitionPipeline` is that the database directory should contain the `index.faiss` file and the `track.csv` file.\n",
    "\n",
    "The `index.faiss` file is a Faiss index, which contains the descriptors of the database. The `track.csv` file contains the metadata of the database, including the id and the pose of the descriptors.\n",
    "\n",
    "The details on how to create the database are described in the [build_database.ipynb](./build_database.ipynb) notebook.\n",
    "\n",
    "Note that the actual data are not required, as the pipeline will only load the `index.faiss` and the `track.csv` file. This can be useful in the real-world scenario, where the database size is too large to be stored on the local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = PlaceRecognitionPipeline(\n",
    "    database_dir=DATABASE_DIR,\n",
    "    model=model,\n",
    "    model_weights_path=WEIGHTS_PATH,\n",
    "    device=DEVICE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = query_dataset[2]\n",
    "sample_pose_gt = sample_data.pop(\"pose\")  # removing those keys are not necessary, we just\n",
    "sample_data.pop(\"idx\")                    # want to simulate that we pass the data without GT information :)\n",
    "print(f\"sample_data.keys() = {sample_data.keys()}\")\n",
    "start = time()\n",
    "sample_output = pipe.infer(sample_data)\n",
    "print(time() - start)\n",
    "print(f\"sample_output.keys() = {sample_output.keys()}\")\n",
    "print(f\"sample_output['idx'] = {sample_output['idx']}\")\n",
    "print(f\"pose = {sample_output['pose']}\")\n",
    "print(f\"pose_gt = {sample_pose_gt.numpy()}\")\n",
    "dist_error, angle_error = compute_error(sample_output[\"pose\"], sample_pose_gt.numpy())\n",
    "print(f\"dist_error = {dist_error}, angle_error = {angle_error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage example - Pointcloud Registration Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRACK_DIR = QUERY_TRACK_DIR\n",
    "\n",
    "REGISTRATION_MODEL_CONFIG_PATH = \"../configs/model/registration/geotransformer_kitti.yaml\"\n",
    "REGISTRATION_WEIGHTS_PATH = \"../weights/registration/geotransformer_kitti.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geotransformer = instantiate(OmegaConf.load(REGISTRATION_MODEL_CONFIG_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "registration_pipe = PointcloudRegistrationPipeline(\n",
    "    model=geotransformer,\n",
    "    model_weights_path=REGISTRATION_WEIGHTS_PATH,\n",
    "    device=\"cuda\",  # the GeoTransformer currently only supports CUDA\n",
    "    voxel_downsample_size=0.3,  # recommended for geotransformer_kitti configuration\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 3\n",
    "db_pc = query_dataset[i-1][\"pointcloud_lidar_coords\"]\n",
    "query_pc = query_dataset[i][\"pointcloud_lidar_coords\"]\n",
    "db_pose = pose_to_matrix(query_dataset[i-1][\"pose\"])\n",
    "query_pose = pose_to_matrix(query_dataset[i][\"pose\"])\n",
    "# we want to find the transformation from the \"database\" pose to the \"query\" pose\n",
    "gt_transformation = np.linalg.inv(db_pose) @ query_pose\n",
    "\n",
    "estimated_transformation = registration_pipe.infer(query_pc, db_pc)\n",
    "\n",
    "print(f\"gt_transformation = \\n{gt_transformation}\\n\")\n",
    "print(f\"estimated_transformation = \\n{estimated_transformation}\\n\")\n",
    "\n",
    "rre, rte = compute_registration_error(gt_transformation, estimated_transformation)\n",
    "print(f\"Relative Rotation Error (RRE) = {rre:0.3f}\\nRelative Translation Error (RTE) = {rte:0.3f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"gt_pose = \\n{query_pose}\")\n",
    "print(f\"optimized_pose = \\n{db_pose @ estimated_transformation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage example - Localization Pipeline\n",
    "\n",
    "Localization pipeline combines the place recognition and the pointcloud registration pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init database dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_dataset = ITLPCampus(\n",
    "    dataset_root=DATABASE_TRACK_DIR,\n",
    "    sensors=[\"lidar\", \"front_cam\", \"back_cam\"],\n",
    "    mink_quantization_size=0.5,\n",
    "    load_semantics=True,\n",
    "    exclude_dynamic_classes=True,\n",
    "    indoor=True,\n",
    "    semantic_transform=ToTensorTransform(),\n",
    "    image_transform=ToTensorTransform()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init Localization Pipeline\n",
    "\n",
    "Here we will use the PlaceRecognition and Registration pipelines that we have initialized in the previous examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "localization_pipe = LocalizationPipeline(\n",
    "    place_recognition_pipeline=pipe,\n",
    "    registration_pipeline=registration_pipe,\n",
    "    db_dataset=db_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "PR_MATCH_THRESHOLD = 25.0\n",
    "pr_matches = []\n",
    "rre_list = []\n",
    "rte_list = []\n",
    "times = []\n",
    "\n",
    "for i, sample in enumerate(query_dataset):\n",
    "    print(f\"frame {i}\")\n",
    "    gt_pose = sample.pop(\"pose\")\n",
    "    gt_pose = get_transform_from_rotation_translation(Rotation.from_quat(gt_pose[3:]).as_matrix(), gt_pose[:3])\n",
    "\n",
    "    start_time = time()\n",
    "    pipe_out = localization_pipe.infer(sample)\n",
    "    times.append(time() - start_time)\n",
    "\n",
    "    db_match_pose = pipe_out[\"db_match_pose\"]\n",
    "    db_match_pose = get_transform_from_rotation_translation(Rotation.from_quat(db_match_pose[3:]).as_matrix(), db_match_pose[:3])\n",
    "    estimated_pose = pipe_out[\"estimated_pose\"]\n",
    "    estimated_pose = get_transform_from_rotation_translation(Rotation.from_quat(estimated_pose[3:]).as_matrix(), estimated_pose[:3])\n",
    "\n",
    "    _, db_match_distance = compute_registration_error(gt_pose, db_match_pose)\n",
    "    pr_matched = db_match_distance <= PR_MATCH_THRESHOLD\n",
    "    pr_matches.append(pr_matched)\n",
    "\n",
    "    rre, rte = compute_registration_error(gt_pose, estimated_pose)\n",
    "    rre_list.append(rre)\n",
    "    rte_list.append(rte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Place Recognition R@1 = {np.mean(pr_matches):0.3f}\")\n",
    "print(f\"Localization Mean RRE = {np.mean(rre_list):0.3f}\")\n",
    "print(f\"Localization Mean RTE = {np.mean(rte_list):0.3f}\")\n",
    "\n",
    "print(f\"Localization Median RRE = {np.median(rre_list):0.3f}\")\n",
    "print(f\"Localization Median RTE = {np.median(rte_list):0.3f}\")\n",
    "\n",
    "print(f\"Mean Time = {(np.mean(times) * 1000):0.2f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Place Recognition R@1 = 0.900\n",
    "\n",
    "Localization Mean RRE = 43.318\n",
    "\n",
    "Localization Mean RTE = 7.877\n",
    "\n",
    "Localization Median RRE = 3.817\n",
    "\n",
    "Localization Median RTE = 0.581\n",
    "\n",
    "Mean Time = 493.25 ms"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
