{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5673f90",
   "metadata": {},
   "source": [
    "# Tutorial on `opr.inference.index` subpackage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6054c195",
   "metadata": {},
   "source": [
    "This short tutorial shows how to use `opr.inference.index` with a tiny, synthetic example.\n",
    "\n",
    "What you'll learn:\n",
    "- On-disk layout expected by the index: `descriptors.npy`, `meta.parquet` (must include `idx` and `pose[7]`; optional `pointcloud_path` with relative paths like `scans/000227.pcd` or `scans/000227.bin`), `schema.json`.\n",
    "- How to load a FAISS Flat index (`FaissFlatIndex.load(...)`).\n",
    "- How to run a top-k search and map results to dataset indices, poses, and pointcloud paths.\n",
    "\n",
    "Requirements:\n",
    "- `faiss` (faiss-cpu or faiss-gpu)\n",
    "- A Parquet engine for pandas: `pyarrow` (recommended) or `fastparquet`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9417415e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment looks good.\n"
     ]
    }
   ],
   "source": [
    "# Check environment\n",
    "import importlib, sys\n",
    "\n",
    "missing = []\n",
    "for pkg in (\"faiss\", \"pandas\", \"numpy\", \"pyarrow\"):\n",
    "    if importlib.util.find_spec(pkg) is None:\n",
    "        missing.append(pkg)\n",
    "\n",
    "if missing:\n",
    "    print(\"Missing packages:\", \", \".join(missing))\n",
    "    print(\"Please install them, e.g.: pip install faiss-cpu pyarrow pandas numpy\")\n",
    "else:\n",
    "    print(\"Environment looks good.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8134eb65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: ['meta.parquet', 'descriptors.npy', 'schema.json']\n"
     ]
    }
   ],
   "source": [
    "# Build a tiny on-disk example index\n",
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "base = Path(\"./_demo_index\").resolve()\n",
    "base.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "N, D = 8, 4\n",
    "rng = np.random.default_rng(0)\n",
    "descriptors = rng.normal(size=(N, D)).astype(np.float32)\n",
    "poses = [[float(i), float(i+1), float(i+2), 0.0, 0.0, 0.0, 1.0] for i in range(N)]\n",
    "# Optional pointcloud paths: mix .pcd/.bin and NaN\n",
    "pc_paths = [\n",
    "    \"scans/%06d.pcd\" % i if i % 3 == 0 else (\"scans/%06d.bin\" % i if i % 3 == 1 else np.nan)\n",
    "    for i in range(N)\n",
    "]\n",
    "meta = pd.DataFrame({\n",
    "    \"idx\": np.arange(100, 100+N, dtype=np.int64),\n",
    "    \"pose\": poses,\n",
    "    \"pointcloud_path\": pc_paths,\n",
    "})\n",
    "\n",
    "np.save(base / \"descriptors.npy\", descriptors)\n",
    "meta.to_parquet(base / \"meta.parquet\")\n",
    "schema = {\"version\": \"1\", \"dim\": D, \"metric\": \"l2\", \"created_at\": \"\", \"opr_version\": \"\"}\n",
    "(base / \"schema.json\").write_text(json.dumps(schema))\n",
    "\n",
    "print(f\"Wrote: {[p.name for p in base.iterdir()]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdaf22b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index size: 8, dim: 4 metric: l2\n",
      "inds shape: (2, 3), dists shape: (2, 3)\n",
      "Top-k indices for first query: [0, 4, 1]\n",
      "Distances for first query: [0.0003999998443759978, 1.4175851345062256, 1.8044313192367554]\n",
      "DB idx: [100, 104, 101]\n",
      "DB poses (first 2): [[0. 1. 2. 0. 0. 0. 1.]\n",
      " [4. 5. 6. 0. 0. 0. 1.]]\n",
      "DB pointcloud paths: ['scans/000000.pcd', 'scans/000004.bin', 'scans/000001.bin']\n"
     ]
    }
   ],
   "source": [
    "# Load index and run a top-k search\n",
    "from opr.inference.index import FaissFlatIndex\n",
    "\n",
    "index = FaissFlatIndex.load(base)\n",
    "print(f\"Index size: {index.size()}, dim: {index.dim()} metric: {index.metric()}\")\n",
    "\n",
    "# Query: take first two descriptors with a small offset\n",
    "queries = descriptors[:2] + 0.01\n",
    "k = 3\n",
    "inds, dists = index.search(queries, k)\n",
    "\n",
    "print(f\"inds shape: {inds.shape}, dists shape: {dists.shape}\")\n",
    "print(f\"Top-k indices for first query: {inds[0].tolist()}\")\n",
    "print(f\"Distances for first query: {dists[0].tolist()}\")\n",
    "\n",
    "# Map to dataset idx, poses, and pointcloud paths for first query's candidates\n",
    "db_idx, db_pose, db_pc = index.get_meta(inds[0])\n",
    "print(f\"DB idx: {db_idx.tolist()}\")\n",
    "print(f\"DB poses (first 2): {db_pose[:2]}\")\n",
    "print(f\"DB pointcloud paths: {db_pc.tolist()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7f1870",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- FAISS returns raw distances (L2 for this example); smaller is better.\n",
    "- `get_meta` maps internal row positions to your dataset ids and poses.\n",
    "- For large datasets, `descriptors.npy` can be memory-mapped automatically to reduce RAM usage.\n",
    "- We do not persist FAISS index files; the index is built at load time.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
