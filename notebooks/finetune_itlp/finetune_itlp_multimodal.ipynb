{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df = pd.read_csv(\"/home/docker_opr/Datasets/OpenPlaceRecognition/itlp_campus_outdoor_part2/_test.csv\")\n",
    "# test_condition = (test_df['ty'] > -60) & (test_df['tx'] > -130) & (test_df['tx'] < 20) \\\n",
    "#     | (test_df['ty'] > -95) & (test_df['ty'] < -50) & (test_df['tx'] > -120) & (test_df['tx'] < -40)\n",
    "# test_df = test_df[test_condition]\n",
    "\n",
    "# train_df = pd.read_csv(\"/home/docker_opr/Datasets/OpenPlaceRecognition/itlp_campus_outdoor_part2/_train.csv\")\n",
    "# train_condition = (train_df['ty'] > -60) & (train_df['tx'] > -130) & (train_df['tx'] < 20) \\\n",
    "#     | (train_df['ty'] > -95) & (train_df['ty'] < -50) & (train_df['tx'] > -120) & (train_df['tx'] < -40)\n",
    "# train_condition = ~train_condition\n",
    "# train_df = train_df[train_condition]\n",
    "\n",
    "# train_df.to_csv(\"/home/docker_opr/Datasets/OpenPlaceRecognition/itlp_campus_outdoor_part2/train.csv\", index=False)\n",
    "# test_df.to_csv(\"/home/docker_opr/Datasets/OpenPlaceRecognition/itlp_campus_outdoor_part2/test.csv\", index=False)\n",
    "# test_df.to_csv(\"/home/docker_opr/Datasets/OpenPlaceRecognition/itlp_campus_outdoor_part2/val.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.scatter(train_df['tx'].to_numpy(), train_df['ty'].to_numpy(), marker='o', c='b')\n",
    "# plt.scatter(test_df['tx'].to_numpy(), test_df['ty'].to_numpy(), marker='o', c='r')\n",
    "# plt.xlabel('tx')\n",
    "# plt.ylabel('ty')\n",
    "# plt.title('Trajectory')\n",
    "# plt.axis('equal')\n",
    "# plt.legend(['train', 'test'])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:albumentations.check_version:A new version of Albumentations is available: 1.4.24 (you have 1.4.10). Upgrade using: pip install --upgrade albumentations\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, Literal\n",
    "\n",
    "import torch\n",
    "import hydra\n",
    "import wandb\n",
    "from hydra.utils import instantiate\n",
    "from loguru import logger\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from opr.datasets.dataloader_factory import make_dataloaders\n",
    "from opr.trainers.place_recognition import MultimodalPlaceRecognitionTrainer\n",
    "from opr.utils import set_seed\n",
    "from torch.utils.data import DataLoader\n",
    "from opr.datasets.itlp import ITLPCampus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hydra import compose, initialize\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "with initialize(version_base=None, config_path=\"../../configs\"):\n",
    "    cfg = compose(config_name=\"finetune_itlp_multimodal\")\n",
    "\n",
    "print(OmegaConf.to_yaml(cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_ROOT = \"/home/docker_opr/Datasets/OpenPlaceRecognition/itlp_campus_outdoor_part2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmelekhin-aa-work\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/docker_opr/OpenPlaceRecognition/notebooks/finetune_itlp/wandb/run-20241227_111945-q97tnmci</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/melekhin-aa-work/OPR_MODEL_ZOO/runs/q97tnmci' target=\"_blank\">finetune_itlp_multimodal</a></strong> to <a href='https://wandb.ai/melekhin-aa-work/OPR_MODEL_ZOO' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/melekhin-aa-work/OPR_MODEL_ZOO' target=\"_blank\">https://wandb.ai/melekhin-aa-work/OPR_MODEL_ZOO</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/melekhin-aa-work/OPR_MODEL_ZOO/runs/q97tnmci' target=\"_blank\">https://wandb.ai/melekhin-aa-work/OPR_MODEL_ZOO/runs/q97tnmci</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Seed: 3121999\n"
     ]
    }
   ],
   "source": [
    "if not cfg.debug and not cfg.wandb.disabled:\n",
    "    config_dict = OmegaConf.to_container(cfg, resolve=True, throw_on_missing=True)\n",
    "    wandb.init(\n",
    "        name=cfg.exp_name,\n",
    "        project=cfg.wandb.project,\n",
    "        settings=wandb.Settings(start_method=\"thread\"),\n",
    "        config=config_dict,\n",
    "    )\n",
    "    run_name = wandb.run.name\n",
    "else:\n",
    "    run_name = \"debug\"\n",
    "\n",
    "checkpoints_dir = (\n",
    "    Path(cfg.checkpoints_dir) / f\"{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}_{run_name}\"\n",
    ")\n",
    "if not checkpoints_dir.exists():\n",
    "    checkpoints_dir.mkdir(parents=True)\n",
    "\n",
    "set_seed(seed=cfg.seed, make_deterministic=False)  # we cannot use determenistic operators here :(\n",
    "print(f\"=> Seed: {cfg.seed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ITLPCampus(\n",
    "    dataset_root=DATASET_ROOT,\n",
    "    subset=\"train\",\n",
    "    csv_file=\"train.csv\",\n",
    "    sensors=[\"front_cam\", \"back_cam\", \"lidar\"],\n",
    ")\n",
    "val_dataset = ITLPCampus(\n",
    "    dataset_root=DATASET_ROOT,\n",
    "    subset=\"val\",\n",
    "    csv_file=\"val.csv\",\n",
    "    sensors=[\"front_cam\", \"back_cam\", \"lidar\"],\n",
    ")\n",
    "test_dataset = ITLPCampus(\n",
    "    dataset_root=DATASET_ROOT,\n",
    "    subset=\"test\",\n",
    "    csv_file=\"test.csv\",\n",
    "    sensors=[\"front_cam\", \"back_cam\", \"lidar\"],\n",
    ")\n",
    "# test_dataset.dataset_df = test_dataset.dataset_df[test_dataset.dataset_df[\"track\"].isin([\"05_2023-08-15-day\", \"07_2023-10-04-day\"])].reset_index(drop=True)\n",
    "\n",
    "train_sampler = instantiate(cfg.sampler, dataset=train_dataset)\n",
    "val_sampler = instantiate(cfg.sampler, dataset=val_dataset)\n",
    "\n",
    "dataloaders = {}\n",
    "dataloaders[\"train\"] = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_sampler=train_sampler,\n",
    "    collate_fn=train_dataset.collate_fn,\n",
    "    num_workers=cfg.num_workers,\n",
    "    pin_memory=True,\n",
    ")\n",
    "dataloaders[\"val\"] = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_sampler=val_sampler,\n",
    "    collate_fn=val_dataset.collate_fn,\n",
    "    num_workers=cfg.num_workers,\n",
    "    pin_memory=True,\n",
    ")\n",
    "dataloaders[\"test\"] = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=cfg.sampler.batch_size_limit,\n",
    "    collate_fn=test_dataset.collate_fn,\n",
    "    num_workers=cfg.num_workers,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "610"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataloaders[\"test\"].dataset.dataset_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = instantiate(cfg.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:faiss.loader:Loading faiss with AVX2 support.\n",
      "INFO:faiss.loader:Could not load library with AVX2 support due to:\n",
      "ModuleNotFoundError(\"No module named 'faiss.swigfaiss_avx2'\")\n",
      "INFO:faiss.loader:Loading faiss.\n",
      "INFO:faiss.loader:Successfully loaded faiss.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = instantiate(cfg.model)\n",
    "\n",
    "# load pretrained NCLT checkpoint\n",
    "ckpt = torch.load(\"/home/docker_opr/OpenPlaceRecognition/weights/place_recognition/multi-image_lidar_late-fusion_nclt.pth\")\n",
    "model.load_state_dict(ckpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init optimizer and scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = instantiate(cfg.optimizer, params=model.parameters())\n",
    "scheduler = instantiate(cfg.scheduler, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = MultimodalPlaceRecognitionTrainer(\n",
    "    modalities_weights=cfg.modalities_weights,\n",
    "    checkpoints_dir=checkpoints_dir,\n",
    "    model=model,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    batch_expansion_threshold=cfg.batch_expansion_threshold,\n",
    "    wandb_log=(not cfg.debug and not cfg.wandb.disabled),\n",
    "    device=cfg.device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(epochs=cfg.epochs, train_dataloader=dataloaders[\"train\"], val_dataloader=dataloaders[\"val\"], test_dataloader=dataloaders[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_ckpt = torch.load(str(checkpoints_dir / \"best.pth\"))\n",
    "trainer.model.load_state_dict(best_ckpt[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-12-27 11:37:17.158\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mopr.trainers.place_recognition.unimodal\u001b[0m:\u001b[36mtest\u001b[0m:\u001b[36m172\u001b[0m - \u001b[1m=> Test stage:\u001b[0m\n",
      "\u001b[32m2024-12-27 11:37:28.833\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mopr.trainers.place_recognition.unimodal\u001b[0m:\u001b[36mtest\u001b[0m:\u001b[36m194\u001b[0m - \u001b[34m\u001b[1mTest embeddings: (610, 512)\u001b[0m\n",
      "\u001b[32m2024-12-27 11:37:28.994\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mopr.trainers.place_recognition.unimodal\u001b[0m:\u001b[36mtest\u001b[0m:\u001b[36m235\u001b[0m - \u001b[1mTest time: 00:11\u001b[0m\n",
      "\u001b[32m2024-12-27 11:37:28.995\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mopr.trainers.place_recognition.unimodal\u001b[0m:\u001b[36mtest\u001b[0m:\u001b[36m236\u001b[0m - \u001b[1mMean Recall@N:\n",
      "[0.99509334 0.99782782 1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.        ]\u001b[0m\n",
      "\u001b[32m2024-12-27 11:37:28.996\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mopr.trainers.place_recognition.unimodal\u001b[0m:\u001b[36mtest\u001b[0m:\u001b[36m237\u001b[0m - \u001b[1mMean Recall@1% = 0.9978278227620333\u001b[0m\n",
      "\u001b[32m2024-12-27 11:37:28.996\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mopr.trainers.place_recognition.unimodal\u001b[0m:\u001b[36mtest\u001b[0m:\u001b[36m238\u001b[0m - \u001b[1mMean top-1 distance = 1.1823934802782412\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "trainer.test(dataloaders[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
