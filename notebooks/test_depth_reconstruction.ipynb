{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тест модуля восстановления глубины"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Импортирование общих библиотек и OPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/docker_opr/OpenPlaceRecognition/src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from cv2 import imread\n",
    "from skimage.transform import resize\n",
    "from scipy.spatial.transform import Rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/docker_opr/OpenPlaceRecognition/third_party/Depth-Anything-V2/')  # локальный путь до кода\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "import open3d\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import transformers\n",
    "import cv2.aruco as aruco\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from copy import deepcopy\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from accelerate import DistributedDataParallelKwargs, Accelerator, notebook_launcher\n",
    "from accelerate.utils import set_seed\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from depth_anything_v2.dpt import DepthAnythingV2\n",
    "from metric_depth.depth_anything_v2.dpt import DepthAnythingV2 as DepthAnythingV2Metric\n",
    "from metric_depth.dataset.transform import Resize, NormalizeImage, PrepareForNet, Crop\n",
    "from metric_depth.util.loss import SiLogLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opr.pipelines.depth_estimation import DepthEstimationPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание нейросети восстановления глубины"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузка чекпоинтов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p /home/docker_opr/OpenPlaceRecognition/third_party/AdelaiDepth/weights/\n",
    "!wget -O /home/docker_opr/OpenPlaceRecognition/third_party/AdelaiDepth/weights/res50.pth https://huggingface.co/ffranchina/LeReS/resolve/main/res50.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/home/docker_opr/OpenPlaceRecognition/third_party/AdelaiDepth/LeReS/Minist_Test')\n",
    "from lib.multi_depth_model_woauxi import RelDepthModel\n",
    "from lib.net_tools import load_ckpt\n",
    "\n",
    "def parse_args(a):\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description='Configs for LeReS')\n",
    "    parser.add_argument('--load_ckpt', default='./res50.pth', help='Checkpoint path to load')\n",
    "    parser.add_argument('--backbone', default='resnext101', help='Checkpoint path to load')\n",
    "\n",
    "    args = parser.parse_args(a)\n",
    "    return args\n",
    "\n",
    "import argparse\n",
    "arguments = \"--load_ckpt /home/docker_opr/OpenPlaceRecognition/third_party/AdelaiDepth/weights/res50.pth \\\n",
    "            --backbone resnet50\".split()\n",
    "args = parse_args(arguments)\n",
    "\n",
    "old_model = RelDepthModel(backbone='resnet50').cuda()\n",
    "load_ckpt(args, old_model, None, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset download\n",
    "\n",
    "You can download the dataset:\n",
    "\n",
    "- Kaggle:\n",
    "  - [ITLP Campus Indoor](https://www.kaggle.com/datasets/alexandermelekhin/itlp-campus-indoor)\n",
    "  - [ITLP Campus Outdoor](https://www.kaggle.com/datasets/alexandermelekhin/itlp-campus-outdoor)\n",
    "- Hugging Face:\n",
    "  - [ITLP Campus Indoor](https://huggingface.co/datasets/OPR-Project/ITLP-Campus-Indoor)\n",
    "  - [ITLP Campus Outdoor](https://huggingface.co/datasets/OPR-Project/ITLP-Campus-Outdoor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS_BASE_PATH = '/home/docker_opr/OpenPlaceRecognition/third_party/Depth-Anything-V2/weights'\n",
    "#DATA_BASE_PATH = '/home/docker_opr/Datasets/OpenPlaceRecognition/itlp_campus_indoor/00_2023-10-25-night/floor_5'\n",
    "DATA_BASE_PATH = '/home/docker_opr/Datasets/OpenPlaceRecognition/itlp_campus_outdoor/03_2023-04-11/'\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "def project_points_to_camera(\n",
    "    points: np.ndarray, proj_matrix: np.ndarray, cam_res: Tuple[int, int]\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    if points.shape[0] == 3:\n",
    "        points = np.vstack((points, np.ones((1, points.shape[1]))))\n",
    "    if len(points.shape) != 2 or points.shape[0] != 4:\n",
    "        raise ValueError(\n",
    "            f\"Wrong shape of points array: {points.shape}; expected: (4, n), where n - number of points.\"\n",
    "        )\n",
    "    if proj_matrix.shape != (3, 4):\n",
    "        raise ValueError(f\"Wrong proj_matrix shape: {proj_matrix}; expected: (3, 4).\")\n",
    "    in_image = points[2, :] > 0\n",
    "    depths = points[2, in_image]\n",
    "    uvw = np.dot(proj_matrix, points[:, in_image])\n",
    "    uv = uvw[:2, :]\n",
    "    w = uvw[2, :]\n",
    "    uv[0, :] /= w\n",
    "    uv[1, :] /= w\n",
    "    in_image = (uv[0, :] >= 0) * (uv[0, :] < cam_res[0]) * (uv[1, :] >= 0) * (uv[1, :] < cam_res[1])\n",
    "    return uv[:, in_image].astype(int), depths[in_image]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_configs = {\n",
    "    'small': {'encoder': 'vits', 'features': 64, 'out_channels': [48, 96, 192, 384]},\n",
    "    'base': {'encoder': 'vitb', 'features': 128, 'out_channels': [96, 192, 384, 768]},\n",
    "    'large': {'encoder': 'vitl', 'features': 256, 'out_channels': [256, 512, 1024, 1024]}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "загрузка чекпоинта для Depth Anything:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p /home/docker_opr/OpenPlaceRecognition/third_party/Depth-Anything-V2/weights\n",
    "!wget -O /home/docker_opr/OpenPlaceRecognition/third_party/Depth-Anything-V2/weights/depth_anything_v2_metric_vkitti_vits.pth https://huggingface.co/depth-anything/Depth-Anything-V2-Small/resolve/main/depth_anything_v2_vits.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "type = 'small'\n",
    "params = model_configs[type]\n",
    "new_model = DepthAnythingV2Metric(**params, max_depth=20.0)\n",
    "model_path=os.path.join(MODELS_BASE_PATH, 'depth_anything_v2_metric_vkitti_vits.pth')\n",
    "new_model.load_state_dict(torch.load(model_path))\n",
    "new_model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание трансформации между лидаром и камерой (данные из росбэга с робота Husky)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fx = 683.6\n",
    "fy = fx\n",
    "cx = 615.1\n",
    "cy = 345.3\n",
    "camera_matrix = {'f': fx, 'cx': cx, 'cy': cy}\n",
    "proj_matrix = np.array([\n",
    "    [fx, 0.0, cx, 0],\n",
    "    [0.0, fy, cy, 0],\n",
    "    [0.0, 0.0, 1.0, 0]\n",
    "])\n",
    "rotation = [-0.498, 0.498, -0.495, 0.510]\n",
    "R = Rotation.from_quat(rotation).as_matrix()\n",
    "#R = np.linalg.inv(R)\n",
    "translation = np.array([[0.061], [0.049], [-0.131]])\n",
    "tf_matrix = np.concatenate([R, translation], axis=1)\n",
    "tf_matrix = np.concatenate([tf_matrix, np.array([[0, 0, 0, 1]])], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_matrix.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Инициализация модуля восстановления глубины"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_old = DepthEstimationPipeline(old_model, model_type='AdelaiDepth', align_type='average')\n",
    "de_old.set_camera_matrix(camera_matrix)\n",
    "de_old.set_lidar_to_camera_transform(tf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_new = DepthEstimationPipeline(new_model, model_type='DepthAnything', align_type='average', mode='outdoor')\n",
    "de_new.set_camera_matrix(camera_matrix)\n",
    "de_new.set_lidar_to_camera_transform(tf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Запуск восстановления глубины"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmses_old = []\n",
    "rels_old = []\n",
    "rmses_new = []\n",
    "rels_new = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook\n",
    "zs_all = {}\n",
    "rel_errors_all = {}\n",
    "errors_all = {}\n",
    "for track_dir in ['01_2023-02-21', '03_2023-04-11']:\n",
    "    DATA_BASE_PATH = '/home/docker_opr/Datasets/OpenPlaceRecognition/itlp_campus_outdoor/{}'.format(track_dir)\n",
    "    track_csv = pd.read_csv(os.path.join(DATA_BASE_PATH, 'track.csv'))\n",
    "    track_csv['front_cam_ts'] = track_csv['front_cam_ts'].astype(str)\n",
    "    track_csv['lidar_ts'] = track_csv['lidar_ts'].astype(str)\n",
    "    zs_all[track_dir] = []\n",
    "    rel_errors_all[track_dir] = []\n",
    "    errors_all[track_dir] = []\n",
    "    for i in tqdm_notebook(range(track_csv.shape[0])):\n",
    "        ts_cam = track_csv['front_cam_ts'].iloc[i]\n",
    "        ts_lidar = track_csv['lidar_ts'].iloc[i]\n",
    "        print(ts_cam, ts_lidar)\n",
    "        test_img_file = os.path.join(DATA_BASE_PATH, 'front_cam', '{}.png'.format(ts_cam))\n",
    "        test_cloud_file = os.path.join(DATA_BASE_PATH, 'lidar', '{}.bin'.format(ts_lidar))\n",
    "        test_img = imread(test_img_file)\n",
    "        test_img = test_img[:, :, :3]\n",
    "        test_cloud = np.fromfile(test_cloud_file, dtype=np.float32).reshape((-1, 4))[:, :-1]\n",
    "        test_cloud = test_cloud[test_cloud == test_cloud].reshape((-1, 3))\n",
    "        print(test_img.shape)\n",
    "        print(test_cloud.shape)\n",
    "        print(test_cloud.min(), test_cloud.max())\n",
    "        # depth, rmse, rel = de_old.get_depth_with_lidar(test_img, test_cloud[:, :3])\n",
    "        # rmses_old.append(rmse)\n",
    "        # rels_old.append(rel)\n",
    "        # depth, rmse, rel = de_new.get_depth_with_lidar(test_img, test_cloud[:, :3])\n",
    "        depth, zs, errors, rel_errors = de_new.get_depth_with_lidar(test_img, test_cloud[:, :3])\n",
    "        rmse = np.sqrt(np.mean(errors ** 2))\n",
    "        rel = np.mean(rel_errors)\n",
    "        rmses_new.append(rmse)\n",
    "        rels_new.append(rel)\n",
    "        zs_all[track_dir] += list(zs)\n",
    "        rel_errors_all[track_dir] += list(rel_errors)\n",
    "        errors_all[track_dir] += list(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cloud_by_timestamp(data_dir, ts, float32=False):\n",
    "    stamps = [int(x[:-4]) for x in os.listdir(os.path.join(data_dir, 'lidar'))]\n",
    "    stamps.sort()\n",
    "    i = 0\n",
    "    #if float32:\n",
    "    #    stamps = np.array(stamps) * 1000\n",
    "    while i < len(stamps) and stamps[i] < ts:\n",
    "        i += 1\n",
    "    #print(i, len(stamps))\n",
    "    if i == len(stamps):\n",
    "        stamp = stamps[-1]\n",
    "    elif i == 0:\n",
    "        stamp = stamps[0]\n",
    "    elif ts - stamps[i - 1] < stamps[i] - ts:\n",
    "        stamp = stamps[i - 1]\n",
    "    else:\n",
    "        stamp = stamps[i]\n",
    "    #print(ts, stamp, ts - stamp)\n",
    "    if float32:\n",
    "        return stamp, np.fromfile(os.path.join('dataset', data_dir, 'lidar', '{}.bin'.format(stamp)), \n",
    "                           dtype=np.float32).reshape((-1, 4))[:, :-1]\n",
    "    return stamp, np.fromfile(os.path.join('dataset', data_dir, 'lidar', '{}.bin'.format(stamp))).reshape((-1, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_by_timestamp(data_dir, ts):\n",
    "    stamps = [int(x[:-4]) for x in os.listdir(os.path.join(data_dir, 'front_cam'))]\n",
    "    stamps.sort()\n",
    "    i = 0\n",
    "    #if float32:\n",
    "    #    stamps = np.array(stamps) * 1000\n",
    "    while i < len(stamps) and stamps[i] < ts:\n",
    "        i += 1\n",
    "    #print(i, len(stamps))\n",
    "    if i == len(stamps):\n",
    "        stamp = stamps[-1]\n",
    "    elif i == 0:\n",
    "        stamp = stamps[0]\n",
    "    elif ts - stamps[i - 1] < stamps[i] - ts:\n",
    "        stamp = stamps[i - 1]\n",
    "    else:\n",
    "        stamp = stamps[i]\n",
    "    print(ts, stamp, ts - stamp)\n",
    "    return stamp, imread(os.path.join(data_dir, 'front_cam', '{}.png'.format(stamp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for track_dir in ['05_2023-08-15-day', '07_2023-10-04-day']:\n",
    "    DATA_BASE_PATH = '/home/docker_opr/Datasets/OpenPlaceRecognition/itlp_campus_outdoor/{}'.format(track_dir)\n",
    "    track_csv = pd.read_csv(os.path.join(DATA_BASE_PATH, 'track.csv'))\n",
    "    print(track_csv.shape, track_csv.columns)\n",
    "    track_csv['timestamp'] = track_csv['timestamp'].astype(int)\n",
    "    print(track_csv['timestamp'].values)\n",
    "    zs_all[track_dir] = []\n",
    "    rel_errors_all[track_dir] = []\n",
    "    errors_all[track_dir] = []\n",
    "    for i in tqdm_notebook(range(0, track_csv.shape[0], 10)):\n",
    "        stamp = track_csv.iloc[i]['timestamp']\n",
    "        ts_lidar, test_cloud = get_cloud_by_timestamp(DATA_BASE_PATH, stamp)\n",
    "        ts_cam, test_img = get_image_by_timestamp(DATA_BASE_PATH, stamp)\n",
    "        test_img = test_img[:, :, :3]\n",
    "        test_cloud = test_cloud[test_cloud == test_cloud].reshape((-1, 3))\n",
    "        print(test_img.shape)\n",
    "        print(test_cloud.shape)\n",
    "        print(test_cloud.min(), test_cloud.max())\n",
    "        # depth, rmse, rel = de_old.get_depth_with_lidar(test_img, test_cloud[:, :3])\n",
    "        # rmses_old.append(rmse)\n",
    "        # rels_old.append(rel)\n",
    "        # depth, rmse, rel = de_new.get_depth_with_lidar(test_img, test_cloud[:, :3])\n",
    "        depth, zs, errors, rel_errors = de_new.get_depth_with_lidar(test_img, test_cloud[:, :3])\n",
    "        rmse = np.sqrt(np.mean(errors ** 2))\n",
    "        rel = np.mean(rel_errors)\n",
    "        rmses_new.append(rmse)\n",
    "        rels_new.append(rel)\n",
    "        zs_all[track_dir] += list(zs)\n",
    "        rel_errors_all[track_dir] += list(rel_errors)\n",
    "        errors_all[track_dir] += list(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_by_z = {}\n",
    "rel_by_z = {}\n",
    "for track in zs_all.keys():\n",
    "    rmse_by_z[track] = []\n",
    "    rel_by_z[track] = []\n",
    "    zs = np.array(zs_all[track])\n",
    "    errors = np.array(errors_all[track])\n",
    "    rels = np.array(rel_errors_all[track])\n",
    "    for z in range(0, 50, 5):\n",
    "        rel = rels[(zs > z) * (zs <= z + 5)].mean()\n",
    "        rmse = np.sqrt(np.mean(errors[(zs > z) * (zs <= z + 5)] ** 2))\n",
    "        rel_by_z[track].append(rel)\n",
    "        rmse_by_z[track].append(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasons = {'01':'Winter', '03':'Spring', '05':'Summer', '07':'Autumn'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 8))\n",
    "plt.grid(ls=':')\n",
    "for track in zs_all.keys():\n",
    "    plt.plot(np.linspace(5, 50, 10), rel_by_z[track], label=seasons[track[:2]])\n",
    "plt.xlabel('Depth, m', fontsize=16)\n",
    "plt.ylabel('Rel error', fontsize=16)\n",
    "plt.ylim((0, 1))\n",
    "plt.legend(fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 8))\n",
    "plt.grid(ls=':')\n",
    "for track in zs_all.keys():\n",
    "    plt.plot(np.linspace(5, 50, 10), rmse_by_z[track], label=seasons[track[:2]])\n",
    "plt.xlabel('Depth, m', fontsize=16)\n",
    "plt.ylabel('RMSE, m', fontsize=16)\n",
    "plt.ylim((0, 20))\n",
    "plt.legend(fontsize=16)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('RMSE of old model:', np.mean(rmses_old))\n",
    "print('RMSE of new model:', np.mean(rmses_new))\n",
    "print('Rel error of old model:', np.mean(rels_old))\n",
    "print('Rel error of new model:', np.mean(rels_new))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Визуализация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "def project_points_to_camera(\n",
    "    points: np.ndarray, proj_matrix: np.ndarray, cam_res: Tuple[int, int]\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    if points.shape[0] == 3:\n",
    "        points = np.vstack((points, np.ones((1, points.shape[1]))))\n",
    "    if len(points.shape) != 2 or points.shape[0] != 4:\n",
    "        raise ValueError(\n",
    "            f\"Wrong shape of points array: {points.shape}; expected: (4, n), where n - number of points.\"\n",
    "        )\n",
    "    if proj_matrix.shape != (3, 4):\n",
    "        raise ValueError(f\"Wrong proj_matrix shape: {proj_matrix}; expected: (3, 4).\")\n",
    "    in_image = points[2, :] > 0\n",
    "    depths = points[2, in_image]\n",
    "    uvw = np.dot(proj_matrix, points[:, in_image])\n",
    "    uv = uvw[:2, :]\n",
    "    w = uvw[2, :]\n",
    "    uv[0, :] /= w\n",
    "    uv[1, :] /= w\n",
    "    in_image = (uv[0, :] >= 0) * (uv[0, :] < cam_res[0]) * (uv[1, :] >= 0) * (uv[1, :] < cam_res[1])\n",
    "    return uv[:, in_image].astype(int), depths[in_image]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def depths_to_colors(depths: np.ndarray, max_depth: int = 10, cmap: str = \"hsv\") -> np.ndarray:\n",
    "    depths /= max_depth\n",
    "    to_colormap = plt.get_cmap(cmap)\n",
    "    rgba_values = to_colormap(depths, bytes=True)\n",
    "    return rgba_values[:, :3].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cv2 import resize\n",
    "%timeit resize(test_img, (480, 640))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_resized = resize(test_img, (640, 480))\n",
    "image_resized.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud_expanded = np.hstack([test_cloud, np.ones((test_cloud.shape[0], 1))])\n",
    "b = tf_matrix.T @ cloud_expanded.T\n",
    "b = b.T[b.T[:, 1] < 0].T\n",
    "uv, depths = project_points_to_camera(b, proj_matrix, (1280, 720))\n",
    "rgb_distances = depths_to_colors(depths, max_depth=50)\n",
    "for point, d in zip(uv.T, rgb_distances):\n",
    "    c = (int(d[0]), int(d[1]), int(d[2]))\n",
    "    cv2.circle(test_img, point, radius=2, color=c, thickness=cv2.FILLED)\n",
    "plt.imshow(test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(depth)\n",
    "plt.colorbar(shrink=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Создание облака точек по восстановленной глубине методом обратной проекции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h, w = depth.shape\n",
    "def get_point_cloud_from_depth(depth, f, cx, cy):\n",
    "    print(depth.shape, f, cx, cy)\n",
    "    i = np.tile(np.arange(h), w).reshape((w, h)).T\n",
    "    j = np.tile(np.arange(w), h).reshape((h, w))\n",
    "    z = depth.ravel()\n",
    "    x = (j.ravel() - cx) / f * z\n",
    "    y = (i.ravel() - cy) / f * z\n",
    "    pcd = np.zeros((x.shape[0], 3))\n",
    "    pcd[:, 0] = x\n",
    "    pcd[:, 1] = y\n",
    "    pcd[:, 2] = z\n",
    "    return pcd\n",
    "\n",
    "pcd = get_point_cloud_from_depth(depth, camera_matrix['f'], camera_matrix['cx'], camera_matrix['cy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 12))\n",
    "plt.scatter(pcd[:, 2][pcd[:, 1] < 0.], -pcd[:, 0][pcd[:, 1] < 0.], s=5, alpha=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Сравнение восстановленного и лидарного облака"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 12))\n",
    "plt.xlim((-40, 40))\n",
    "plt.ylim((-40, 40))\n",
    "plt.scatter(test_cloud[:, 0], test_cloud[:, 1], s=5, alpha=0.1)\n",
    "#plt.scatter(test_cloud[:, 0][test_cloud[:, 2] > 0], test_cloud[:, 1][test_cloud[:, 2] > 0], s=5, alpha=0.1)\n",
    "plt.scatter(pcd[::17, 2], -pcd[::17, 0], color='orange', s=5, alpha=0.1)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
