{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SequencePointcloudRegistrationPipeline\n",
    "\n",
    "A module that implements an algorithm for optimizing the position and orientation of a vehicle in space based on a sequence of multimodal data using neural network methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 1.4.23 (you have 1.4.22). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "from pathlib import Path\n",
    "from time import time\n",
    "\n",
    "import faiss\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "import open3d.core as o3c\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchshow as ts\n",
    "\n",
    "from hydra.utils import instantiate\n",
    "from omegaconf import OmegaConf\n",
    "from scipy.spatial.transform import Rotation\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from geotransformer.utils.registration import compute_registration_error\n",
    "\n",
    "\n",
    "from opr.datasets import NCLTDataset\n",
    "from opr.pipelines.place_recognition import PlaceRecognitionPipeline\n",
    "from opr.pipelines.registration.pointcloud import SequencePointcloudRegistrationPipeline\n",
    "from opr.pipelines.localization import LocalizationPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"DISPLAY\"] = \":1\"\n",
    "\n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pose_to_matrix(pose):\n",
    "    \"\"\"From the 6D poses in the [tx ty tz qx qy qz qw] format to 4x4 pose matrices.\"\"\"\n",
    "    position = pose[:3]\n",
    "    orientation_quat = pose[3:]\n",
    "    rotation = Rotation.from_quat(orientation_quat)\n",
    "    pose_matrix = np.eye(4)\n",
    "    pose_matrix[:3,:3] = rotation.as_matrix()\n",
    "    pose_matrix[:3,3] = position\n",
    "    return pose_matrix\n",
    "\n",
    "\n",
    "# def compute_error(estimated_pose, gt_pose):\n",
    "#     \"\"\"For the 6D poses in the [tx ty tz qx qy qz qw] format.\"\"\"\n",
    "#     estimated_pose = pose_to_matrix(estimated_pose)\n",
    "#     gt_pose = pose_to_matrix(gt_pose)\n",
    "#     return compute_registration_error(estimated_pose, gt_pose)\n",
    "\n",
    "def compute_error(estimated_pose, gt_pose):\n",
    "    \"\"\"For the 6D poses in the [tx ty tz qx qy qz qw] format.\"\"\"\n",
    "    estimated_pose = pose_to_matrix(estimated_pose)\n",
    "    gt_pose = pose_to_matrix(gt_pose)\n",
    "    error_pose = np.linalg.inv(estimated_pose) @ gt_pose\n",
    "    dist_error = np.sum(error_pose[:3, 3]**2) ** 0.5\n",
    "    r = Rotation.from_matrix(error_pose[:3, :3])\n",
    "    rotvec = r.as_rotvec()\n",
    "    angle_error = (np.sum(rotvec**2)**0.5) * 180 / np.pi\n",
    "    angle_error = abs(90 - abs(angle_error-90))\n",
    "    return angle_error, dist_error\n",
    "\n",
    "\n",
    "def draw_pc(pc: Tensor, color: str = \"blue\"):\n",
    "    pc_o3d = o3c.Tensor.from_dlpack(torch.utils.dlpack.to_dlpack(pc))\n",
    "    pcd = o3d.t.geometry.PointCloud(pc_o3d)\n",
    "    if color == \"blue\":\n",
    "        c = [0.0, 0.0, 1.0]\n",
    "    elif color == \"red\":\n",
    "        c = [1.0, 0.0, 0.0]\n",
    "    else:\n",
    "        c = [0.0, 1.0, 0.0]\n",
    "    pcd = pcd.paint_uniform_color(c)\n",
    "    o3d.visualization.draw_geometries(\n",
    "        [pcd.to_legacy()],\n",
    "    )\n",
    "\n",
    "\n",
    "def invert_rigid_transformation_matrix(T: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Inverts a 4x4 rigid body transformation matrix.\n",
    "\n",
    "    Args:\n",
    "        T (np.ndarray): A 4x4 rigid body transformation matrix.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The inverted 4x4 rigid body transformation matrix.\n",
    "    \"\"\"\n",
    "    assert T.shape == (4, 4), \"Input matrix must be 4x4.\"\n",
    "\n",
    "    R = T[:3, :3]\n",
    "    t = T[:3, 3]\n",
    "\n",
    "    R_inv = R.T\n",
    "    t_inv = -R.T @ t\n",
    "\n",
    "    T_inv = np.eye(4)\n",
    "    T_inv[:3, :3] = R_inv\n",
    "    T_inv[:3, 3] = t_inv\n",
    "\n",
    "    return T_inv\n",
    "\n",
    "\n",
    "def draw_pc_pair(\n",
    "    pc_blue: Tensor, pc_blue_pose: np.ndarray | Tensor, pc_red: Tensor, pc_red_pose: np.ndarray | Tensor\n",
    "):\n",
    "    pc_blue_o3d = o3c.Tensor.from_dlpack(torch.utils.dlpack.to_dlpack(copy.deepcopy(pc_blue)))\n",
    "    pc_red_o3d = o3c.Tensor.from_dlpack(torch.utils.dlpack.to_dlpack(copy.deepcopy(pc_red)))\n",
    "\n",
    "    blue_pcd = o3d.t.geometry.PointCloud(pc_blue_o3d)\n",
    "    blue_pcd_tmp = copy.deepcopy(blue_pcd)\n",
    "\n",
    "    red_pcd = o3d.t.geometry.PointCloud(pc_red_o3d)\n",
    "    red_pcd_tmp = copy.deepcopy(red_pcd)\n",
    "\n",
    "    blue_pcd_tmp.voxel_down_sample(voxel_size=0.3)\n",
    "    # blue_pcd_tmp.transform(pose_to_matrix(pc_blue_pose))\n",
    "    blue_pcd_tmp = blue_pcd_tmp.paint_uniform_color([0.0, 0.0, 1.0])\n",
    "\n",
    "    red_pcd_tmp.voxel_down_sample(voxel_size=0.3)\n",
    "    red_pcd_tmp.transform(pose_to_matrix(pc_red_pose))\n",
    "    red_pcd_tmp.transform(invert_rigid_transformation_matrix(pose_to_matrix(pc_blue_pose)))\n",
    "    red_pcd_tmp = red_pcd_tmp.paint_uniform_color([1.0, 0.0, 0.0])\n",
    "    o3d.visualization.draw_geometries(\n",
    "        [blue_pcd_tmp.to_legacy(), red_pcd_tmp.to_legacy()],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can download the dataset:\n",
    "\n",
    "- Kaggle:\n",
    "  - [NCLT_OpenPlaceRecognition](https://www.kaggle.com/datasets/creatorofuniverses/nclt-iprofi-hack-23)\n",
    "- Hugging Face:\n",
    "  - [NCLT_OpenPlaceRecognition](https://huggingface.co/datasets/OPR-Project/NCLT_OpenPlaceRecognition)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_ROOT = \"/home/docker_opr/Datasets/OpenPlaceRecognition/NCLT_preprocessed\"\n",
    "\n",
    "SENSOR_SUITE = [\"image_Cam5\", \"image_Cam2\", \"pointcloud_lidar\"]\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 4\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "PR_MODEL_CONFIG_PATH = \"../../configs/model/place_recognition/multi-image_lidar_late-fusion.yaml\"\n",
    "PR_WEIGHTS_PATH = \"../../weights/place_recognition/multi-image_lidar_late-fusion_nclt.pth\"\n",
    "\n",
    "REGISTRATION_MODEL_CONFIG_PATH = \"../../configs/model/registration/hregnet_light_feats.yaml\"\n",
    "REGISTRATION_WEIGHTS_PATH = \"../../weights/registration/hregnet_light_feats_nuscenes.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 tracks\n",
      "['2012-01-08', '2012-01-22', '2012-02-12', '2012-02-18', '2012-03-31', '2012-05-26', '2012-08-04', '2012-10-28', '2012-11-04', '2012-12-01']\n",
      "WARNING: track list limited\n",
      "['2012-01-08', '2012-01-22']\n"
     ]
    }
   ],
   "source": [
    "TRACK_LIST = sorted([str(subdir.name) for subdir in Path(DATASET_ROOT).iterdir() if subdir.is_dir()])\n",
    "print(f\"Found {len(TRACK_LIST)} tracks\")\n",
    "print(TRACK_LIST)\n",
    "\n",
    "print(\"WARNING: track list limited\")\n",
    "TRACK_LIST = TRACK_LIST[:2]\n",
    "print(TRACK_LIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_model_config = OmegaConf.load(PR_MODEL_CONFIG_PATH)\n",
    "pr_model = instantiate(pr_model_config)\n",
    "pr_model.load_state_dict(torch.load(PR_WEIGHTS_PATH))\n",
    "pr_model = pr_model.to(DEVICE)\n",
    "pr_model.eval();\n",
    "\n",
    "reg_model_config = OmegaConf.load(REGISTRATION_MODEL_CONFIG_PATH)\n",
    "reg_model = instantiate(reg_model_config)\n",
    "reg_model.load_state_dict(torch.load(REGISTRATION_WEIGHTS_PATH))\n",
    "reg_model = reg_model.to(DEVICE)\n",
    "reg_model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate descriptors for databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = NCLTDataset(\n",
    "    dataset_root=DATASET_ROOT,\n",
    "    subset=\"test\",\n",
    "    data_to_load=SENSOR_SUITE,\n",
    "    pointcloud_quantization_size=0.5,\n",
    "    max_point_distance=None,\n",
    ")\n",
    "dataset.dataset_df = dataset.dataset_df[dataset.dataset_df[\"track\"].isin(TRACK_LIST)]\n",
    "dataset.dataset_df.reset_index(inplace=True)\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    collate_fn=dataset.collate_fn,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:04<00:00,  4.10it/s]\n"
     ]
    }
   ],
   "source": [
    "descriptors = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(dataloader):\n",
    "        batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "        final_descriptor = pr_model(batch)[\"final_descriptor\"]\n",
    "        descriptors.append(final_descriptor.detach().cpu().numpy())\n",
    "\n",
    "descriptors = np.concatenate(descriptors, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving database indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved index /home/docker_opr/Datasets/OpenPlaceRecognition/NCLT_preprocessed/2012-01-08/index.faiss\n",
      "Saved index /home/docker_opr/Datasets/OpenPlaceRecognition/NCLT_preprocessed/2012-01-22/index.faiss\n"
     ]
    }
   ],
   "source": [
    "dataset_df = dataset.dataset_df\n",
    "\n",
    "for track, indices in dataset_df.groupby(\"track\").groups.items():\n",
    "    track_descriptors = descriptors[indices]\n",
    "    track_index = faiss.IndexFlatL2(track_descriptors.shape[1])\n",
    "    track_index.add(track_descriptors)\n",
    "    faiss.write_index(track_index, f\"{DATASET_ROOT}/{track}/index.faiss\")\n",
    "    print(f\"Saved index {DATASET_ROOT}/{track}/index.faiss\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/274 [00:00<?, ?it/s]/home/docker_opr/OpenPlaceRecognition/third_party/HRegNet/hregnet/utils.py:24: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  output = torch.cuda.IntTensor(B, npoint)\n",
      "100%|██████████| 274/274 [00:35<00:00,  7.83it/s]\n",
      "100%|██████████| 330/330 [00:42<00:00,  7.73it/s]\n"
     ]
    }
   ],
   "source": [
    "RECALL_THRESHOLD = 25.0\n",
    "\n",
    "test_csv = pd.read_csv(Path(DATASET_ROOT) / \"test.csv\", index_col=0)\n",
    "\n",
    "all_pr_recalls = []\n",
    "all_reg_recalls = []  # it is recall after registration (if estimated pose within RECALL_THRESHOLD), do not confuse with registration recall\n",
    "\n",
    "all_mean_pr_rotation_errors = []\n",
    "all_mean_pr_translation_errors = []\n",
    "\n",
    "all_median_pr_rotation_errors = []\n",
    "all_median_pr_translation_errors = []\n",
    "\n",
    "all_mean_reg_rotation_errors = []\n",
    "all_mean_reg_translation_errors = []\n",
    "\n",
    "all_median_reg_rotation_errors = []\n",
    "all_median_reg_translation_errors = []\n",
    "\n",
    "all_times = []\n",
    "\n",
    "correct_examples = []  # the most representative correct pairs\n",
    "pr_incorrect_examples = []  # the most representative incorrect pairs where place recognition failed\n",
    "reg_incorrect_examples = []  # the most representative incorrect pairs where registration failed\n",
    "\n",
    "for db_track in TRACK_LIST:\n",
    "    pr_pipe = PlaceRecognitionPipeline(\n",
    "        database_dir=Path(DATASET_ROOT) / db_track,\n",
    "        model=pr_model,\n",
    "        model_weights_path=PR_WEIGHTS_PATH,\n",
    "        device=DEVICE,\n",
    "    )\n",
    "    for query_track in TRACK_LIST:\n",
    "        if db_track == query_track:\n",
    "            continue\n",
    "\n",
    "        reg_pipe = SequencePointcloudRegistrationPipeline(\n",
    "            model=reg_model,\n",
    "            model_weights_path=REGISTRATION_WEIGHTS_PATH,\n",
    "            device=DEVICE,\n",
    "            voxel_downsample_size=0.3,\n",
    "            num_points_downsample=8192,\n",
    "        )\n",
    "        loc_pipe = LocalizationPipeline(\n",
    "            place_recognition_pipeline=pr_pipe,\n",
    "            registration_pipeline=reg_pipe,\n",
    "            precomputed_reg_feats=False,\n",
    "            pointclouds_subdir=\"velodyne_data\",\n",
    "        )\n",
    "\n",
    "        query_dataset = copy.deepcopy(dataset)\n",
    "        query_dataset.dataset_df = query_dataset.dataset_df[query_dataset.dataset_df[\"track\"] == query_track].reset_index(drop=True)\n",
    "        query_df = pd.read_csv(Path(DATASET_ROOT) / query_track / \"track.csv\", index_col=0)\n",
    "        query_df = query_df[query_df['image'].isin(query_dataset.dataset_df['image'])].reset_index(drop=True)\n",
    "\n",
    "        db_dataset = copy.deepcopy(dataset)\n",
    "        db_dataset.dataset_df = db_dataset.dataset_df[db_dataset.dataset_df[\"track\"] == db_track].reset_index(drop=True)\n",
    "        db_df = pd.read_csv(Path(DATASET_ROOT) / db_track / \"track.csv\", index_col=0)\n",
    "        db_df = db_df[db_df['image'].isin(db_dataset.dataset_df['image'])].reset_index(drop=True)\n",
    "\n",
    "        loc_pipe.pr_pipe.database_df = db_df\n",
    "        loc_pipe.database_df = db_df\n",
    "\n",
    "        pr_matches = []\n",
    "        pr_rotation_errors = []\n",
    "        pr_translation_errors = []\n",
    "\n",
    "        reg_matches = []\n",
    "        reg_rotation_errors = []\n",
    "        reg_translation_errors = []\n",
    "\n",
    "        times = []\n",
    "\n",
    "        for q_i in tqdm(range(1, len(query_dataset))):\n",
    "            # query_seq = [query_dataset[q_i-1], query_dataset[q_i]]\n",
    "            query_seq = [query_dataset[q_i]]\n",
    "            query = query_seq[-1]\n",
    "            query_pose = query_df.iloc[q_i][[\"tx\", \"ty\", \"tz\", \"qx\", \"qy\", \"qz\", \"qw\"]].to_numpy()\n",
    "\n",
    "            t = time()\n",
    "            output = loc_pipe.infer(query_seq)\n",
    "            times.append(time() - t)\n",
    "\n",
    "            pr_rotation_error, pr_translation_error = compute_error(output[\"db_match_pose\"], query_pose)\n",
    "            reg_rotation_error, reg_translation_error = compute_error(output[\"estimated_pose\"], query_pose)\n",
    "\n",
    "            pr_correct = pr_translation_error < RECALL_THRESHOLD\n",
    "            reg_correct = reg_translation_error < RECALL_THRESHOLD\n",
    "\n",
    "            pr_matches.append(pr_correct)\n",
    "            pr_rotation_errors.append(pr_rotation_error)\n",
    "            pr_translation_errors.append(pr_translation_error)\n",
    "\n",
    "            reg_matches.append(reg_correct)\n",
    "            reg_rotation_errors.append(reg_rotation_error)\n",
    "            reg_translation_errors.append(reg_translation_error)\n",
    "\n",
    "            if pr_correct and reg_correct \\\n",
    "                and reg_rotation_error < pr_rotation_error and reg_translation_error < pr_translation_error \\\n",
    "                and reg_rotation_error < 3.0 and reg_translation_error < 1.0:  # below median\n",
    "                query[\"pose\"] = query_pose\n",
    "                db_match = db_dataset[output[\"db_match_idx\"]]\n",
    "                db_match[\"pose\"] = output[\"db_match_pose\"]\n",
    "                correct_examples.append((query, db_match, output[\"estimated_pose\"]))\n",
    "\n",
    "            if pr_correct and not reg_correct:\n",
    "                query[\"pose\"] = query_pose\n",
    "                db_match = db_dataset[output[\"db_match_idx\"]]\n",
    "                db_match[\"pose\"] = output[\"db_match_pose\"]\n",
    "                reg_incorrect_examples.append((query, db_match, output[\"estimated_pose\"]))\n",
    "\n",
    "            if not pr_correct and pr_translation_error > 50.0:\n",
    "                query[\"pose\"] = query_pose\n",
    "                db_match = db_dataset[output[\"db_match_idx\"]]\n",
    "                db_match[\"pose\"] = output[\"db_match_pose\"]\n",
    "                pr_incorrect_examples.append((query, db_match, output[\"estimated_pose\"]))\n",
    "\n",
    "        all_pr_recalls.append(np.mean(pr_matches))\n",
    "        all_reg_recalls.append(np.mean(reg_matches))\n",
    "\n",
    "        all_mean_pr_rotation_errors.append(np.mean(pr_rotation_errors))\n",
    "        all_mean_pr_translation_errors.append(np.mean(pr_translation_errors))\n",
    "        all_median_pr_rotation_errors.append(np.median(pr_rotation_errors))\n",
    "        all_median_pr_translation_errors.append(np.median(pr_translation_errors))\n",
    "\n",
    "        all_mean_reg_rotation_errors.append(np.mean(reg_rotation_errors))\n",
    "        all_mean_reg_translation_errors.append(np.mean(reg_translation_errors))\n",
    "        all_median_reg_rotation_errors.append(np.median(reg_rotation_errors))\n",
    "        all_median_reg_translation_errors.append(np.median(reg_translation_errors))\n",
    "        all_times.extend(times[1:]) # drop the first iteration cause it is always slower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(73, 27, 11)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(correct_examples), len(pr_incorrect_examples), len(reg_incorrect_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_str = f\"\"\"Average PR Recall@1:   {np.mean(all_pr_recalls)*100:.2f}\n",
    "Average REG Recall@1:  {np.mean(all_reg_recalls)*100:.2f}\n",
    "\n",
    "Average Mean RRE PR:   {np.mean(all_mean_pr_rotation_errors):.2f}\n",
    "Average Mean RTE PR:   {np.mean(all_mean_pr_translation_errors):.2f}\n",
    "Average Median RRE PR:   {np.mean(all_median_pr_rotation_errors):.2f}\n",
    "Average Median RTE PR:   {np.mean(all_median_pr_translation_errors):.2f}\n",
    "\n",
    "Average Mean RRE REG:  {np.mean(all_mean_reg_rotation_errors):.2f}\n",
    "Average Mean RTE REG:  {np.mean(all_mean_reg_translation_errors):.2f}\n",
    "Average Median RRE REG:  {np.mean(all_median_reg_rotation_errors):.2f}\n",
    "Average Median RTE REG:  {np.mean(all_median_reg_translation_errors):.2f}\n",
    "\n",
    "Mean inference time (PR + REG):     {np.mean(all_times)*1000:.2f} ms\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PR Recall@1:   92.78\n",
      "Average REG Recall@1:  91.75\n",
      "\n",
      "Average Mean RRE PR:   11.52\n",
      "Average Mean RTE PR:   12.24\n",
      "Average Median RRE PR:   6.59\n",
      "Average Median RTE PR:   4.46\n",
      "\n",
      "Average Mean RRE REG:  12.99\n",
      "Average Mean RTE REG:  13.23\n",
      "Average Median RRE REG:  6.16\n",
      "Average Median RTE REG:  4.51\n",
      "\n",
      "Mean inference time (PR + REG):     107.66 ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(results_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean registration time: 50.00 ms\n"
     ]
    }
   ],
   "source": [
    "loc_pipe.reg_pipe.stats_history.keys()\n",
    "print(f\"Mean registration time: {np.mean(loc_pipe.reg_pipe.stats_history['total_time'][1:]) * 1000:.2f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_example = correct_examples[10]\n",
    "\n",
    "query_sample, db_match_sample, estimated_pose = correct_example\n",
    "\n",
    "# draw_pc_pair(\n",
    "#     query_sample[\"pointcloud_lidar_coords\"],\n",
    "#     estimated_pose,\n",
    "#     db_match_sample[\"pointcloud_lidar_coords\"],\n",
    "#     db_match_sample[\"pose\"],\n",
    "# )\n",
    "\n",
    "ts.show([\n",
    "    query_sample[\"image_Cam5\"], query_sample[\"image_Cam2\"],\n",
    "])\n",
    "ts.show([\n",
    "    db_match_sample[\"image_Cam5\"], db_match_sample[\"image_Cam2\"],\n",
    "])\n",
    "print(f\"PR pose error: {compute_error(db_match_sample['pose'], query_sample['pose'])}\")\n",
    "print(f\"REG pose error: {compute_error(estimated_pose, query_sample['pose'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_incorrect_example = pr_incorrect_examples[0]\n",
    "\n",
    "query_sample, db_match_sample, estimated_pose = pr_incorrect_example\n",
    "\n",
    "# draw_pc_pair(\n",
    "#     query_sample[\"pointcloud_lidar_coords\"],\n",
    "#     estimated_pose,\n",
    "#     db_match_sample[\"pointcloud_lidar_coords\"],\n",
    "#     db_match_sample[\"pose\"],\n",
    "# )\n",
    "\n",
    "ts.show([\n",
    "    query_sample[\"image_Cam5\"], query_sample[\"image_Cam2\"],\n",
    "])\n",
    "ts.show([\n",
    "    db_match_sample[\"image_Cam5\"], db_match_sample[\"image_Cam2\"],\n",
    "])\n",
    "print(f\"PR pose error: {compute_error(db_match_sample['pose'], query_sample['pose'])}\")\n",
    "print(f\"REG pose error: {compute_error(estimated_pose, query_sample['pose'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_incorrect_example = reg_incorrect_examples[0]\n",
    "\n",
    "query_sample, db_match_sample, estimated_pose = reg_incorrect_example\n",
    "\n",
    "# draw_pc_pair(\n",
    "#     query_sample[\"pointcloud_lidar_coords\"],\n",
    "#     estimated_pose,\n",
    "#     db_match_sample[\"pointcloud_lidar_coords\"],\n",
    "#     db_match_sample[\"pose\"],\n",
    "# )\n",
    "\n",
    "ts.show([\n",
    "    query_sample[\"image_Cam5\"], query_sample[\"image_Cam2\"],\n",
    "])\n",
    "ts.show([\n",
    "    db_match_sample[\"image_Cam5\"], db_match_sample[\"image_Cam2\"],\n",
    "])\n",
    "print(f\"PR pose error: {compute_error(db_match_sample['pose'], query_sample['pose'])}\")\n",
    "print(f\"REG pose error: {compute_error(estimated_pose, query_sample['pose'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
