{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PlaceRecognitionPipeline with SOC - ITLP dataset test\n",
    "\n",
    "A module that implements a neural network algorithm for searching a database of places already visited by a vehicle for the most similar records using data from lidars and cameras and highlighting special elements of a three-dimensional scene (doors, buildings, street signs, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:albumentations.check_version:A new version of Albumentations is available: 1.4.24 (you have 1.4.10). Upgrade using: pip install --upgrade albumentations\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "from pathlib import Path\n",
    "from time import time\n",
    "\n",
    "import faiss\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "import open3d.core as o3c\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchshow as ts\n",
    "\n",
    "from hydra.utils import instantiate\n",
    "from omegaconf import OmegaConf\n",
    "from scipy.spatial.transform import Rotation\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from geotransformer.utils.registration import compute_registration_error\n",
    "\n",
    "\n",
    "from opr.datasets.itlp import ITLPCampus\n",
    "from opr.pipelines.place_recognition import PlaceRecognitionPipeline\n",
    "from opr.pipelines.registration import PointcloudRegistrationPipeline\n",
    "from opr.pipelines.localization import LocalizationPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"DISPLAY\"] = \":1\"\n",
    "\n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pose_to_matrix(pose):\n",
    "    \"\"\"From the 6D poses in the [tx ty tz qx qy qz qw] format to 4x4 pose matrices.\"\"\"\n",
    "    position = pose[:3]\n",
    "    orientation_quat = pose[3:]\n",
    "    rotation = Rotation.from_quat(orientation_quat)\n",
    "    pose_matrix = np.eye(4)\n",
    "    pose_matrix[:3,:3] = rotation.as_matrix()\n",
    "    pose_matrix[:3,3] = position\n",
    "    return pose_matrix\n",
    "\n",
    "\n",
    "# def compute_error(estimated_pose, gt_pose):\n",
    "#     \"\"\"For the 6D poses in the [tx ty tz qx qy qz qw] format.\"\"\"\n",
    "#     estimated_pose = pose_to_matrix(estimated_pose)\n",
    "#     gt_pose = pose_to_matrix(gt_pose)\n",
    "#     return compute_registration_error(estimated_pose, gt_pose)\n",
    "\n",
    "def compute_error(estimated_pose, gt_pose):\n",
    "    \"\"\"For the 6D poses in the [tx ty tz qx qy qz qw] format.\"\"\"\n",
    "    estimated_pose = pose_to_matrix(estimated_pose)\n",
    "    gt_pose = pose_to_matrix(gt_pose)\n",
    "    error_pose = np.linalg.inv(estimated_pose) @ gt_pose\n",
    "    dist_error = np.sum(error_pose[:3, 3]**2) ** 0.5\n",
    "    r = Rotation.from_matrix(error_pose[:3, :3])\n",
    "    rotvec = r.as_rotvec()\n",
    "    angle_error = (np.sum(rotvec**2)**0.5) * 180 / np.pi\n",
    "    angle_error = abs(90 - abs(angle_error-90))\n",
    "    return angle_error, dist_error\n",
    "\n",
    "\n",
    "def draw_pc(pc: Tensor, color: str = \"blue\"):\n",
    "    pc_o3d = o3c.Tensor.from_dlpack(torch.utils.dlpack.to_dlpack(pc))\n",
    "    pcd = o3d.t.geometry.PointCloud(pc_o3d)\n",
    "    if color == \"blue\":\n",
    "        c = [0.0, 0.0, 1.0]\n",
    "    elif color == \"red\":\n",
    "        c = [1.0, 0.0, 0.0]\n",
    "    else:\n",
    "        c = [0.0, 1.0, 0.0]\n",
    "    pcd = pcd.paint_uniform_color(c)\n",
    "    o3d.visualization.draw_geometries(\n",
    "        [pcd.to_legacy()],\n",
    "    )\n",
    "\n",
    "\n",
    "def invert_rigid_transformation_matrix(T: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Inverts a 4x4 rigid body transformation matrix.\n",
    "\n",
    "    Args:\n",
    "        T (np.ndarray): A 4x4 rigid body transformation matrix.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The inverted 4x4 rigid body transformation matrix.\n",
    "    \"\"\"\n",
    "    assert T.shape == (4, 4), \"Input matrix must be 4x4.\"\n",
    "\n",
    "    R = T[:3, :3]\n",
    "    t = T[:3, 3]\n",
    "\n",
    "    R_inv = R.T\n",
    "    t_inv = -R.T @ t\n",
    "\n",
    "    T_inv = np.eye(4)\n",
    "    T_inv[:3, :3] = R_inv\n",
    "    T_inv[:3, 3] = t_inv\n",
    "\n",
    "    return T_inv\n",
    "\n",
    "\n",
    "def draw_pc_pair(\n",
    "    pc_blue: Tensor, pc_blue_pose: np.ndarray | Tensor, pc_red: Tensor, pc_red_pose: np.ndarray | Tensor\n",
    "):\n",
    "    pc_blue_o3d = o3c.Tensor.from_dlpack(torch.utils.dlpack.to_dlpack(copy.deepcopy(pc_blue)))\n",
    "    pc_red_o3d = o3c.Tensor.from_dlpack(torch.utils.dlpack.to_dlpack(copy.deepcopy(pc_red)))\n",
    "\n",
    "    blue_pcd = o3d.t.geometry.PointCloud(pc_blue_o3d)\n",
    "    blue_pcd_tmp = copy.deepcopy(blue_pcd)\n",
    "\n",
    "    red_pcd = o3d.t.geometry.PointCloud(pc_red_o3d)\n",
    "    red_pcd_tmp = copy.deepcopy(red_pcd)\n",
    "\n",
    "    blue_pcd_tmp.voxel_down_sample(voxel_size=0.3)\n",
    "    # blue_pcd_tmp.transform(pose_to_matrix(pc_blue_pose))\n",
    "    blue_pcd_tmp = blue_pcd_tmp.paint_uniform_color([0.0, 0.0, 1.0])\n",
    "\n",
    "    red_pcd_tmp.voxel_down_sample(voxel_size=0.3)\n",
    "    red_pcd_tmp.transform(pose_to_matrix(pc_red_pose))\n",
    "    red_pcd_tmp.transform(invert_rigid_transformation_matrix(pose_to_matrix(pc_blue_pose)))\n",
    "    red_pcd_tmp = red_pcd_tmp.paint_uniform_color([1.0, 0.0, 0.0])\n",
    "    o3d.visualization.draw_geometries(\n",
    "        [blue_pcd_tmp.to_legacy(), red_pcd_tmp.to_legacy()],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can download the dataset:\n",
    "\n",
    "- Kaggle:\n",
    "  - [ITLP Campus Outdoor](https://www.kaggle.com/datasets/alexandermelekhin/itlp-campus-outdoor)\n",
    "- Hugging Face:\n",
    "  - [ITLP Campus Outdoor](https://huggingface.co/datasets/OPR-Project/ITLP-Campus-Outdoor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test track list:\n",
      "['05_2023-08-15-day', '06_2023-08-18-night', '07_2023-10-04-day', '08_2023-10-11-night']\n"
     ]
    }
   ],
   "source": [
    "DATASET_ROOT = \"/home/docker_opr/Datasets/OpenPlaceRecognition/itlp_campus_outdoor\"\n",
    "\n",
    "TRACK_LIST = sorted([str(subdir.name) for subdir in Path(DATASET_ROOT).iterdir() if subdir.is_dir()])\n",
    "TRACK_LIST = TRACK_LIST[5:]\n",
    "\n",
    "print(\"Test track list:\")\n",
    "print(TRACK_LIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Original version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SENSOR_SUITE = [\"front_cam\", \"back_cam\", \"lidar\"]\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "NUM_WORKERS = 4\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "PR_MODEL_CONFIG_PATH = \"../../configs/model/place_recognition/multimodal_with_soc_outdoor.yaml\"\n",
    "PR_WEIGHTS_PATH = \"../../weights/place_recognition/multimodal_with_soc_outdoor_nclt.pth\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_model_config = OmegaConf.load(PR_MODEL_CONFIG_PATH)\n",
    "pr_model = instantiate(pr_model_config)\n",
    "pr_model.load_state_dict(torch.load(PR_WEIGHTS_PATH))\n",
    "pr_model = pr_model.to(DEVICE)\n",
    "pr_model.eval();\n",
    "\n",
    "sensors_cfg = OmegaConf.load(\"/home/docker_opr/OpenPlaceRecognition/configs/dataset/sensors_cfg/husky.yaml\")\n",
    "anno_cfg = OmegaConf.load(\"/home/docker_opr/OpenPlaceRecognition/configs/dataset/anno/oneformer.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate descriptors for databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ITLPCampus(\n",
    "    dataset_root=DATASET_ROOT,\n",
    "    subset=\"test\",\n",
    "    csv_file=\"test.csv\",\n",
    "    sensors=SENSOR_SUITE,\n",
    "    load_semantics=False,\n",
    "    load_soc=True,\n",
    "    sensors_cfg=sensors_cfg,\n",
    "    anno=anno_cfg,\n",
    ")\n",
    "dataset.dataset_df = dataset.dataset_df[dataset.dataset_df[\"track\"].isin(TRACK_LIST)]\n",
    "dataset.dataset_df.reset_index(inplace=True)\n",
    "\n",
    "no_masks = []\n",
    "no_masks_filenames = []\n",
    "\n",
    "for index, row in dataset.dataset_df.iterrows():\n",
    "    mask_path = f\"{DATASET_ROOT}/{row['track']}/masks/back_cam/{row['back_cam_ts']}.png\"\n",
    "    filename = f\"{row['back_cam_ts']}.png\"\n",
    "    if not Path(mask_path).exists():\n",
    "        no_masks.append(index)\n",
    "        no_masks_filenames.append(filename)\n",
    "\n",
    "dataset.dataset_df.drop(no_masks, inplace=True)\n",
    "dataset.dataset_df.reset_index(inplace=True)\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    collate_fn=dataset.collate_fn,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(no_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 153/153 [01:26<00:00,  1.77it/s]\n"
     ]
    }
   ],
   "source": [
    "descriptors = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(dataloader):\n",
    "        batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "        final_descriptor = pr_model(batch)[\"final_descriptor\"]\n",
    "        descriptors.append(final_descriptor.detach().cpu().numpy())\n",
    "\n",
    "descriptors = np.concatenate(descriptors, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving database indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved index /home/docker_opr/Datasets/OpenPlaceRecognition/itlp_campus_outdoor_part2/05_2023-08-15-day/index.faiss\n",
      "Saved index /home/docker_opr/Datasets/OpenPlaceRecognition/itlp_campus_outdoor_part2/06_2023-08-18-night/index.faiss\n",
      "Saved index /home/docker_opr/Datasets/OpenPlaceRecognition/itlp_campus_outdoor_part2/07_2023-10-04-day/index.faiss\n",
      "Saved index /home/docker_opr/Datasets/OpenPlaceRecognition/itlp_campus_outdoor_part2/08_2023-10-11-night/index.faiss\n"
     ]
    }
   ],
   "source": [
    "dataset_df = dataset.dataset_df\n",
    "\n",
    "for track, indices in dataset_df.groupby(\"track\").groups.items():\n",
    "    track_descriptors = descriptors[indices]\n",
    "    track_index = faiss.IndexFlatL2(track_descriptors.shape[1])\n",
    "    track_index.add(track_descriptors)\n",
    "    faiss.write_index(track_index, f\"{DATASET_ROOT}/{track}/index.faiss\")\n",
    "    print(f\"Saved index {DATASET_ROOT}/{track}/index.faiss\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "156it [00:48,  3.23it/s]\n",
      "152it [00:46,  3.29it/s]\n",
      "150it [00:46,  3.19it/s]\n",
      "152it [00:47,  3.21it/s]\n",
      "152it [00:44,  3.40it/s]\n",
      "150it [00:51,  2.93it/s]\n",
      "152it [00:44,  3.42it/s]\n",
      "156it [00:49,  3.16it/s]\n",
      "150it [00:50,  2.95it/s]\n",
      "152it [00:50,  2.99it/s]\n",
      "156it [01:04,  2.43it/s]\n",
      "152it [00:47,  3.19it/s]\n"
     ]
    }
   ],
   "source": [
    "RECALL_THRESHOLD = 25.0\n",
    "\n",
    "test_csv = pd.read_csv(Path(DATASET_ROOT) / \"test.csv\", index_col=0)\n",
    "\n",
    "all_pr_recalls = []\n",
    "\n",
    "all_mean_pr_rotation_errors = []\n",
    "all_mean_pr_translation_errors = []\n",
    "\n",
    "all_median_pr_rotation_errors = []\n",
    "all_median_pr_translation_errors = []\n",
    "\n",
    "all_times = []\n",
    "\n",
    "correct_examples = []  # the most representative correct pairs\n",
    "pr_incorrect_examples = []  # the most representative incorrect pairs where place recognition failed\n",
    "\n",
    "for db_track in TRACK_LIST:\n",
    "    pr_pipe = PlaceRecognitionPipeline(\n",
    "        database_dir=Path(DATASET_ROOT) / db_track,\n",
    "        model=pr_model,\n",
    "        model_weights_path=None,\n",
    "        device=DEVICE,\n",
    "    )\n",
    "    for query_track in TRACK_LIST:\n",
    "        if db_track == query_track:\n",
    "            continue\n",
    "\n",
    "        query_dataset = copy.deepcopy(dataset)\n",
    "        query_dataset.dataset_df = query_dataset.dataset_df[query_dataset.dataset_df[\"track\"] == query_track].reset_index(drop=True)\n",
    "        query_df = query_dataset.dataset_df\n",
    "\n",
    "        db_dataset = copy.deepcopy(dataset)\n",
    "        db_dataset.dataset_df = db_dataset.dataset_df[db_dataset.dataset_df[\"track\"] == db_track].reset_index(drop=True)\n",
    "        db_df = db_dataset.dataset_df\n",
    "\n",
    "        pr_pipe.database_df = db_df\n",
    "\n",
    "        pr_matches = []\n",
    "        pr_rotation_errors = []\n",
    "        pr_translation_errors = []\n",
    "\n",
    "        times = []\n",
    "\n",
    "        for q_i, query in tqdm(enumerate(query_dataset)):\n",
    "            query_pose = query_df.iloc[q_i][[\"tx\", \"ty\", \"tz\", \"qx\", \"qy\", \"qz\", \"qw\"]].to_numpy()\n",
    "\n",
    "            t = time()\n",
    "            output = pr_pipe.infer(query)\n",
    "            torch.cuda.current_stream().synchronize()\n",
    "            times.append(time() - t)\n",
    "\n",
    "            pr_rotation_error, pr_translation_error = compute_error(output[\"pose\"], query_pose)\n",
    "\n",
    "            pr_correct = pr_translation_error < RECALL_THRESHOLD\n",
    "\n",
    "            pr_matches.append(pr_correct)\n",
    "            pr_rotation_errors.append(pr_rotation_error)\n",
    "            pr_translation_errors.append(pr_translation_error)\n",
    "\n",
    "            if pr_correct and pr_translation_error < 5.0:  # close enough\n",
    "                query[\"pose\"] = query_pose\n",
    "                db_match = db_dataset[output[\"idx\"]]\n",
    "                db_match[\"pose\"] = output[\"pose\"]\n",
    "                correct_examples.append((query, db_match))\n",
    "\n",
    "            if not pr_correct and pr_translation_error > 50.0:  # very far from GT\n",
    "                query[\"pose\"] = query_pose\n",
    "                db_match = db_dataset[output[\"idx\"]]\n",
    "                db_match[\"pose\"] = output[\"pose\"]\n",
    "                pr_incorrect_examples.append((query, db_match))\n",
    "\n",
    "        all_pr_recalls.append(np.mean(pr_matches))\n",
    "\n",
    "        all_mean_pr_rotation_errors.append(np.mean(pr_rotation_errors))\n",
    "        all_mean_pr_translation_errors.append(np.mean(pr_translation_errors))\n",
    "        all_median_pr_rotation_errors.append(np.median(pr_rotation_errors))\n",
    "        all_median_pr_translation_errors.append(np.median(pr_translation_errors))\n",
    "\n",
    "        all_times.extend(times[1:]) # drop the first iteration cause it is always slower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1280, 47)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(correct_examples), len(pr_incorrect_examples), #len(reg_incorrect_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_str = f\"\"\"Average PR Recall@1:   {np.mean(all_pr_recalls)*100:.2f}\n",
    "\n",
    "Average Mean RRE PR:   {np.mean(all_mean_pr_rotation_errors):.2f}\n",
    "Average Mean RTE PR:   {np.mean(all_mean_pr_translation_errors):.2f}\n",
    "Average Median RRE PR:   {np.mean(all_median_pr_rotation_errors):.2f}\n",
    "Average Median RTE PR:   {np.mean(all_median_pr_translation_errors):.2f}\n",
    "\n",
    "Mean inference time:     {np.mean(all_times)*1000:.2f} ms\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PR Recall@1:   96.39\n",
      "\n",
      "Average Mean RRE PR:   10.07\n",
      "Average Mean RTE PR:   6.48\n",
      "Average Median RRE PR:   5.13\n",
      "Average Median RTE PR:   3.38\n",
      "\n",
      "Mean inference time:     38.63 ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(results_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_example = correct_examples[100]\n",
    "\n",
    "query_sample, db_match_sample = correct_example\n",
    "\n",
    "ts.show([\n",
    "    query_sample[\"image_front_cam\"], query_sample[\"image_back_cam\"],\n",
    "])\n",
    "ts.show([\n",
    "    db_match_sample[\"image_front_cam\"], db_match_sample[\"image_back_cam\"],\n",
    "])\n",
    "print(f\"PR pose error: {compute_error(db_match_sample['pose'], query_sample['pose'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_incorrect_example = pr_incorrect_examples[20]\n",
    "\n",
    "query_sample, db_match_sample = pr_incorrect_example\n",
    "\n",
    "ts.show([\n",
    "    query_sample[\"image_front_cam\"], query_sample[\"image_back_cam\"],\n",
    "])\n",
    "ts.show([\n",
    "    db_match_sample[\"image_front_cam\"], db_match_sample[\"image_back_cam\"],\n",
    "])\n",
    "print(f\"PR pose error: {compute_error(db_match_sample['pose'], query_sample['pose'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improved version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "SENSOR_SUITE = [\"front_cam\", \"back_cam\", \"lidar\"]\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "NUM_WORKERS = 4\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "PR_MODEL_CONFIG_PATH = \"../../configs/model/place_recognition/multimodal_with_soc_outdoor.yaml\"\n",
    "PR_WEIGHTS_PATH = \"../../weights/place_recognition/multimodal_with_soc_outdoor_itlp-finetune.pth\"\n",
    "\n",
    "REGISTRATION_MODEL_CONFIG_PATH = \"../../configs/model/registration/hregnet_light_feats.yaml\"\n",
    "REGISTRATION_WEIGHTS_PATH = \"../../weights/registration/hregnet_light_feats_nuscenes.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_model_config = OmegaConf.load(PR_MODEL_CONFIG_PATH)\n",
    "pr_model = instantiate(pr_model_config)\n",
    "pr_model.load_state_dict(torch.load(PR_WEIGHTS_PATH))\n",
    "pr_model = pr_model.to(DEVICE)\n",
    "pr_model.eval();\n",
    "\n",
    "reg_model_config = OmegaConf.load(REGISTRATION_MODEL_CONFIG_PATH)\n",
    "reg_model = instantiate(reg_model_config)\n",
    "reg_model.load_state_dict(torch.load(REGISTRATION_WEIGHTS_PATH))\n",
    "reg_model = reg_model.to(DEVICE)\n",
    "reg_model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate descriptors for databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_cfg = OmegaConf.load(\"/home/docker_opr/OpenPlaceRecognition/configs/dataset/sensors_cfg/husky.yaml\")\n",
    "anno_cfg = OmegaConf.load(\"/home/docker_opr/OpenPlaceRecognition/configs/dataset/anno/oneformer.yaml\")\n",
    "\n",
    "dataset = ITLPCampus(\n",
    "    dataset_root=DATASET_ROOT,\n",
    "    subset=\"test\",\n",
    "    csv_file=\"test.csv\",\n",
    "    sensors=SENSOR_SUITE,\n",
    "    load_semantics=False,\n",
    "    load_soc=True,\n",
    "    sensors_cfg=sensors_cfg,\n",
    "    anno=anno_cfg,\n",
    ")\n",
    "\n",
    "dataset.dataset_df = dataset.dataset_df[dataset.dataset_df[\"track\"].isin(TRACK_LIST)]\n",
    "dataset.dataset_df.reset_index(inplace=True)\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    collate_fn=dataset.collate_fn,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 153/153 [01:06<00:00,  2.32it/s]\n"
     ]
    }
   ],
   "source": [
    "descriptors = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(dataloader):\n",
    "        batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "        final_descriptor = pr_model(batch)[\"final_descriptor\"]\n",
    "        descriptors.append(final_descriptor.detach().cpu().numpy())\n",
    "\n",
    "descriptors = np.concatenate(descriptors, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving database indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved index /home/docker_opr/Datasets/OpenPlaceRecognition/itlp_campus_outdoor_part2/05_2023-08-15-day/index.faiss\n",
      "Saved index /home/docker_opr/Datasets/OpenPlaceRecognition/itlp_campus_outdoor_part2/06_2023-08-18-night/index.faiss\n",
      "Saved index /home/docker_opr/Datasets/OpenPlaceRecognition/itlp_campus_outdoor_part2/07_2023-10-04-day/index.faiss\n",
      "Saved index /home/docker_opr/Datasets/OpenPlaceRecognition/itlp_campus_outdoor_part2/08_2023-10-11-night/index.faiss\n"
     ]
    }
   ],
   "source": [
    "dataset_df = dataset.dataset_df\n",
    "\n",
    "for track, indices in dataset_df.groupby(\"track\").groups.items():\n",
    "    track_descriptors = descriptors[indices]\n",
    "    track_index = faiss.IndexFlatL2(track_descriptors.shape[1])\n",
    "    track_index.add(track_descriptors)\n",
    "    faiss.write_index(track_index, f\"{DATASET_ROOT}/{track}/index.faiss\")\n",
    "    print(f\"Saved index {DATASET_ROOT}/{track}/index.faiss\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/home/docker_opr/.local/lib/python3.10/site-packages/hregnet/utils.py:24: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  output = torch.cuda.IntTensor(B, npoint)\n",
      "156it [00:38,  4.02it/s]\n",
      "152it [00:37,  4.02it/s]\n",
      "150it [00:35,  4.25it/s]\n",
      "152it [00:37,  4.10it/s]\n",
      "152it [00:34,  4.35it/s]\n",
      "150it [00:36,  4.05it/s]\n",
      "152it [00:36,  4.17it/s]\n",
      "156it [00:36,  4.25it/s]\n",
      "150it [00:35,  4.18it/s]\n",
      "152it [00:35,  4.32it/s]\n",
      "156it [00:38,  4.10it/s]\n",
      "152it [00:35,  4.25it/s]\n"
     ]
    }
   ],
   "source": [
    "RECALL_THRESHOLD = 25.0\n",
    "\n",
    "test_csv = pd.read_csv(Path(DATASET_ROOT) / \"test.csv\", index_col=0)\n",
    "\n",
    "all_pr_recalls = []\n",
    "all_reg_recalls = []  # it is recall after registration (if estimated pose within RECALL_THRESHOLD), do not confuse with registration recall\n",
    "\n",
    "all_mean_pr_rotation_errors = []\n",
    "all_mean_pr_translation_errors = []\n",
    "\n",
    "all_median_pr_rotation_errors = []\n",
    "all_median_pr_translation_errors = []\n",
    "\n",
    "all_mean_reg_rotation_errors = []\n",
    "all_mean_reg_translation_errors = []\n",
    "\n",
    "all_median_reg_rotation_errors = []\n",
    "all_median_reg_translation_errors = []\n",
    "\n",
    "all_times = []\n",
    "\n",
    "correct_examples = []  # the most representative correct pairs\n",
    "pr_incorrect_examples = []  # the most representative incorrect pairs where place recognition failed\n",
    "reg_incorrect_examples = []  # the most representative incorrect pairs where registration failed\n",
    "\n",
    "for db_track in TRACK_LIST:\n",
    "    pr_pipe = PlaceRecognitionPipeline(\n",
    "        database_dir=Path(DATASET_ROOT) / db_track,\n",
    "        model=pr_model,\n",
    "        model_weights_path=PR_WEIGHTS_PATH,\n",
    "        device=DEVICE,\n",
    "    )\n",
    "    for query_track in TRACK_LIST:\n",
    "        if db_track == query_track:\n",
    "            continue\n",
    "\n",
    "        reg_pipe = PointcloudRegistrationPipeline(\n",
    "            model=reg_model,\n",
    "            model_weights_path=REGISTRATION_WEIGHTS_PATH,\n",
    "            device=DEVICE,\n",
    "            voxel_downsample_size=0.3,\n",
    "            num_points_downsample=8192,\n",
    "        )\n",
    "        loc_pipe = LocalizationPipeline(\n",
    "            place_recognition_pipeline=pr_pipe,\n",
    "            registration_pipeline=reg_pipe,\n",
    "            precomputed_reg_feats=True,\n",
    "            pointclouds_subdir=\"lidar\",\n",
    "        )\n",
    "\n",
    "        query_dataset = copy.deepcopy(dataset)\n",
    "        query_dataset.dataset_df = query_dataset.dataset_df[query_dataset.dataset_df[\"track\"] == query_track].reset_index(drop=True)\n",
    "        query_df = query_dataset.dataset_df\n",
    "\n",
    "        db_dataset = copy.deepcopy(dataset)\n",
    "        db_dataset.dataset_df = db_dataset.dataset_df[db_dataset.dataset_df[\"track\"] == db_track].reset_index(drop=True)\n",
    "        db_df = db_dataset.dataset_df\n",
    "\n",
    "        loc_pipe.pr_pipe.database_df = db_df\n",
    "        loc_pipe.database_df = db_df\n",
    "\n",
    "\n",
    "        pr_matches = []\n",
    "        pr_rotation_errors = []\n",
    "        pr_translation_errors = []\n",
    "\n",
    "        reg_matches = []\n",
    "        reg_rotation_errors = []\n",
    "        reg_translation_errors = []\n",
    "\n",
    "        times = []\n",
    "\n",
    "        for q_i, query in tqdm(enumerate(query_dataset)):\n",
    "            query_pose = query_df.iloc[q_i][[\"tx\", \"ty\", \"tz\", \"qx\", \"qy\", \"qz\", \"qw\"]].to_numpy()\n",
    "\n",
    "            t = time()\n",
    "            output = loc_pipe.infer(query)\n",
    "            torch.cuda.current_stream().synchronize()\n",
    "            times.append(time() - t)\n",
    "\n",
    "            pr_rotation_error, pr_translation_error = compute_error(output[\"db_match_pose\"], query_pose)\n",
    "            reg_rotation_error, reg_translation_error = compute_error(output[\"estimated_pose\"], query_pose)\n",
    "\n",
    "            pr_correct = pr_translation_error < RECALL_THRESHOLD\n",
    "            reg_correct = reg_translation_error < RECALL_THRESHOLD\n",
    "\n",
    "            pr_matches.append(pr_correct)\n",
    "            pr_rotation_errors.append(pr_rotation_error)\n",
    "            pr_translation_errors.append(pr_translation_error)\n",
    "\n",
    "            reg_matches.append(reg_correct)\n",
    "            reg_rotation_errors.append(reg_rotation_error)\n",
    "            reg_translation_errors.append(reg_translation_error)\n",
    "\n",
    "            if pr_correct and reg_correct \\\n",
    "                and reg_rotation_error < pr_rotation_error and reg_translation_error < pr_translation_error \\\n",
    "                and reg_rotation_error < 3.0 and reg_translation_error < 1.0:\n",
    "                query[\"pose\"] = query_pose\n",
    "                db_match = db_dataset[output[\"db_match_idx\"]]\n",
    "                db_match[\"pose\"] = output[\"db_match_pose\"]\n",
    "                correct_examples.append((query, db_match, output[\"estimated_pose\"]))\n",
    "\n",
    "            if pr_correct and not reg_correct:\n",
    "                query[\"pose\"] = query_pose\n",
    "                db_match = db_dataset[output[\"db_match_idx\"]]\n",
    "                db_match[\"pose\"] = output[\"db_match_pose\"]\n",
    "                reg_incorrect_examples.append((query, db_match, output[\"estimated_pose\"]))\n",
    "\n",
    "            if not pr_correct and pr_translation_error > 50.0:\n",
    "                query[\"pose\"] = query_pose\n",
    "                db_match = db_dataset[output[\"db_match_idx\"]]\n",
    "                db_match[\"pose\"] = output[\"db_match_pose\"]\n",
    "                pr_incorrect_examples.append((query, db_match, output[\"estimated_pose\"]))\n",
    "\n",
    "        all_pr_recalls.append(np.mean(pr_matches))\n",
    "        all_reg_recalls.append(np.mean(reg_matches))\n",
    "\n",
    "        all_mean_pr_rotation_errors.append(np.mean(pr_rotation_errors))\n",
    "        all_mean_pr_translation_errors.append(np.mean(pr_translation_errors))\n",
    "        all_median_pr_rotation_errors.append(np.median(pr_rotation_errors))\n",
    "        all_median_pr_translation_errors.append(np.median(pr_translation_errors))\n",
    "\n",
    "        all_mean_reg_rotation_errors.append(np.mean(reg_rotation_errors))\n",
    "        all_mean_reg_translation_errors.append(np.mean(reg_translation_errors))\n",
    "        all_median_reg_rotation_errors.append(np.median(reg_rotation_errors))\n",
    "        all_median_reg_translation_errors.append(np.median(reg_translation_errors))\n",
    "        all_times.extend(times[1:]) # drop the first iteration cause it is always slower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35, 5, 1)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(correct_examples), len(pr_incorrect_examples), len(reg_incorrect_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_str = f\"\"\"Average REG Recall@1:  {np.mean(all_reg_recalls)*100:.2f}\n",
    "\n",
    "Average Mean RRE REG:  {np.mean(all_mean_reg_rotation_errors):.2f}\n",
    "Average Mean RTE REG:  {np.mean(all_mean_reg_translation_errors):.2f}\n",
    "Average Median RRE REG:  {np.mean(all_median_reg_rotation_errors):.2f}\n",
    "Average Median RTE REG:  {np.mean(all_median_reg_translation_errors):.2f}\n",
    "\n",
    "Mean inference time (PR + REG):     {np.mean(all_times)*1000:.2f} ms\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average REG Recall@1:  99.62\n",
      "\n",
      "Average Mean RRE REG:  9.24\n",
      "Average Mean RTE REG:  3.58\n",
      "Average Median RRE REG:  4.90\n",
      "Average Median RTE REG:  2.56\n",
      "\n",
      "Mean inference time (PR + REG):     71.11 ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(results_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_example = correct_examples[20]\n",
    "\n",
    "query_sample, db_match_sample, estimated_pose = correct_example\n",
    "\n",
    "draw_pc_pair(\n",
    "    query_sample[\"pointcloud_lidar_coords\"],\n",
    "    estimated_pose,\n",
    "    db_match_sample[\"pointcloud_lidar_coords\"],\n",
    "    db_match_sample[\"pose\"],\n",
    ")\n",
    "\n",
    "ts.show([\n",
    "    query_sample[\"image_front_cam\"], query_sample[\"image_back_cam\"],\n",
    "])\n",
    "ts.show([\n",
    "    db_match_sample[\"image_front_cam\"], db_match_sample[\"image_back_cam\"],\n",
    "])\n",
    "print(f\"PR pose error: {compute_error(db_match_sample['pose'], query_sample['pose'])}\")\n",
    "print(f\"REG pose error: {compute_error(estimated_pose, query_sample['pose'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_incorrect_example = pr_incorrect_examples[0]\n",
    "\n",
    "query_sample, db_match_sample, estimated_pose = correct_example\n",
    "\n",
    "ts.show([\n",
    "    query_sample[\"image_front_cam\"], query_sample[\"image_back_cam\"],\n",
    "])\n",
    "ts.show([\n",
    "    db_match_sample[\"image_front_cam\"], db_match_sample[\"image_back_cam\"],\n",
    "])\n",
    "print(f\"PR pose error: {compute_error(db_match_sample['pose'], query_sample['pose'])}\")\n",
    "print(f\"REG pose error: {compute_error(estimated_pose, query_sample['pose'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_incorrect_example = reg_incorrect_examples[0]\n",
    "\n",
    "query_sample, db_match_sample, estimated_pose = reg_incorrect_example\n",
    "\n",
    "ts.show([\n",
    "    query_sample[\"image_Cam5\"], query_sample[\"image_Cam2\"],\n",
    "])\n",
    "ts.show([\n",
    "    db_match_sample[\"image_Cam5\"], db_match_sample[\"image_Cam2\"],\n",
    "])\n",
    "print(f\"PR pose error: {compute_error(db_match_sample['pose'], query_sample['pose'])}\")\n",
    "print(f\"REG pose error: {compute_error(estimated_pose, query_sample['pose'])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
