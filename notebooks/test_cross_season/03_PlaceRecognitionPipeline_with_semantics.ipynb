{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PlaceRecognitionPipeline with semantics - ITLP dataset test\n",
    "\n",
    "A module that implements a neural network algorithm for searching a database of places already visited by a vehicle for the most similar records using sequences of data from lidars and cameras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:albumentations.check_version:A new version of Albumentations is available: 1.4.24 (you have 1.4.10). Upgrade using: pip install --upgrade albumentations\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "from pathlib import Path\n",
    "from time import time\n",
    "\n",
    "import faiss\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "import open3d.core as o3c\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchshow as ts\n",
    "\n",
    "from hydra.utils import instantiate\n",
    "from omegaconf import OmegaConf\n",
    "from scipy.spatial.transform import Rotation\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from geotransformer.utils.registration import compute_registration_error\n",
    "\n",
    "\n",
    "from opr.datasets.itlp import ITLPCampus\n",
    "from opr.pipelines.place_recognition import PlaceRecognitionPipeline\n",
    "from opr.pipelines.registration import PointcloudRegistrationPipeline\n",
    "from opr.pipelines.localization import LocalizationPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"DISPLAY\"] = \":1\"\n",
    "\n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pose_to_matrix(pose):\n",
    "    \"\"\"From the 6D poses in the [tx ty tz qx qy qz qw] format to 4x4 pose matrices.\"\"\"\n",
    "    position = pose[:3]\n",
    "    orientation_quat = pose[3:]\n",
    "    rotation = Rotation.from_quat(orientation_quat)\n",
    "    pose_matrix = np.eye(4)\n",
    "    pose_matrix[:3,:3] = rotation.as_matrix()\n",
    "    pose_matrix[:3,3] = position\n",
    "    return pose_matrix\n",
    "\n",
    "\n",
    "# def compute_error(estimated_pose, gt_pose):\n",
    "#     \"\"\"For the 6D poses in the [tx ty tz qx qy qz qw] format.\"\"\"\n",
    "#     estimated_pose = pose_to_matrix(estimated_pose)\n",
    "#     gt_pose = pose_to_matrix(gt_pose)\n",
    "#     return compute_registration_error(estimated_pose, gt_pose)\n",
    "\n",
    "def compute_error(estimated_pose, gt_pose):\n",
    "    \"\"\"For the 6D poses in the [tx ty tz qx qy qz qw] format.\"\"\"\n",
    "    estimated_pose = pose_to_matrix(estimated_pose)\n",
    "    gt_pose = pose_to_matrix(gt_pose)\n",
    "    error_pose = np.linalg.inv(estimated_pose) @ gt_pose\n",
    "    dist_error = np.sum(error_pose[:3, 3]**2) ** 0.5\n",
    "    r = Rotation.from_matrix(error_pose[:3, :3])\n",
    "    rotvec = r.as_rotvec()\n",
    "    angle_error = (np.sum(rotvec**2)**0.5) * 180 / np.pi\n",
    "    angle_error = abs(90 - abs(angle_error-90))\n",
    "    return angle_error, dist_error\n",
    "\n",
    "\n",
    "def draw_pc(pc: Tensor, color: str = \"blue\"):\n",
    "    pc_o3d = o3c.Tensor.from_dlpack(torch.utils.dlpack.to_dlpack(pc))\n",
    "    pcd = o3d.t.geometry.PointCloud(pc_o3d)\n",
    "    if color == \"blue\":\n",
    "        c = [0.0, 0.0, 1.0]\n",
    "    elif color == \"red\":\n",
    "        c = [1.0, 0.0, 0.0]\n",
    "    else:\n",
    "        c = [0.0, 1.0, 0.0]\n",
    "    pcd = pcd.paint_uniform_color(c)\n",
    "    o3d.visualization.draw_geometries(\n",
    "        [pcd.to_legacy()],\n",
    "    )\n",
    "\n",
    "\n",
    "def invert_rigid_transformation_matrix(T: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Inverts a 4x4 rigid body transformation matrix.\n",
    "\n",
    "    Args:\n",
    "        T (np.ndarray): A 4x4 rigid body transformation matrix.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The inverted 4x4 rigid body transformation matrix.\n",
    "    \"\"\"\n",
    "    assert T.shape == (4, 4), \"Input matrix must be 4x4.\"\n",
    "\n",
    "    R = T[:3, :3]\n",
    "    t = T[:3, 3]\n",
    "\n",
    "    R_inv = R.T\n",
    "    t_inv = -R.T @ t\n",
    "\n",
    "    T_inv = np.eye(4)\n",
    "    T_inv[:3, :3] = R_inv\n",
    "    T_inv[:3, 3] = t_inv\n",
    "\n",
    "    return T_inv\n",
    "\n",
    "\n",
    "def draw_pc_pair(\n",
    "    pc_blue: Tensor, pc_blue_pose: np.ndarray | Tensor, pc_red: Tensor, pc_red_pose: np.ndarray | Tensor\n",
    "):\n",
    "    pc_blue_o3d = o3c.Tensor.from_dlpack(torch.utils.dlpack.to_dlpack(copy.deepcopy(pc_blue)))\n",
    "    pc_red_o3d = o3c.Tensor.from_dlpack(torch.utils.dlpack.to_dlpack(copy.deepcopy(pc_red)))\n",
    "\n",
    "    blue_pcd = o3d.t.geometry.PointCloud(pc_blue_o3d)\n",
    "    blue_pcd_tmp = copy.deepcopy(blue_pcd)\n",
    "\n",
    "    red_pcd = o3d.t.geometry.PointCloud(pc_red_o3d)\n",
    "    red_pcd_tmp = copy.deepcopy(red_pcd)\n",
    "\n",
    "    blue_pcd_tmp.voxel_down_sample(voxel_size=0.3)\n",
    "    # blue_pcd_tmp.transform(pose_to_matrix(pc_blue_pose))\n",
    "    blue_pcd_tmp = blue_pcd_tmp.paint_uniform_color([0.0, 0.0, 1.0])\n",
    "\n",
    "    red_pcd_tmp.voxel_down_sample(voxel_size=0.3)\n",
    "    red_pcd_tmp.transform(pose_to_matrix(pc_red_pose))\n",
    "    red_pcd_tmp.transform(invert_rigid_transformation_matrix(pose_to_matrix(pc_blue_pose)))\n",
    "    red_pcd_tmp = red_pcd_tmp.paint_uniform_color([1.0, 0.0, 0.0])\n",
    "    o3d.visualization.draw_geometries(\n",
    "        [blue_pcd_tmp.to_legacy(), red_pcd_tmp.to_legacy()],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can **download the dataset**:\n",
    "\n",
    "- Kaggle:\n",
    "  - [ITLP Campus Outdoor](https://www.kaggle.com/datasets/alexandermelekhin/itlp-campus-outdoor)\n",
    "- Hugging Face:\n",
    "  - [ITLP Campus Outdoor](https://huggingface.co/datasets/OPR-Project/ITLP-Campus-Outdoor)\n",
    "\n",
    "To **download the model weights**, run the following command:\n",
    "\n",
    "```bash\n",
    "# place recognition weights\n",
    "wget -O ../../weights/place_recognition/multi-image_multi-semantic_lidar_late-fusion_itlp-finetune.pth https://huggingface.co/OPR-Project/PlaceRecognition-NCLT/resolve/main/multi-image_multi-semantic_lidar_late-fusion_itlp-finetune.pth\n",
    "\n",
    "# registration weights\n",
    "wget -O ../../weights/registration/hregnet_light_feats_nuscenes.pth https://huggingface.co/OPR-Project/Registration-nuScenes/resolve/main/hregnet_light_feats_nuscenes.pth\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# place recognition weights\n",
    "!wget -O ../../weights/place_recognition/multi-image_multi-semantic_lidar_late-fusion_itlp-finetune.pth https://huggingface.co/OPR-Project/PlaceRecognition-NCLT/resolve/main/multi-image_multi-semantic_lidar_late-fusion_itlp-finetune.pth\n",
    "\n",
    "# registration weights\n",
    "!wget -O ../../weights/registration/hregnet_light_feats_nuscenes.pth https://huggingface.co/OPR-Project/Registration-nuScenes/resolve/main/hregnet_light_feats_nuscenes.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test track list:\n",
      "['00_2023-02-10', '03_2023-04-11', '05_2023-08-15-day', '07_2023-10-04-day']\n"
     ]
    }
   ],
   "source": [
    "DATASET_ROOT = \"/home/docker_opr/Datasets/OpenPlaceRecognition/itlp_campus_outdoor\"\n",
    "\n",
    "TRACK_LIST = [\n",
    "    \"00_2023-02-10\",\n",
    "    \"03_2023-04-11\",\n",
    "    \"05_2023-08-15-day\",\n",
    "    \"07_2023-10-04-day\",\n",
    "]\n",
    "\n",
    "SEASON_MAPPING = {\n",
    "    \"00_2023-02-10\": \"winter\",\n",
    "    \"03_2023-04-11\": \"spring\",\n",
    "    \"05_2023-08-15-day\": \"summer\",\n",
    "    \"07_2023-10-04-day\": \"fall\",\n",
    "}\n",
    "\n",
    "print(\"Test track list:\")\n",
    "print(TRACK_LIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SENSOR_SUITE = [\"front_cam\", \"back_cam\", \"lidar\"]\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "NUM_WORKERS = 4\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "PR_MODEL_CONFIG_PATH = \"../../configs/model/place_recognition/multi-image_multi-semantic_lidar_late-fusion.yaml\"\n",
    "PR_WEIGHTS_PATH = \"../../weights/place_recognition/multi-image_multi-semantic_lidar_late-fusion_itlp-finetune.pth\"\n",
    "\n",
    "REGISTRATION_MODEL_CONFIG_PATH = \"../../configs/model/registration/hregnet_light_feats.yaml\"\n",
    "REGISTRATION_WEIGHTS_PATH = \"../../weights/registration/hregnet_light_feats_nuscenes.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_model_config = OmegaConf.load(PR_MODEL_CONFIG_PATH)\n",
    "pr_model = instantiate(pr_model_config)\n",
    "pr_model.load_state_dict(torch.load(PR_WEIGHTS_PATH))\n",
    "pr_model = pr_model.to(DEVICE)\n",
    "pr_model.eval();\n",
    "\n",
    "reg_model_config = OmegaConf.load(REGISTRATION_MODEL_CONFIG_PATH)\n",
    "reg_model = instantiate(reg_model_config)\n",
    "reg_model.load_state_dict(torch.load(REGISTRATION_WEIGHTS_PATH))\n",
    "reg_model = reg_model.to(DEVICE)\n",
    "reg_model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate descriptors for databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ITLPCampus(\n",
    "    dataset_root=DATASET_ROOT,\n",
    "    subset=\"test\",\n",
    "    csv_file=\"full_test.csv\",\n",
    "    sensors=SENSOR_SUITE,\n",
    "    load_semantics=True,\n",
    ")\n",
    "dataset.dataset_df = dataset.dataset_df[dataset.dataset_df[\"track\"].isin(TRACK_LIST)]\n",
    "dataset.dataset_df.reset_index(inplace=True)\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    collate_fn=dataset.collate_fn,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/145 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 145/145 [00:05<00:00, 24.88it/s]\n"
     ]
    }
   ],
   "source": [
    "descriptors = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(dataloader):\n",
    "        batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "        final_descriptor = pr_model(batch)[\"final_descriptor\"]\n",
    "        descriptors.append(final_descriptor.detach().cpu().numpy())\n",
    "\n",
    "descriptors = np.concatenate(descriptors, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving database indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved index /home/docker_opr/Datasets/OpenPlaceRecognition/itlp_campus_outdoor_part2/00_2023-02-10/index.faiss\n",
      "Saved index /home/docker_opr/Datasets/OpenPlaceRecognition/itlp_campus_outdoor_part2/03_2023-04-11/index.faiss\n",
      "Saved index /home/docker_opr/Datasets/OpenPlaceRecognition/itlp_campus_outdoor_part2/05_2023-08-15-day/index.faiss\n",
      "Saved index /home/docker_opr/Datasets/OpenPlaceRecognition/itlp_campus_outdoor_part2/07_2023-10-04-day/index.faiss\n"
     ]
    }
   ],
   "source": [
    "dataset_df = dataset.dataset_df\n",
    "\n",
    "for track, indices in dataset_df.groupby(\"track\").groups.items():\n",
    "    track_descriptors = descriptors[indices]\n",
    "    track_index = faiss.IndexFlatL2(track_descriptors.shape[1])\n",
    "    track_index.add(track_descriptors)\n",
    "    faiss.write_index(track_index, f\"{DATASET_ROOT}/{track}/index.faiss\")\n",
    "    print(f\"Saved index {DATASET_ROOT}/{track}/index.faiss\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing old pre-computed registration features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using the `precomputed_reg_feats=True` option, the pipeline will use the pre-computed registration features. If you want to re-compute them, you need to remove the old ones first (if they exist). You can do this by running the following command:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing existing registration features directory: /home/docker_opr/Datasets/OpenPlaceRecognition/itlp_campus_outdoor/00_2023-02-10/HRegNet_features\n",
      "Successfully removed /home/docker_opr/Datasets/OpenPlaceRecognition/itlp_campus_outdoor/00_2023-02-10/HRegNet_features\n",
      "Removing existing registration features directory: /home/docker_opr/Datasets/OpenPlaceRecognition/itlp_campus_outdoor/03_2023-04-11/HRegNet_features\n",
      "Successfully removed /home/docker_opr/Datasets/OpenPlaceRecognition/itlp_campus_outdoor/03_2023-04-11/HRegNet_features\n",
      "Removing existing registration features directory: /home/docker_opr/Datasets/OpenPlaceRecognition/itlp_campus_outdoor/05_2023-08-15-day/HRegNet_features\n",
      "Successfully removed /home/docker_opr/Datasets/OpenPlaceRecognition/itlp_campus_outdoor/05_2023-08-15-day/HRegNet_features\n",
      "Removing existing registration features directory: /home/docker_opr/Datasets/OpenPlaceRecognition/itlp_campus_outdoor/07_2023-10-04-day/HRegNet_features\n",
      "Successfully removed /home/docker_opr/Datasets/OpenPlaceRecognition/itlp_campus_outdoor/07_2023-10-04-day/HRegNet_features\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "for track in TRACK_LIST:\n",
    "    reg_model_name = \"HRegNet\"\n",
    "    reg_features_dir = Path(f\"{DATASET_ROOT}/{track}/{reg_model_name}_features\")\n",
    "    if reg_features_dir.exists():\n",
    "        print(f\"Removing existing registration features directory: {reg_features_dir}\")\n",
    "        shutil.rmtree(reg_features_dir)\n",
    "        print(f\"Successfully removed {reg_features_dir}\")\n",
    "    else:\n",
    "        print(f\"No existing registration features directory found at {reg_features_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-12-29 17:55:36.065\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mopr.pipelines.localization.base\u001b[0m:\u001b[36m_setup_precomputed_reg_feats\u001b[0m:\u001b[36m95\u001b[0m - \u001b[33m\u001b[1mPrecomputed registration features directory not found: /home/docker_opr/Datasets/OpenPlaceRecognition/itlp_campus_outdoor_part2/00_2023-02-10/HRegNet_features. It will be created and features will be computed.\u001b[0m\n",
      "\u001b[32m2024-12-29 17:55:36.066\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mopr.pipelines.localization.base\u001b[0m:\u001b[36m_setup_precomputed_reg_feats\u001b[0m:\u001b[36m105\u001b[0m - \u001b[33m\u001b[1mPrecomputed registration features directory is empty. Computing features.\u001b[0m\n",
      "  0%|          | 0/609 [00:00<?, ?it/s]/home/docker_opr/OpenPlaceRecognition/third_party/HRegNet/hregnet/utils.py:24: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  output = torch.cuda.IntTensor(B, npoint)\n",
      "0it [00:00, ?it/s]                               /usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "136it [00:16,  8.00it/s]\n",
      "152it [00:17,  8.45it/s]\n",
      "152it [00:18,  8.38it/s]\n",
      "\u001b[32m2024-12-29 17:56:44.409\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mopr.pipelines.localization.base\u001b[0m:\u001b[36m_setup_precomputed_reg_feats\u001b[0m:\u001b[36m95\u001b[0m - \u001b[33m\u001b[1mPrecomputed registration features directory not found: /home/docker_opr/Datasets/OpenPlaceRecognition/itlp_campus_outdoor_part2/03_2023-04-11/HRegNet_features. It will be created and features will be computed.\u001b[0m\n",
      "\u001b[32m2024-12-29 17:56:44.410\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mopr.pipelines.localization.base\u001b[0m:\u001b[36m_setup_precomputed_reg_feats\u001b[0m:\u001b[36m105\u001b[0m - \u001b[33m\u001b[1mPrecomputed registration features directory is empty. Computing features.\u001b[0m\n",
      "139it [00:17,  8.12it/s]                         \n",
      "152it [00:18,  8.42it/s]\n",
      "152it [00:18,  8.23it/s]\n",
      "\u001b[32m2024-12-29 17:57:52.501\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mopr.pipelines.localization.base\u001b[0m:\u001b[36m_setup_precomputed_reg_feats\u001b[0m:\u001b[36m95\u001b[0m - \u001b[33m\u001b[1mPrecomputed registration features directory not found: /home/docker_opr/Datasets/OpenPlaceRecognition/itlp_campus_outdoor_part2/05_2023-08-15-day/HRegNet_features. It will be created and features will be computed.\u001b[0m\n",
      "\u001b[32m2024-12-29 17:57:52.501\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mopr.pipelines.localization.base\u001b[0m:\u001b[36m_setup_precomputed_reg_feats\u001b[0m:\u001b[36m105\u001b[0m - \u001b[33m\u001b[1mPrecomputed registration features directory is empty. Computing features.\u001b[0m\n",
      "139it [00:16,  8.42it/s]                         \n",
      "136it [00:16,  8.36it/s]\n",
      "152it [00:18,  8.32it/s]\n",
      "\u001b[32m2024-12-29 17:59:01.172\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mopr.pipelines.localization.base\u001b[0m:\u001b[36m_setup_precomputed_reg_feats\u001b[0m:\u001b[36m95\u001b[0m - \u001b[33m\u001b[1mPrecomputed registration features directory not found: /home/docker_opr/Datasets/OpenPlaceRecognition/itlp_campus_outdoor_part2/07_2023-10-04-day/HRegNet_features. It will be created and features will be computed.\u001b[0m\n",
      "\u001b[32m2024-12-29 17:59:01.173\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mopr.pipelines.localization.base\u001b[0m:\u001b[36m_setup_precomputed_reg_feats\u001b[0m:\u001b[36m105\u001b[0m - \u001b[33m\u001b[1mPrecomputed registration features directory is empty. Computing features.\u001b[0m\n",
      "139it [00:16,  8.38it/s]                         \n",
      "136it [00:16,  8.39it/s]\n",
      "152it [00:18,  8.30it/s]\n"
     ]
    }
   ],
   "source": [
    "RECALL_THRESHOLD = 25.0\n",
    "\n",
    "all_pr_recalls = {}\n",
    "all_reg_recalls = {}  # it is recall after registration (if estimated pose within RECALL_THRESHOLD), do not confuse with registration recall\n",
    "\n",
    "all_mean_pr_rotation_errors = {}\n",
    "all_mean_pr_translation_errors = {}\n",
    "\n",
    "all_median_pr_rotation_errors = {}\n",
    "all_median_pr_translation_errors = {}\n",
    "\n",
    "all_mean_reg_rotation_errors = {}\n",
    "all_mean_reg_translation_errors = {}\n",
    "\n",
    "all_median_reg_rotation_errors = {}\n",
    "all_median_reg_translation_errors = {}\n",
    "\n",
    "all_times = []\n",
    "\n",
    "correct_examples = []  # the most representative correct pairs\n",
    "pr_incorrect_examples = []  # the most representative incorrect pairs where place recognition failed\n",
    "reg_incorrect_examples = []  # the most representative incorrect pairs where registration failed\n",
    "\n",
    "for db_track in TRACK_LIST:\n",
    "    pr_pipe = PlaceRecognitionPipeline(\n",
    "        database_dir=Path(DATASET_ROOT) / db_track,\n",
    "        model=pr_model,\n",
    "        model_weights_path=PR_WEIGHTS_PATH,\n",
    "        device=DEVICE,\n",
    "    )\n",
    "    for query_track in TRACK_LIST:\n",
    "        if db_track == query_track:\n",
    "            continue\n",
    "\n",
    "        reg_pipe = PointcloudRegistrationPipeline(\n",
    "            model=reg_model,\n",
    "            model_weights_path=REGISTRATION_WEIGHTS_PATH,\n",
    "            device=DEVICE,\n",
    "            voxel_downsample_size=0.3,\n",
    "            num_points_downsample=8192,\n",
    "        )\n",
    "        loc_pipe = LocalizationPipeline(\n",
    "            place_recognition_pipeline=pr_pipe,\n",
    "            registration_pipeline=reg_pipe,\n",
    "            precomputed_reg_feats=True,\n",
    "            pointclouds_subdir=\"lidar\",\n",
    "        )\n",
    "\n",
    "        query_dataset = copy.deepcopy(dataset)\n",
    "        query_dataset.dataset_df = query_dataset.dataset_df[query_dataset.dataset_df[\"track\"] == query_track].reset_index(drop=True)\n",
    "        query_df = query_dataset.dataset_df\n",
    "\n",
    "        db_dataset = copy.deepcopy(dataset)\n",
    "        db_dataset.dataset_df = db_dataset.dataset_df[db_dataset.dataset_df[\"track\"] == db_track].reset_index(drop=True)\n",
    "        db_df = db_dataset.dataset_df\n",
    "\n",
    "        loc_pipe.pr_pipe.database_df = db_df\n",
    "        loc_pipe.database_df = db_df\n",
    "\n",
    "\n",
    "        pr_matches = []\n",
    "        pr_rotation_errors = []\n",
    "        pr_translation_errors = []\n",
    "\n",
    "        reg_matches = []\n",
    "        reg_rotation_errors = []\n",
    "        reg_translation_errors = []\n",
    "\n",
    "        times = []\n",
    "\n",
    "        for q_i, query in tqdm(enumerate(query_dataset)):\n",
    "            query_pose = query_df.iloc[q_i][[\"tx\", \"ty\", \"tz\", \"qx\", \"qy\", \"qz\", \"qw\"]].to_numpy()\n",
    "\n",
    "            t = time()\n",
    "            output = loc_pipe.infer(query)\n",
    "            torch.cuda.current_stream().synchronize()\n",
    "            times.append(time() - t)\n",
    "\n",
    "            pr_rotation_error, pr_translation_error = compute_error(output[\"db_match_pose\"], query_pose)\n",
    "            reg_rotation_error, reg_translation_error = compute_error(output[\"estimated_pose\"], query_pose)\n",
    "\n",
    "            pr_correct = pr_translation_error < RECALL_THRESHOLD\n",
    "            reg_correct = reg_translation_error < RECALL_THRESHOLD\n",
    "\n",
    "            pr_matches.append(pr_correct)\n",
    "            pr_rotation_errors.append(pr_rotation_error)\n",
    "            pr_translation_errors.append(pr_translation_error)\n",
    "\n",
    "            reg_matches.append(reg_correct)\n",
    "            reg_rotation_errors.append(reg_rotation_error)\n",
    "            reg_translation_errors.append(reg_translation_error)\n",
    "\n",
    "            if pr_correct and reg_correct \\\n",
    "                and reg_rotation_error < pr_rotation_error and reg_translation_error < pr_translation_error \\\n",
    "                and reg_rotation_error < 3.0 and reg_translation_error < 1.0:\n",
    "                query[\"pose\"] = query_pose\n",
    "                db_match = db_dataset[output[\"db_match_idx\"]]\n",
    "                db_match[\"pose\"] = output[\"db_match_pose\"]\n",
    "                correct_examples.append((query, db_match, output[\"estimated_pose\"]))\n",
    "\n",
    "            if pr_correct and not reg_correct:\n",
    "                query[\"pose\"] = query_pose\n",
    "                db_match = db_dataset[output[\"db_match_idx\"]]\n",
    "                db_match[\"pose\"] = output[\"db_match_pose\"]\n",
    "                reg_incorrect_examples.append((query, db_match, output[\"estimated_pose\"]))\n",
    "\n",
    "            if not pr_correct and pr_translation_error > 50.0:\n",
    "                query[\"pose\"] = query_pose\n",
    "                db_match = db_dataset[output[\"db_match_idx\"]]\n",
    "                db_match[\"pose\"] = output[\"db_match_pose\"]\n",
    "                pr_incorrect_examples.append((query, db_match, output[\"estimated_pose\"]))\n",
    "\n",
    "        key_str = f\"DB {SEASON_MAPPING[db_track]}, Query {SEASON_MAPPING[query_track]}\"\n",
    "\n",
    "        all_pr_recalls[key_str] = np.mean(pr_matches)\n",
    "        all_reg_recalls[key_str] = np.mean(reg_matches)\n",
    "\n",
    "        all_mean_pr_rotation_errors[key_str] = np.mean(pr_rotation_errors)\n",
    "        all_mean_pr_translation_errors[key_str] = np.mean(pr_translation_errors)\n",
    "        all_median_pr_rotation_errors[key_str] = np.median(pr_rotation_errors)\n",
    "        all_median_pr_translation_errors[key_str] = np.median(pr_translation_errors)\n",
    "\n",
    "        all_mean_reg_rotation_errors[key_str] = np.mean(reg_rotation_errors)\n",
    "        all_mean_reg_translation_errors[key_str] = np.mean(reg_translation_errors)\n",
    "        all_median_reg_rotation_errors[key_str] = np.median(reg_rotation_errors)\n",
    "        all_median_reg_translation_errors[key_str] = np.median(reg_translation_errors)\n",
    "        all_times.extend(times[1:]) # drop the first iteration cause it is always slower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(137, 22, 0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(correct_examples), len(pr_incorrect_examples), len(reg_incorrect_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@1:\n",
      "DB winter, Query spring: 100.00\n",
      "DB winter, Query summer: 96.71\n",
      "DB winter, Query fall: 100.00\n",
      "DB spring, Query winter: 100.00\n",
      "DB spring, Query summer: 98.03\n",
      "DB spring, Query fall: 100.00\n",
      "DB summer, Query winter: 93.53\n",
      "DB summer, Query spring: 94.85\n",
      "DB summer, Query fall: 100.00\n",
      "DB fall, Query winter: 95.68\n",
      "DB fall, Query spring: 97.79\n",
      "DB fall, Query summer: 100.00\n",
      "Mean: 98.05\n"
     ]
    }
   ],
   "source": [
    "print(\"Recall@1:\")\n",
    "for key, value in all_reg_recalls.items():\n",
    "    print(f\"{key}: {value*100:.2f}\")\n",
    "\n",
    "print(f\"Mean: {np.mean(list(all_reg_recalls.values()))*100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median RRE:\n",
      "DB winter, Query spring: 1.64\n",
      "DB winter, Query summer: 6.71\n",
      "DB winter, Query fall: 4.26\n",
      "DB spring, Query winter: 1.95\n",
      "DB spring, Query summer: 6.18\n",
      "DB spring, Query fall: 4.59\n",
      "DB summer, Query winter: 7.41\n",
      "DB summer, Query spring: 6.26\n",
      "DB summer, Query fall: 4.83\n",
      "DB fall, Query winter: 4.26\n",
      "DB fall, Query spring: 4.46\n",
      "DB fall, Query summer: 4.23\n",
      "Mean: 4.73\n"
     ]
    }
   ],
   "source": [
    "print(\"Median RRE:\")\n",
    "for key, value in all_median_reg_rotation_errors.items():\n",
    "    print(f\"{key}: {value:.2f}\")\n",
    "\n",
    "print(f\"Mean: {np.mean(list(all_median_reg_rotation_errors.values())):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median RTE:\n",
      "DB winter, Query spring: 0.78\n",
      "DB winter, Query summer: 3.99\n",
      "DB winter, Query fall: 4.20\n",
      "DB spring, Query winter: 0.78\n",
      "DB spring, Query summer: 3.22\n",
      "DB spring, Query fall: 4.15\n",
      "DB summer, Query winter: 3.61\n",
      "DB summer, Query spring: 3.50\n",
      "DB summer, Query fall: 2.81\n",
      "DB fall, Query winter: 4.17\n",
      "DB fall, Query spring: 4.25\n",
      "DB fall, Query summer: 2.99\n",
      "Mean: 3.20\n"
     ]
    }
   ],
   "source": [
    "print(\"Median RTE:\")\n",
    "for key, value in all_median_reg_translation_errors.items():\n",
    "    print(f\"{key}: {value:.2f}\")\n",
    "\n",
    "\n",
    "print(f\"Mean: {np.mean(list(all_median_reg_translation_errors.values())):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean inference time: 74.78 ms\n"
     ]
    }
   ],
   "source": [
    "print(f\"Mean inference time: {np.mean(all_times[1:]) * 1000:.2f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_example = correct_examples[-20]\n",
    "\n",
    "query_sample, db_match_sample, estimated_pose = correct_example\n",
    "\n",
    "draw_pc_pair(\n",
    "    query_sample[\"pointcloud_lidar_coords\"],\n",
    "    estimated_pose,\n",
    "    db_match_sample[\"pointcloud_lidar_coords\"],\n",
    "    db_match_sample[\"pose\"]\n",
    ")\n",
    "\n",
    "ts.show([\n",
    "    query_sample[\"image_front_cam\"], query_sample[\"image_back_cam\"],\n",
    "])\n",
    "ts.show([\n",
    "    db_match_sample[\"image_front_cam\"], db_match_sample[\"image_back_cam\"],\n",
    "])\n",
    "print(f\"Pose error: {compute_error(estimated_pose, query_sample['pose'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_incorrect_example = pr_incorrect_examples[0]\n",
    "\n",
    "query_sample, db_match_sample, estimated_pose = pr_incorrect_example\n",
    "\n",
    "draw_pc_pair(\n",
    "    query_sample[\"pointcloud_lidar_coords\"],\n",
    "    estimated_pose,\n",
    "    db_match_sample[\"pointcloud_lidar_coords\"],\n",
    "    db_match_sample[\"pose\"]\n",
    ")\n",
    "\n",
    "ts.show([\n",
    "    query_sample[\"image_front_cam\"], query_sample[\"image_back_cam\"],\n",
    "])\n",
    "ts.show([\n",
    "    db_match_sample[\"image_front_cam\"], db_match_sample[\"image_back_cam\"],\n",
    "])\n",
    "print(f\"Pose error: {compute_error(estimated_pose, query_sample['pose'])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
