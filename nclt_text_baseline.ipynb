{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.15) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import hydra\n",
    "import torch\n",
    "import wandb\n",
    "from hydra.utils import instantiate\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "\n",
    "from opr.datasets.dataloader_factory import make_dataloaders\n",
    "from opr.testing import test\n",
    "from opr.training import epoch_loop\n",
    "from opr.utils import flatten_dict, set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hydra import initialize, compose\n",
    "from hydra.utils import instantiate\n",
    "\n",
    "with initialize(version_base=None, config_path=\"configs\"):\n",
    "    cfg = compose(config_name='config_nclt_text.yaml')  # only_text\n",
    "    # cfg = compose(config_name='config.yaml')  # lidar + cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'debug': False, 'seed': 31299, 'checkpoints_dir': 'checkpoints/', 'device': 'cuda', 'num_workers': 4, 'batch_expansion_th': 0.7, 'modalities': ['text_cam5', 'text_cam2', 'text_cam1', 'text_cam3', 'text_cam4'], 'test_modality': 'text', 'epochs': 60}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_target_': 'opr.models.base_models.ComposedModel', 'image_module': None, 'cloud_module': None, 'fusion_module': None, 'text_module': {'_target_': 'opr.models.base_models.MultiTextModule', 'text_module': {'_target_': 'opr.models.base_models.TextModule', 'text_emb_size': 100, 'hidden_size': 100}, 'fusion_module': {'_target_': 'opr.models.fusion.Concat'}}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not cfg.general.debug and not cfg.wandb.disabled:\n",
    "#     config_dict = OmegaConf.to_container(cfg, resolve=True, throw_on_missing=True)\n",
    "#     wandb.init(\n",
    "#         name=cfg.wandb.run_name,\n",
    "#         project=cfg.wandb.project,\n",
    "#         settings=wandb.Settings(start_method=\"thread\"),\n",
    "#         config=config_dict,\n",
    "#     )\n",
    "#     wandb.save(f\"configs/{wandb.run.name}.yaml\")\n",
    "#     run_name = wandb.run.name\n",
    "# else:\n",
    "#     run_name = \"debug\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoints_dir = (\n",
    "#     Path(cfg.general.checkpoints_dir) / f\"{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}_{run_name}\"\n",
    "# )\n",
    "# if not checkpoints_dir.exists():\n",
    "#     checkpoints_dir.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from joblib import dump"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train TFIDF and PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>track</th>\n",
       "      <th>image</th>\n",
       "      <th>pointcloud</th>\n",
       "      <th>northing</th>\n",
       "      <th>easting</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2012-01-08</td>\n",
       "      <td>1326030979526128</td>\n",
       "      <td>1326030979526128</td>\n",
       "      <td>0.338313</td>\n",
       "      <td>0.222726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2012-01-08</td>\n",
       "      <td>1326031015326922</td>\n",
       "      <td>1326031015326922</td>\n",
       "      <td>-2.798348</td>\n",
       "      <td>-9.238316</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0       track             image        pointcloud  northing   \n",
       "0           0  2012-01-08  1326030979526128  1326030979526128  0.338313  \\\n",
       "1           1  2012-01-08  1326031015326922  1326031015326922 -2.798348   \n",
       "\n",
       "    easting  \n",
       "0  0.222726  \n",
       "1 -9.238316  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_path = \"/home/docker_opr/Datasets/NCLT_preprocessed\"\n",
    "train_df_path = os.path.join(base_path, \"train.csv\")\n",
    "\n",
    "train_df = pd.read_csv(train_df_path)\n",
    "train_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks = train_df[\"track\"].unique()\n",
    "\n",
    "descriptions = []\n",
    "\n",
    "for track in tracks:\n",
    "    cur_track_path = os.path.join(base_path, track)\n",
    "    cur_track_df = train_df[train_df[\"track\"] == track]\n",
    "    images = cur_track_df.image.values\n",
    "    images = [str(int(i))+\".png\" for i in images]\n",
    "    for cam_id in range(1, 6):\n",
    "        cam_df_path = os.path.join(cur_track_path, f\"descriptions_Cam{cam_id}.csv\")\n",
    "        cam_df = pd.read_csv(cam_df_path)\n",
    "        cam_descriptions = cam_df[cam_df[\"path\"].isin(images)][\"description\"].values\n",
    "        descriptions.append(cam_descriptions)\n",
    "        \n",
    "descriptions = np.hstack((descriptions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14640,), 14640)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "descriptions.shape, len(train_df) * 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tfidf_pca(corpus, \n",
    "                    max_features_tfidf=None,\n",
    "                    n_components_pca=100, \n",
    "                    base_savepath=\"./opr/datasets/\"):\n",
    "    vectorizer = TfidfVectorizer(max_features=max_features_tfidf)\n",
    "    vectorizer.fit(corpus)\n",
    "    print(\"n tfidf features = \", vectorizer.get_feature_names_out().shape)\n",
    "    vectorized_corpus = vectorizer.transform(corpus).toarray()\n",
    "\n",
    "    pca = PCA(n_components=n_components_pca)\n",
    "    pca.fit(vectorized_corpus)\n",
    "    \n",
    "    vectorizer_savepath = os.path.join(base_savepath, 'vectorizer.joblib')\n",
    "    pca_savepath = os.path.join(base_savepath, 'pca.joblib')\n",
    "    dump(vectorizer, vectorizer_savepath) \n",
    "    dump(pca, pca_savepath) \n",
    "\n",
    "def text_transform(self, text):\n",
    "    vect_data = self.vectorizer.transform([text]).toarray()\n",
    "    pca_data = self.pca.transform(vect_data)\n",
    "    pca_data = torch.tensor(pca_data, dtype=torch.float32)\n",
    "    return pca_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text here\n",
      "n tfidf features =  (7857,)\n"
     ]
    }
   ],
   "source": [
    "if \"text_cam1\" in cfg.general.modalities:\n",
    "    print(\"text here\")\n",
    "    train_tfidf_pca(descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_root = \"/home/docker_opr/Datasets/NCLT_preprocessed\"\n",
    "# tracks = [i for i in os.listdir(dataset_root) if os.path.isdir(os.path.join(dataset_root, i))]\n",
    "# df_dict = {}\n",
    "\n",
    "# for track in tracks:\n",
    "#     track_path = os.path.join(dataset_root, track)\n",
    "#     df_dict[track] = {f\"cam{n}\" : pd.read_csv(os.path.join(track_path, f\"descriptions_Cam{n}.csv\")) for n in range(1, 6)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# track = '2012-02-12'\n",
    "# cam = \"cam1\"\n",
    "# image = \"1329070191225477\"\n",
    "\n",
    "# cam_df = df_dict[track][cam]\n",
    "# text = cam_df[cam_df[\"path\"] == f\"{image}.png\"][\"description\"].values[0]\n",
    "# text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"=> Instantiating model...\")\n",
    "# model = instantiate(cfg.model)\n",
    "\n",
    "# print(\"=> Instantiating loss...\")\n",
    "# loss_fn = instantiate(cfg.loss)\n",
    "\n",
    "# print(\"=> Making dataloaders...\")\n",
    "# dataloaders = make_dataloaders(\n",
    "#     dataset_cfg=cfg.dataset.dataset,\n",
    "#     batch_sampler_cfg=cfg.dataset.sampler,\n",
    "#     num_workers=cfg.dataset.num_workers,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch, _, _ = next(iter(dataloaders[\"train\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch[\"text_emb_cam1\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = instantiate(cfg.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model(batch)[\"text\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Instantiating model...\n",
      "=> Instantiating loss...\n",
      "=> Making dataloaders...\n",
      "=> Instantiating optimizer...\n",
      "Instantiating scheduler...\n"
     ]
    }
   ],
   "source": [
    "print(\"=> Instantiating model...\")\n",
    "model = instantiate(cfg.model)\n",
    "\n",
    "print(\"=> Instantiating loss...\")\n",
    "loss_fn = instantiate(cfg.loss)\n",
    "\n",
    "print(\"=> Making dataloaders...\")\n",
    "dataloaders = make_dataloaders(\n",
    "    dataset_cfg=cfg.dataset.dataset,\n",
    "    batch_sampler_cfg=cfg.dataset.sampler,\n",
    "    num_workers=cfg.dataset.num_workers,\n",
    ")\n",
    "\n",
    "print(\"=> Instantiating optimizer...\")\n",
    "params_list = []\n",
    "modalities = list(set([m.split(\"_\")[0] for m in cfg.general.modalities]))\n",
    "for modality in modalities:\n",
    "    params_list.append(\n",
    "        {\n",
    "            \"params\": getattr(model, f\"{modality}_module\").parameters(),\n",
    "            \"lr\": cfg.optimizer.learning_rates[f\"{modality}_lr\"],\n",
    "        }\n",
    "    )\n",
    "optimizer = instantiate(cfg.optimizer.fn, params=params_list)\n",
    "print(\"Instantiating scheduler...\")\n",
    "scheduler = instantiate(cfg.scheduler, optimizer=optimizer)\n",
    "\n",
    "model = model.to(cfg.general.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(ComposedModel(\n",
       "   (text_module): MultiTextModule(\n",
       "     (text_module): TextModule(\n",
       "       (fc1): Linear(in_features=100, out_features=100, bias=True)\n",
       "       (relu): ReLU()\n",
       "       (fc2): Linear(in_features=100, out_features=100, bias=True)\n",
       "     )\n",
       "     (fusion_module): Concat()\n",
       "   )\n",
       " ),\n",
       " Adam (\n",
       " Parameter Group 0\n",
       "     amsgrad: False\n",
       "     betas: (0.9, 0.999)\n",
       "     capturable: False\n",
       "     eps: 1e-08\n",
       "     foreach: None\n",
       "     initial_lr: 0.001\n",
       "     lr: 0.001\n",
       "     maximize: False\n",
       "     weight_decay: 0.0001\n",
       " ),\n",
       " ['text'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, optimizer, modalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.15) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpetili\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/docker_opr/OpenPlaceRecognition/wandb/run-20230512_151444-r9k6h3jg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/petili/OpenPlaceRecognition/runs/r9k6h3jg' target=\"_blank\">only_text_PCA_all_cam_Concat</a></strong> to <a href='https://wandb.ai/petili/OpenPlaceRecognition' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/petili/OpenPlaceRecognition' target=\"_blank\">https://wandb.ai/petili/OpenPlaceRecognition</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/petili/OpenPlaceRecognition/runs/r9k6h3jg' target=\"_blank\">https://wandb.ai/petili/OpenPlaceRecognition/runs/r9k6h3jg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if not cfg.general.debug and not cfg.wandb.disabled:\n",
    "    config_dict = OmegaConf.to_container(cfg, resolve=True, throw_on_missing=True)\n",
    "    wandb.init(\n",
    "        name=cfg.wandb.run_name,\n",
    "        project=cfg.wandb.project,\n",
    "        settings=wandb.Settings(start_method=\"thread\"),\n",
    "        config=config_dict,\n",
    "    )\n",
    "    wandb.save(f\"configs/{wandb.run.name}.yaml\")\n",
    "    run_name = wandb.run.name\n",
    "else:\n",
    "    run_name = \"debug\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints_dir = (\n",
    "    Path(cfg.general.checkpoints_dir) / f\"{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}_{run_name}\"\n",
    ")\n",
    "if not checkpoints_dir.exists():\n",
    "    checkpoints_dir.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "=====> Epoch 1:\n",
      "\n",
      "=> Training:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train:   8%|â–Š         | 30/377 [00:02<00:24, 14.32it/s]"
     ]
    }
   ],
   "source": [
    "best_recall_at_1 = 0.0\n",
    "\n",
    "# for epoch in range(40, 60):\n",
    "for epoch in range(cfg.general.epochs):\n",
    "    print(f\"\\n\\n=====> Epoch {epoch+1}:\")\n",
    "    # TODO: resolve mypy typing here\n",
    "    train_batch_size = dataloaders[\"train\"].batch_sampler.batch_size  # type: ignore\n",
    "    val_batch_size = dataloaders[\"val\"].batch_sampler.batch_size  # type: ignore\n",
    "\n",
    "    print(\"\\n=> Training:\\n\")\n",
    "\n",
    "    train_stats, train_rate_non_zero = epoch_loop(\n",
    "        dataloader=dataloaders[\"train\"],\n",
    "        model=model,\n",
    "        loss_fn=loss_fn,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        phase=\"train\",\n",
    "        device=cfg.general.device,\n",
    "    )\n",
    "\n",
    "    print(f\"\\ntrain_rate_non_zero = {train_rate_non_zero}\")\n",
    "\n",
    "    batch_expansion_th = cfg.general.batch_expansion_th\n",
    "    if batch_expansion_th is not None:\n",
    "        if batch_expansion_th == 1.0:\n",
    "            print(\"Batch expansion rate is set to every epoch. Increasing batch size.\")\n",
    "            # TODO: resolve mypy typing here\n",
    "            dataloaders[\"train\"].batch_sampler.expand_batch()  # type: ignore\n",
    "        elif train_rate_non_zero is None:\n",
    "            print(\n",
    "                \"\\nWARNING: 'batch_expansion_th' was set, but 'train_rate_non_zero' is None. \",\n",
    "                \"The batch size was not expanded.\",\n",
    "            )\n",
    "        elif train_rate_non_zero < batch_expansion_th:\n",
    "            print(\n",
    "                \"Average non-zero triplet ratio is less than threshold: \",\n",
    "                f\"{train_rate_non_zero} < {batch_expansion_th}\",\n",
    "            )\n",
    "            # TODO: resolve mypy typing here\n",
    "            dataloaders[\"train\"].batch_sampler.expand_batch()  # type: ignore\n",
    "\n",
    "    print(\"\\n=> Validating:\\n\")\n",
    "\n",
    "    val_stats, val_rate_non_zero = epoch_loop(\n",
    "        dataloader=dataloaders[\"val\"],\n",
    "        model=model,\n",
    "        loss_fn=loss_fn,\n",
    "        optimizer=optimizer,\n",
    "        phase=\"val\",\n",
    "        device=cfg.general.device,\n",
    "    )\n",
    "\n",
    "    print(f\"\\nval_rate_non_zero = {val_rate_non_zero}\")\n",
    "\n",
    "    print(\"\\n=> Testing:\\n\")\n",
    "\n",
    "    recall_at_n, recall_at_one_percent, mean_top1_distance = test(\n",
    "        model=model,\n",
    "        descriptor_key=cfg.general.test_modality,\n",
    "        dataloader=dataloaders[\"test\"],\n",
    "        device=cfg.general.device,\n",
    "    )\n",
    "\n",
    "    stats_dict = {}\n",
    "    stats_dict[\"test\"] = {\n",
    "        \"mean_top1_distance\": mean_top1_distance,\n",
    "        \"recall_at_1%\": recall_at_one_percent,\n",
    "        \"recall_at_1\": recall_at_n[0],\n",
    "        \"recall_at_3\": recall_at_n[2],\n",
    "        \"recall_at_5\": recall_at_n[4],\n",
    "        \"recall_at_10\": recall_at_n[9],\n",
    "    }\n",
    "    stats_dict[\"train\"] = train_stats\n",
    "    stats_dict[\"train\"][\"batch_size\"] = train_batch_size\n",
    "    stats_dict[\"val\"] = val_stats\n",
    "    stats_dict[\"val\"][\"batch_size\"] = val_batch_size\n",
    "\n",
    "    # saving checkpoints\n",
    "    checkpoint_dict = {\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"config\": cfg,\n",
    "        \"stats_dict\": stats_dict,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    }\n",
    "    torch.save(checkpoint_dict, checkpoints_dir / f\"epoch_{epoch+1}.pth\")\n",
    "    # wandb logging\n",
    "    if not cfg.general.debug and not cfg.wandb.disabled:\n",
    "        wandb.log(flatten_dict(stats_dict))\n",
    "        wandb.save(str((checkpoints_dir / f\"epoch_{epoch+1}.pth\").relative_to(\".\")))\n",
    "    if recall_at_n[0] > best_recall_at_1:\n",
    "        print(\"Recall@1 improved!\")\n",
    "        torch.save(checkpoint_dict, checkpoints_dir / \"best.pth\")\n",
    "        best_recall_at_1 = recall_at_n[0]\n",
    "        if not cfg.general.debug and not cfg.wandb.disabled:\n",
    "            wandb.save(str((checkpoints_dir / \"best.pth\").relative_to(\".\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
